{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNROaNMc0BND50mV/qwE0fP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mudrab/Natural-Language-Processing/blob/main/NLP_Practicals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRACTICAL NO: 1"
      ],
      "metadata": {
        "id": "UuR8evR3965O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 1A Study different libraries used for nlp in python**"
      ],
      "metadata": {
        "id": "cJjzqEy0iSJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** Convert the given text to speech."
      ],
      "metadata": {
        "id": "oFsJQH0xksM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  playsound"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOS8zOPjCxEK",
        "outputId": "f0e2eb02-c403-4bd3-bae7-762f7e5b7bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playsound\n",
            "  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: playsound\n",
            "  Building wheel for playsound (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7020 sha256=5454fced0912b95d5091a55ccfbc9228f4bed9b5fb50f8e0c9bd4a84501a937a\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/98/42/62753a9e1fb97579a0ce2f84f7db4c21c09d03bb2091e6cef4\n",
            "Successfully built playsound\n",
            "Installing collected packages: playsound\n",
            "Successfully installed playsound-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d-MDleXki_M",
        "outputId": "04fea1fc-6a3e-4cf6-8538-e3ac4d665b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_m8hq41XLrK",
        "outputId": "58a5a15d-4acd-4f08-ecda-18676b75d607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "myfile.mp3 has been saved.\n"
          ]
        }
      ],
      "source": [
        "from gtts import gTTS\n",
        "import os\n",
        "mytext = \"my name is manasi\"\n",
        "language = \"en\"\n",
        "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
        "myobj.save(\"myfile.mp3\")\n",
        "os.system(\"myfile.mp3\")\n",
        "print(\"myfile.mp3 has been saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** Convert the given speech to text."
      ],
      "metadata": {
        "id": "KjLnZcowkwWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UujrpWDnpeg",
        "outputId": "e884c44b-0900-4522-d503-bf016071ca02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.2-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.2)\n",
            "Downloading speechrecognition-3.14.2-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.2 pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub\n",
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYc7nJQWm4sl",
        "outputId": "ce00de26-3b93-46e1-c0a7-270e285f9789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.11/dist-packages (3.14.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.2)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.11/dist-packages (3.14.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr"
      ],
      "metadata": {
        "id": "I-pRrPuqm4md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "filename = \"16-122828-0002.wav\"\n",
        "# initialize the recognizer\n",
        "r = sr.Recognizer()\n",
        "# open the file\n",
        "with sr.AudioFile(filename) as source:\n",
        "# listen for the data (load audio to memory)\n",
        "  audio_data = r.record(source)\n",
        "  # recognize (convert from speech to text)\n",
        "  text = r.recognize_google(audio_data)\n",
        "  print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPgrzeSKlwK8",
        "outputId": "958a3635-c584-4c0d-c804-581aa3afd7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I believe you are just talking nonsense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/x4nth055/pythoncode-tutorials/blob/master/machine-learning/speech-recognition/16-122828-0002.wav"
      ],
      "metadata": {
        "id": "t2MpLHXm9Dve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical: 2**"
      ],
      "metadata": {
        "id": "3AHbYtSrjWCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** a. Study of various Corpus – Brown, Inaugural, Reuters, udhr with various\n",
        "methods like filelds, raw, words, sents, categories."
      ],
      "metadata": {
        "id": "nEdLVZ9umJhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the Brown Corpus\n",
        "nltk.download('brown')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "print ('File ids of brown corpus\\n',brown.fileids())\n",
        "'''Let’s pick out the first of these texts — Emma by Jane Austen — and give it a short\n",
        "name, emma, then find out how many words it contains:'''\n",
        "ca01 = brown.words('ca01')\n",
        "# display first few words\n",
        "print('\\nca01 has following words:\\n',ca01)\n",
        "# total number of words in ca01\n",
        "print('\\nca01 has',len(ca01),'words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8uRuAZ0lwwU",
        "outputId": "9e702ae8-dfad-4af3-e4f7-9e9514a9b4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ids of brown corpus\n",
            " ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n",
            "\n",
            "ca01 has following words:\n",
            " ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "\n",
            "ca01 has 2242 words\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "print ('File ids of brown corpus\\n',brown.fileids())\n",
        "'''Let’s pick out the first of these texts — Emma by Jane Austen — and give it a short\n",
        "name, emma, then find out how many words it contains:'''\n",
        "ca01 = brown.words('ca01')\n",
        "# display first few words\n",
        "print('\\nca01 has following words:\\n',ca01)\n",
        "# total number of words in ca01\n",
        "print('\\nca01 has',len(ca01),'words')\n",
        "#categories or files\n",
        "print ('\\n\\nCategories or file in brown corpus:\\n')\n",
        "print (brown.categories())\n",
        "'''display other information about each text, by looping over all the values of fileid\n",
        "corresponding to the brown file identifiers listed earlier and then computing statistics\n",
        "for each text.'''\n",
        "print ('\\n\\nStatistics for each text:\\n')\n",
        "print\n",
        "('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppearsOnAvg\\t\\tFileName')\n",
        "for fileid in brown.fileids():\n",
        "  num_chars = len(brown.raw(fileid))\n",
        "  num_words = len(brown.words(fileid))\n",
        "  num_sents = len(brown.sents(fileid))\n",
        "  num_vocab = len(set([w.lower() for w in brown.words(fileid)]))\n",
        "  print (int(num_chars/num_words),'\\t\\t\\t', int(num_words/num_sents),'\\t\\t\\t',\n",
        "int(num_words/num_vocab),'\\t\\t\\t', fileid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f0IDyp6le8U",
        "outputId": "343edb5a-a422-451f-b24b-57604cf190d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ids of brown corpus\n",
            " ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n",
            "\n",
            "ca01 has following words:\n",
            " ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "\n",
            "ca01 has 2242 words\n",
            "\n",
            "\n",
            "Categories or file in brown corpus:\n",
            "\n",
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "\n",
            "\n",
            "Statistics for each text:\n",
            "\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ca01\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ca02\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca03\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t ca04\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ca05\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca06\n",
            "9 \t\t\t 18 \t\t\t 2 \t\t\t ca07\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca08\n",
            "9 \t\t\t 19 \t\t\t 2 \t\t\t ca09\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca10\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca11\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca12\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca13\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t ca14\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca15\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca16\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca17\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca18\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca19\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ca20\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t ca21\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca22\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca23\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca24\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca25\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca26\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca27\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ca28\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca29\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca30\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca31\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ca32\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca33\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ca34\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t ca35\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t ca36\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca37\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ca38\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca39\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca40\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca41\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca42\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ca43\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca44\n",
            "9 \t\t\t 21 \t\t\t 2 \t\t\t cb01\n",
            "9 \t\t\t 23 \t\t\t 2 \t\t\t cb02\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cb03\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cb04\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cb05\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t cb06\n",
            "9 \t\t\t 24 \t\t\t 2 \t\t\t cb07\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cb08\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cb09\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cb10\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t cb11\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cb12\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t cb13\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cb14\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cb15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cb16\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cb17\n",
            "9 \t\t\t 19 \t\t\t 2 \t\t\t cb18\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cb19\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cb20\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cb21\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cb22\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cb23\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cb24\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cb25\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cb26\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cb27\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cc01\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cc02\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cc03\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc04\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc05\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cc06\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cc07\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cc08\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cc09\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc10\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc11\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cc12\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cc13\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cc14\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cc15\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cc16\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cc17\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cd01\n",
            "8 \t\t\t 34 \t\t\t 3 \t\t\t cd02\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cd03\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cd04\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t cd05\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cd06\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cd07\n",
            "8 \t\t\t 32 \t\t\t 3 \t\t\t cd08\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cd09\n",
            "9 \t\t\t 23 \t\t\t 2 \t\t\t cd10\n",
            "8 \t\t\t 34 \t\t\t 3 \t\t\t cd11\n",
            "9 \t\t\t 28 \t\t\t 2 \t\t\t cd12\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cd13\n",
            "9 \t\t\t 24 \t\t\t 2 \t\t\t cd14\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cd15\n",
            "8 \t\t\t 14 \t\t\t 4 \t\t\t cd16\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t cd17\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ce01\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ce02\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t ce03\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ce04\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t ce05\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ce06\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ce07\n",
            "8 \t\t\t 17 \t\t\t 4 \t\t\t ce08\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ce09\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ce10\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ce11\n",
            "9 \t\t\t 18 \t\t\t 2 \t\t\t ce12\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t ce13\n",
            "8 \t\t\t 13 \t\t\t 2 \t\t\t ce14\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t ce15\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ce16\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t ce17\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t ce18\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ce19\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t ce20\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t ce21\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ce22\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t ce23\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t ce24\n",
            "9 \t\t\t 23 \t\t\t 2 \t\t\t ce25\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ce26\n",
            "8 \t\t\t 12 \t\t\t 4 \t\t\t ce27\n",
            "9 \t\t\t 18 \t\t\t 3 \t\t\t ce28\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t ce29\n",
            "9 \t\t\t 15 \t\t\t 2 \t\t\t ce30\n",
            "9 \t\t\t 21 \t\t\t 2 \t\t\t ce31\n",
            "9 \t\t\t 19 \t\t\t 3 \t\t\t ce32\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ce33\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ce34\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t ce35\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t ce36\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cf01\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cf02\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cf03\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cf04\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cf05\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cf06\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cf07\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cf08\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cf09\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cf10\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cf11\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cf12\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cf13\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cf14\n",
            "9 \t\t\t 27 \t\t\t 2 \t\t\t cf15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cf16\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cf17\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf18\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cf19\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cf20\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cf21\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cf22\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cf23\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cf24\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cf25\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cf26\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cf27\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf28\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf29\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf30\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cf31\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cf32\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cf33\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cf34\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cf35\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cf36\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cf37\n",
            "8 \t\t\t 40 \t\t\t 2 \t\t\t cf38\n",
            "9 \t\t\t 30 \t\t\t 3 \t\t\t cf39\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cf40\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cf41\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cf42\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cf43\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t cf44\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cf45\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t cf46\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cf47\n",
            "8 \t\t\t 38 \t\t\t 3 \t\t\t cf48\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg01\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t cg02\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg03\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cg04\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg05\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg06\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cg07\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cg08\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg09\n",
            "8 \t\t\t 32 \t\t\t 3 \t\t\t cg10\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg11\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cg12\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg13\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cg14\n",
            "8 \t\t\t 27 \t\t\t 3 \t\t\t cg15\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t cg16\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg17\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg18\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg19\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cg20\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cg21\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg22\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cg23\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cg24\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cg25\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cg26\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cg27\n",
            "8 \t\t\t 34 \t\t\t 2 \t\t\t cg28\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cg29\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg30\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cg31\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg32\n",
            "8 \t\t\t 31 \t\t\t 2 \t\t\t cg33\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg34\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg35\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cg36\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cg37\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t cg38\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg39\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg40\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cg41\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t cg42\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t cg43\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cg44\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cg45\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cg46\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cg47\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg48\n",
            "8 \t\t\t 28 \t\t\t 3 \t\t\t cg49\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg50\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg51\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cg52\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg53\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cg54\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cg55\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cg56\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cg57\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cg58\n",
            "8 \t\t\t 46 \t\t\t 2 \t\t\t cg59\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cg60\n",
            "9 \t\t\t 27 \t\t\t 2 \t\t\t cg61\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cg62\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cg63\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t cg64\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg65\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cg66\n",
            "8 \t\t\t 28 \t\t\t 3 \t\t\t cg67\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cg68\n",
            "8 \t\t\t 42 \t\t\t 2 \t\t\t cg69\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cg70\n",
            "8 \t\t\t 31 \t\t\t 2 \t\t\t cg71\n",
            "8 \t\t\t 27 \t\t\t 3 \t\t\t cg72\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cg73\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg74\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cg75\n",
            "9 \t\t\t 16 \t\t\t 3 \t\t\t ch01\n",
            "9 \t\t\t 20 \t\t\t 3 \t\t\t ch02\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ch03\n",
            "9 \t\t\t 18 \t\t\t 3 \t\t\t ch04\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ch05\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ch06\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t ch07\n",
            "9 \t\t\t 28 \t\t\t 3 \t\t\t ch08\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ch09\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t ch10\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ch11\n",
            "8 \t\t\t 30 \t\t\t 5 \t\t\t ch12\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ch13\n",
            "8 \t\t\t 21 \t\t\t 5 \t\t\t ch14\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t ch15\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t ch16\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ch17\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ch18\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t ch19\n",
            "9 \t\t\t 22 \t\t\t 3 \t\t\t ch20\n",
            "9 \t\t\t 25 \t\t\t 3 \t\t\t ch21\n",
            "9 \t\t\t 30 \t\t\t 4 \t\t\t ch22\n",
            "9 \t\t\t 32 \t\t\t 4 \t\t\t ch23\n",
            "8 \t\t\t 19 \t\t\t 4 \t\t\t ch24\n",
            "8 \t\t\t 28 \t\t\t 3 \t\t\t ch25\n",
            "9 \t\t\t 31 \t\t\t 3 \t\t\t ch26\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t ch27\n",
            "9 \t\t\t 19 \t\t\t 3 \t\t\t ch28\n",
            "9 \t\t\t 30 \t\t\t 3 \t\t\t ch29\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ch30\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t cj01\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cj02\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cj03\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cj04\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t cj05\n",
            "9 \t\t\t 26 \t\t\t 3 \t\t\t cj06\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj07\n",
            "9 \t\t\t 19 \t\t\t 2 \t\t\t cj08\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj09\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj10\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cj11\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cj12\n",
            "8 \t\t\t 30 \t\t\t 4 \t\t\t cj13\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t cj14\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t cj15\n",
            "8 \t\t\t 24 \t\t\t 4 \t\t\t cj16\n",
            "9 \t\t\t 26 \t\t\t 3 \t\t\t cj17\n",
            "7 \t\t\t 18 \t\t\t 6 \t\t\t cj18\n",
            "8 \t\t\t 18 \t\t\t 4 \t\t\t cj19\n",
            "8 \t\t\t 20 \t\t\t 5 \t\t\t cj20\n",
            "8 \t\t\t 28 \t\t\t 7 \t\t\t cj21\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t cj22\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cj23\n",
            "9 \t\t\t 34 \t\t\t 2 \t\t\t cj24\n",
            "9 \t\t\t 19 \t\t\t 3 \t\t\t cj25\n",
            "9 \t\t\t 22 \t\t\t 3 \t\t\t cj26\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj27\n",
            "9 \t\t\t 25 \t\t\t 3 \t\t\t cj28\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t cj29\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cj30\n",
            "8 \t\t\t 38 \t\t\t 3 \t\t\t cj31\n",
            "8 \t\t\t 21 \t\t\t 4 \t\t\t cj32\n",
            "9 \t\t\t 23 \t\t\t 4 \t\t\t cj33\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t cj34\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj35\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cj36\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cj37\n",
            "9 \t\t\t 22 \t\t\t 3 \t\t\t cj38\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cj39\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cj40\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cj41\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cj42\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj43\n",
            "9 \t\t\t 28 \t\t\t 3 \t\t\t cj44\n",
            "9 \t\t\t 20 \t\t\t 3 \t\t\t cj45\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cj46\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cj47\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cj48\n",
            "9 \t\t\t 27 \t\t\t 3 \t\t\t cj49\n",
            "8 \t\t\t 33 \t\t\t 3 \t\t\t cj50\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cj51\n",
            "8 \t\t\t 23 \t\t\t 4 \t\t\t cj52\n",
            "8 \t\t\t 32 \t\t\t 3 \t\t\t cj53\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cj54\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cj55\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cj56\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cj57\n",
            "8 \t\t\t 27 \t\t\t 3 \t\t\t cj58\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cj59\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cj60\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cj61\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cj62\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cj63\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cj64\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t cj65\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cj66\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cj67\n",
            "8 \t\t\t 40 \t\t\t 2 \t\t\t cj68\n",
            "8 \t\t\t 22 \t\t\t 4 \t\t\t cj69\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cj70\n",
            "9 \t\t\t 18 \t\t\t 3 \t\t\t cj71\n",
            "9 \t\t\t 21 \t\t\t 2 \t\t\t cj72\n",
            "9 \t\t\t 13 \t\t\t 3 \t\t\t cj73\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t cj74\n",
            "8 \t\t\t 24 \t\t\t 4 \t\t\t cj75\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cj76\n",
            "9 \t\t\t 20 \t\t\t 3 \t\t\t cj77\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cj78\n",
            "8 \t\t\t 23 \t\t\t 4 \t\t\t cj79\n",
            "8 \t\t\t 19 \t\t\t 4 \t\t\t cj80\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t ck01\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t ck02\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t ck03\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ck04\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t ck05\n",
            "8 \t\t\t 12 \t\t\t 2 \t\t\t ck06\n",
            "7 \t\t\t 10 \t\t\t 3 \t\t\t ck07\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t ck08\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t ck09\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t ck10\n",
            "7 \t\t\t 15 \t\t\t 3 \t\t\t ck11\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t ck12\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ck13\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t ck14\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ck15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ck16\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t ck17\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t ck18\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t ck19\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t ck20\n",
            "8 \t\t\t 11 \t\t\t 2 \t\t\t ck21\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ck22\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ck23\n",
            "7 \t\t\t 10 \t\t\t 4 \t\t\t ck24\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t ck25\n",
            "7 \t\t\t 17 \t\t\t 3 \t\t\t ck26\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ck27\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t ck28\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t ck29\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl01\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cl02\n",
            "7 \t\t\t 12 \t\t\t 3 \t\t\t cl03\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl04\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl05\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl06\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cl07\n",
            "7 \t\t\t 17 \t\t\t 3 \t\t\t cl08\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cl09\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl10\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl11\n",
            "8 \t\t\t 12 \t\t\t 2 \t\t\t cl12\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl13\n",
            "7 \t\t\t 12 \t\t\t 3 \t\t\t cl14\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t cl15\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl16\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cl17\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cl18\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl19\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cl20\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t cl21\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cl22\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cl23\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cl24\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cm01\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cm02\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cm03\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cm04\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cm05\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cm06\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cn01\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cn02\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t cn03\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn04\n",
            "7 \t\t\t 12 \t\t\t 3 \t\t\t cn05\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cn06\n",
            "8 \t\t\t 11 \t\t\t 2 \t\t\t cn07\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cn08\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn09\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cn10\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn11\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn12\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cn13\n",
            "8 \t\t\t 14 \t\t\t 2 \t\t\t cn14\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn16\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cn17\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cn18\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cn19\n",
            "8 \t\t\t 14 \t\t\t 2 \t\t\t cn20\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cn21\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cn22\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cn23\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cn24\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cn25\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn26\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cn27\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn28\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cn29\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cp01\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cp02\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cp03\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp04\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cp05\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cp06\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cp07\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cp08\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cp09\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp10\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp11\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cp12\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cp13\n",
            "7 \t\t\t 10 \t\t\t 3 \t\t\t cp14\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cp15\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cp16\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp17\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cp18\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cp19\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cp20\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cp21\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cp22\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cp23\n",
            "7 \t\t\t 10 \t\t\t 4 \t\t\t cp24\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cp25\n",
            "7 \t\t\t 21 \t\t\t 4 \t\t\t cp26\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t cp27\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cp28\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cp29\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cr01\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cr02\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cr03\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cr04\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cr05\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cr06\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cr07\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t cr08\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cr09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#whole code\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "print ('File ids of brown corpus\\n',brown.fileids())\n",
        "'''Let’s pick out the first of these texts — Emma by Jane\n",
        "Austen — and give it a short name, emma, then find out how\n",
        "many words it contains:'''\n",
        "ca01 = brown.words('ca01')\n",
        "# display first few words\n",
        "print('\\nca01 has following words:\\n',ca01)\n",
        "#Total number of words in ca01\n",
        "print('\\nca01 has',len(ca01),'words')\n",
        "#categories or files\n",
        "print ('\\n\\nCategories or file in brown corpus:\\n')\n",
        "print (brown.categories())\n",
        "'''display other information about each text, by looping\n",
        "over all the values of fileid corresponding to the brown\n",
        "file identifiers listed earlier and then computing\n",
        "statistics for each text.'''\n",
        "print ('\\n\\nStatistics for each text:\\n')\n",
        "print\n",
        "('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppearsOnAvg \\t\\tFileName')\n",
        "for fileid in brown.fileids():\n",
        "  num_chars = len(brown.raw(fileid))\n",
        "  num_words = len(brown.words(fileid))\n",
        "  num_sents = len(brown.sents(fileid))\n",
        "  num_vocab = len(set([w.lower() for w in\n",
        "brown.words(fileid)]))\n",
        "  print (int(num_chars/num_words),'\\t\\t\\t',\n",
        "int(num_words/num_sents),'\\t\\t\\t',\n",
        "int(num_words/num_vocab),'\\t\\t\\t', fileid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TtuHgzk9mtG",
        "outputId": "c1dd9732-5f72-40d1-a5dc-bbde518ade39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ids of brown corpus\n",
            " ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n",
            "\n",
            "ca01 has following words:\n",
            " ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "\n",
            "ca01 has 2242 words\n",
            "\n",
            "\n",
            "Categories or file in brown corpus:\n",
            "\n",
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "\n",
            "\n",
            "Statistics for each text:\n",
            "\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ca01\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ca02\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca03\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t ca04\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ca05\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca06\n",
            "9 \t\t\t 18 \t\t\t 2 \t\t\t ca07\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca08\n",
            "9 \t\t\t 19 \t\t\t 2 \t\t\t ca09\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca10\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca11\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca12\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca13\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t ca14\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca15\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca16\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca17\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca18\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca19\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ca20\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t ca21\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca22\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca23\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca24\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca25\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ca26\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca27\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ca28\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca29\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca30\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ca31\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ca32\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca33\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ca34\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t ca35\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t ca36\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca37\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ca38\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ca39\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca40\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca41\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ca42\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ca43\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ca44\n",
            "9 \t\t\t 21 \t\t\t 2 \t\t\t cb01\n",
            "9 \t\t\t 23 \t\t\t 2 \t\t\t cb02\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cb03\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cb04\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cb05\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t cb06\n",
            "9 \t\t\t 24 \t\t\t 2 \t\t\t cb07\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cb08\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cb09\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cb10\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t cb11\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cb12\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t cb13\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cb14\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cb15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cb16\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cb17\n",
            "9 \t\t\t 19 \t\t\t 2 \t\t\t cb18\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cb19\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cb20\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cb21\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cb22\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cb23\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cb24\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cb25\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cb26\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cb27\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cc01\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cc02\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cc03\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc04\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc05\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cc06\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cc07\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cc08\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cc09\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc10\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cc11\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cc12\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cc13\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cc14\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cc15\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cc16\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cc17\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cd01\n",
            "8 \t\t\t 34 \t\t\t 3 \t\t\t cd02\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cd03\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cd04\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t cd05\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cd06\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cd07\n",
            "8 \t\t\t 32 \t\t\t 3 \t\t\t cd08\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cd09\n",
            "9 \t\t\t 23 \t\t\t 2 \t\t\t cd10\n",
            "8 \t\t\t 34 \t\t\t 3 \t\t\t cd11\n",
            "9 \t\t\t 28 \t\t\t 2 \t\t\t cd12\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cd13\n",
            "9 \t\t\t 24 \t\t\t 2 \t\t\t cd14\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cd15\n",
            "8 \t\t\t 14 \t\t\t 4 \t\t\t cd16\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t cd17\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ce01\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ce02\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t ce03\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ce04\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t ce05\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t ce06\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ce07\n",
            "8 \t\t\t 17 \t\t\t 4 \t\t\t ce08\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ce09\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ce10\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ce11\n",
            "9 \t\t\t 18 \t\t\t 2 \t\t\t ce12\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t ce13\n",
            "8 \t\t\t 13 \t\t\t 2 \t\t\t ce14\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t ce15\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ce16\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t ce17\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t ce18\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t ce19\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t ce20\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t ce21\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ce22\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t ce23\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t ce24\n",
            "9 \t\t\t 23 \t\t\t 2 \t\t\t ce25\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ce26\n",
            "8 \t\t\t 12 \t\t\t 4 \t\t\t ce27\n",
            "9 \t\t\t 18 \t\t\t 3 \t\t\t ce28\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t ce29\n",
            "9 \t\t\t 15 \t\t\t 2 \t\t\t ce30\n",
            "9 \t\t\t 21 \t\t\t 2 \t\t\t ce31\n",
            "9 \t\t\t 19 \t\t\t 3 \t\t\t ce32\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t ce33\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ce34\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t ce35\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t ce36\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cf01\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cf02\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cf03\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cf04\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cf05\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cf06\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cf07\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cf08\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cf09\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cf10\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cf11\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cf12\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cf13\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cf14\n",
            "9 \t\t\t 27 \t\t\t 2 \t\t\t cf15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cf16\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cf17\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf18\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cf19\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cf20\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cf21\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cf22\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cf23\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cf24\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cf25\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cf26\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cf27\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf28\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf29\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cf30\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cf31\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cf32\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cf33\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cf34\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t cf35\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cf36\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cf37\n",
            "8 \t\t\t 40 \t\t\t 2 \t\t\t cf38\n",
            "9 \t\t\t 30 \t\t\t 3 \t\t\t cf39\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cf40\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cf41\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cf42\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cf43\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t cf44\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cf45\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t cf46\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cf47\n",
            "8 \t\t\t 38 \t\t\t 3 \t\t\t cf48\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg01\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t cg02\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg03\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cg04\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg05\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg06\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cg07\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cg08\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg09\n",
            "8 \t\t\t 32 \t\t\t 3 \t\t\t cg10\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg11\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cg12\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg13\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cg14\n",
            "8 \t\t\t 27 \t\t\t 3 \t\t\t cg15\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t cg16\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg17\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg18\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg19\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cg20\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cg21\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg22\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cg23\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cg24\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cg25\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cg26\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cg27\n",
            "8 \t\t\t 34 \t\t\t 2 \t\t\t cg28\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cg29\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg30\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cg31\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg32\n",
            "8 \t\t\t 31 \t\t\t 2 \t\t\t cg33\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg34\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg35\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cg36\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cg37\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t cg38\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg39\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cg40\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cg41\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t cg42\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t cg43\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cg44\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cg45\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cg46\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cg47\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg48\n",
            "8 \t\t\t 28 \t\t\t 3 \t\t\t cg49\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg50\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cg51\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cg52\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cg53\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cg54\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cg55\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cg56\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cg57\n",
            "8 \t\t\t 30 \t\t\t 2 \t\t\t cg58\n",
            "8 \t\t\t 46 \t\t\t 2 \t\t\t cg59\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cg60\n",
            "9 \t\t\t 27 \t\t\t 2 \t\t\t cg61\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cg62\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cg63\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t cg64\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg65\n",
            "8 \t\t\t 32 \t\t\t 2 \t\t\t cg66\n",
            "8 \t\t\t 28 \t\t\t 3 \t\t\t cg67\n",
            "8 \t\t\t 29 \t\t\t 2 \t\t\t cg68\n",
            "8 \t\t\t 42 \t\t\t 2 \t\t\t cg69\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cg70\n",
            "8 \t\t\t 31 \t\t\t 2 \t\t\t cg71\n",
            "8 \t\t\t 27 \t\t\t 3 \t\t\t cg72\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cg73\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cg74\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cg75\n",
            "9 \t\t\t 16 \t\t\t 3 \t\t\t ch01\n",
            "9 \t\t\t 20 \t\t\t 3 \t\t\t ch02\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ch03\n",
            "9 \t\t\t 18 \t\t\t 3 \t\t\t ch04\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ch05\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ch06\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t ch07\n",
            "9 \t\t\t 28 \t\t\t 3 \t\t\t ch08\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ch09\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t ch10\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ch11\n",
            "8 \t\t\t 30 \t\t\t 5 \t\t\t ch12\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t ch13\n",
            "8 \t\t\t 21 \t\t\t 5 \t\t\t ch14\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t ch15\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t ch16\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t ch17\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t ch18\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t ch19\n",
            "9 \t\t\t 22 \t\t\t 3 \t\t\t ch20\n",
            "9 \t\t\t 25 \t\t\t 3 \t\t\t ch21\n",
            "9 \t\t\t 30 \t\t\t 4 \t\t\t ch22\n",
            "9 \t\t\t 32 \t\t\t 4 \t\t\t ch23\n",
            "8 \t\t\t 19 \t\t\t 4 \t\t\t ch24\n",
            "8 \t\t\t 28 \t\t\t 3 \t\t\t ch25\n",
            "9 \t\t\t 31 \t\t\t 3 \t\t\t ch26\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t ch27\n",
            "9 \t\t\t 19 \t\t\t 3 \t\t\t ch28\n",
            "9 \t\t\t 30 \t\t\t 3 \t\t\t ch29\n",
            "9 \t\t\t 22 \t\t\t 2 \t\t\t ch30\n",
            "8 \t\t\t 30 \t\t\t 3 \t\t\t cj01\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cj02\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cj03\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cj04\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t cj05\n",
            "9 \t\t\t 26 \t\t\t 3 \t\t\t cj06\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj07\n",
            "9 \t\t\t 19 \t\t\t 2 \t\t\t cj08\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj09\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj10\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cj11\n",
            "8 \t\t\t 22 \t\t\t 3 \t\t\t cj12\n",
            "8 \t\t\t 30 \t\t\t 4 \t\t\t cj13\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t cj14\n",
            "9 \t\t\t 20 \t\t\t 2 \t\t\t cj15\n",
            "8 \t\t\t 24 \t\t\t 4 \t\t\t cj16\n",
            "9 \t\t\t 26 \t\t\t 3 \t\t\t cj17\n",
            "7 \t\t\t 18 \t\t\t 6 \t\t\t cj18\n",
            "8 \t\t\t 18 \t\t\t 4 \t\t\t cj19\n",
            "8 \t\t\t 20 \t\t\t 5 \t\t\t cj20\n",
            "8 \t\t\t 28 \t\t\t 7 \t\t\t cj21\n",
            "9 \t\t\t 25 \t\t\t 2 \t\t\t cj22\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cj23\n",
            "9 \t\t\t 34 \t\t\t 2 \t\t\t cj24\n",
            "9 \t\t\t 19 \t\t\t 3 \t\t\t cj25\n",
            "9 \t\t\t 22 \t\t\t 3 \t\t\t cj26\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj27\n",
            "9 \t\t\t 25 \t\t\t 3 \t\t\t cj28\n",
            "9 \t\t\t 23 \t\t\t 3 \t\t\t cj29\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cj30\n",
            "8 \t\t\t 38 \t\t\t 3 \t\t\t cj31\n",
            "8 \t\t\t 21 \t\t\t 4 \t\t\t cj32\n",
            "9 \t\t\t 23 \t\t\t 4 \t\t\t cj33\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t cj34\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj35\n",
            "8 \t\t\t 27 \t\t\t 2 \t\t\t cj36\n",
            "9 \t\t\t 26 \t\t\t 2 \t\t\t cj37\n",
            "9 \t\t\t 22 \t\t\t 3 \t\t\t cj38\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t cj39\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cj40\n",
            "8 \t\t\t 26 \t\t\t 3 \t\t\t cj41\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cj42\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cj43\n",
            "9 \t\t\t 28 \t\t\t 3 \t\t\t cj44\n",
            "9 \t\t\t 20 \t\t\t 3 \t\t\t cj45\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cj46\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cj47\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cj48\n",
            "9 \t\t\t 27 \t\t\t 3 \t\t\t cj49\n",
            "8 \t\t\t 33 \t\t\t 3 \t\t\t cj50\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cj51\n",
            "8 \t\t\t 23 \t\t\t 4 \t\t\t cj52\n",
            "8 \t\t\t 32 \t\t\t 3 \t\t\t cj53\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cj54\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cj55\n",
            "8 \t\t\t 22 \t\t\t 2 \t\t\t cj56\n",
            "8 \t\t\t 25 \t\t\t 3 \t\t\t cj57\n",
            "8 \t\t\t 27 \t\t\t 3 \t\t\t cj58\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cj59\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cj60\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cj61\n",
            "8 \t\t\t 25 \t\t\t 2 \t\t\t cj62\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t cj63\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cj64\n",
            "8 \t\t\t 29 \t\t\t 3 \t\t\t cj65\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cj66\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cj67\n",
            "8 \t\t\t 40 \t\t\t 2 \t\t\t cj68\n",
            "8 \t\t\t 22 \t\t\t 4 \t\t\t cj69\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cj70\n",
            "9 \t\t\t 18 \t\t\t 3 \t\t\t cj71\n",
            "9 \t\t\t 21 \t\t\t 2 \t\t\t cj72\n",
            "9 \t\t\t 13 \t\t\t 3 \t\t\t cj73\n",
            "9 \t\t\t 21 \t\t\t 3 \t\t\t cj74\n",
            "8 \t\t\t 24 \t\t\t 4 \t\t\t cj75\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cj76\n",
            "9 \t\t\t 20 \t\t\t 3 \t\t\t cj77\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cj78\n",
            "8 \t\t\t 23 \t\t\t 4 \t\t\t cj79\n",
            "8 \t\t\t 19 \t\t\t 4 \t\t\t cj80\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t ck01\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t ck02\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t ck03\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ck04\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t ck05\n",
            "8 \t\t\t 12 \t\t\t 2 \t\t\t ck06\n",
            "7 \t\t\t 10 \t\t\t 3 \t\t\t ck07\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t ck08\n",
            "8 \t\t\t 24 \t\t\t 3 \t\t\t ck09\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t ck10\n",
            "7 \t\t\t 15 \t\t\t 3 \t\t\t ck11\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t ck12\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ck13\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t ck14\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ck15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t ck16\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t ck17\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t ck18\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t ck19\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t ck20\n",
            "8 \t\t\t 11 \t\t\t 2 \t\t\t ck21\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t ck22\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t ck23\n",
            "7 \t\t\t 10 \t\t\t 4 \t\t\t ck24\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t ck25\n",
            "7 \t\t\t 17 \t\t\t 3 \t\t\t ck26\n",
            "8 \t\t\t 24 \t\t\t 2 \t\t\t ck27\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t ck28\n",
            "8 \t\t\t 26 \t\t\t 2 \t\t\t ck29\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl01\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cl02\n",
            "7 \t\t\t 12 \t\t\t 3 \t\t\t cl03\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl04\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl05\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl06\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cl07\n",
            "7 \t\t\t 17 \t\t\t 3 \t\t\t cl08\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cl09\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl10\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl11\n",
            "8 \t\t\t 12 \t\t\t 2 \t\t\t cl12\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl13\n",
            "7 \t\t\t 12 \t\t\t 3 \t\t\t cl14\n",
            "8 \t\t\t 15 \t\t\t 2 \t\t\t cl15\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cl16\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cl17\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cl18\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cl19\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cl20\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t cl21\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cl22\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cl23\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cl24\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cm01\n",
            "8 \t\t\t 15 \t\t\t 3 \t\t\t cm02\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cm03\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cm04\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cm05\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cm06\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cn01\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cn02\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t cn03\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn04\n",
            "7 \t\t\t 12 \t\t\t 3 \t\t\t cn05\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cn06\n",
            "8 \t\t\t 11 \t\t\t 2 \t\t\t cn07\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cn08\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn09\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cn10\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn11\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn12\n",
            "8 \t\t\t 19 \t\t\t 2 \t\t\t cn13\n",
            "8 \t\t\t 14 \t\t\t 2 \t\t\t cn14\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cn15\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn16\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cn17\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cn18\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cn19\n",
            "8 \t\t\t 14 \t\t\t 2 \t\t\t cn20\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cn21\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cn22\n",
            "8 \t\t\t 18 \t\t\t 3 \t\t\t cn23\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cn24\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cn25\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn26\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cn27\n",
            "8 \t\t\t 18 \t\t\t 2 \t\t\t cn28\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cn29\n",
            "8 \t\t\t 17 \t\t\t 3 \t\t\t cp01\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cp02\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cp03\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp04\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cp05\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cp06\n",
            "8 \t\t\t 16 \t\t\t 3 \t\t\t cp07\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cp08\n",
            "8 \t\t\t 31 \t\t\t 3 \t\t\t cp09\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp10\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp11\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cp12\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cp13\n",
            "7 \t\t\t 10 \t\t\t 3 \t\t\t cp14\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cp15\n",
            "8 \t\t\t 12 \t\t\t 3 \t\t\t cp16\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cp17\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cp18\n",
            "7 \t\t\t 14 \t\t\t 3 \t\t\t cp19\n",
            "7 \t\t\t 13 \t\t\t 3 \t\t\t cp20\n",
            "8 \t\t\t 21 \t\t\t 3 \t\t\t cp21\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cp22\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cp23\n",
            "7 \t\t\t 10 \t\t\t 4 \t\t\t cp24\n",
            "8 \t\t\t 23 \t\t\t 3 \t\t\t cp25\n",
            "7 \t\t\t 21 \t\t\t 4 \t\t\t cp26\n",
            "8 \t\t\t 11 \t\t\t 3 \t\t\t cp27\n",
            "8 \t\t\t 14 \t\t\t 3 \t\t\t cp28\n",
            "8 \t\t\t 16 \t\t\t 2 \t\t\t cp29\n",
            "8 \t\t\t 20 \t\t\t 2 \t\t\t cr01\n",
            "8 \t\t\t 21 \t\t\t 2 \t\t\t cr02\n",
            "8 \t\t\t 28 \t\t\t 2 \t\t\t cr03\n",
            "8 \t\t\t 19 \t\t\t 3 \t\t\t cr04\n",
            "8 \t\t\t 17 \t\t\t 2 \t\t\t cr05\n",
            "8 \t\t\t 20 \t\t\t 3 \t\t\t cr06\n",
            "8 \t\t\t 13 \t\t\t 3 \t\t\t cr07\n",
            "8 \t\t\t 33 \t\t\t 2 \t\t\t cr08\n",
            "8 \t\t\t 23 \t\t\t 2 \t\t\t cr09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra\n",
        "import nltk\n",
        "# Download the udhr Corpus\n",
        "nltk.download('udhr')\n",
        "\n",
        "from nltk.corpus import udhr\n",
        "print ('File ids of udhr corpus\\n', udhr.fileids())\n",
        "# Accessing a file from 'udhr' corpus using a valid fileid\n",
        "# Choosing 'Afrikaans-Latin1' file as an example.\n",
        "# Please replace this fileid with the file you want to analyze from the list of udhr.fileids()\n",
        "fileid = 'Afrikaans-Latin1'\n",
        "# Use the fileid to access the words of the udhr corpus\n",
        "words_in_file = udhr.words(fileid)\n",
        "# display first few words\n",
        "print('\\n', fileid, 'has following words:\\n', words_in_file[:20])  # Displaying first 20 words\n",
        "# total number of words in the selected file\n",
        "print('\\n', fileid, 'has', len(words_in_file), 'words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACbBXQzsv86p",
        "outputId": "c687a6d2-d894-40a7-f5a0-04f0cb0d5b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ids of udhr corpus\n",
            " ['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1', 'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1', 'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', 'Amarakaeri-Latin1', 'Amuesha-Yanesha-UTF8', 'Arabela-Latin1', 'Arabic_Alarabia-Arabic', 'Asante-UTF8', 'Ashaninca-Latin1', 'Asheninca-Latin1', 'Asturian_Bable-Latin1', 'Aymara-Latin1', 'Balinese-Latin1', 'Bambara-UTF8', 'Baoule-UTF8', 'Basque_Euskara-Latin1', 'Batonu_Bariba-UTF8', 'Belorus_Belaruski-Cyrillic', 'Belorus_Belaruski-UTF8', 'Bemba-Latin1', 'Bengali-UTF8', 'Beti-UTF8', 'Bichelamar-Latin1', 'Bikol_Bicolano-Latin1', 'Bora-Latin1', 'Bosnian_Bosanski-Cyrillic', 'Bosnian_Bosanski-Latin2', 'Bosnian_Bosanski-UTF8', 'Breton-Latin1', 'Bugisnese-Latin1', 'Bulgarian_Balgarski-Cyrillic', 'Bulgarian_Balgarski-UTF8', 'Cakchiquel-Latin1', 'Campa_Pajonalino-Latin1', 'Candoshi-Shapra-Latin1', 'Caquinte-Latin1', 'Cashibo-Cacataibo-Latin1', 'Cashinahua-Latin1', 'Catalan-Latin1', 'Catalan_Catala-Latin1', 'Cebuano-Latin1', 'Chamorro-Latin1', 'Chayahuita-Latin1', 'Chechewa_Nyanja-Latin1', 'Chickasaw-Latin1', 'Chinanteco-Ajitlan-Latin1', 'Chinanteco-UTF8', 'Chinese_Mandarin-GB2312', 'Chuuk_Trukese-Latin1', 'Cokwe-Latin1', 'Corsican-Latin1', 'Croatian_Hrvatski-Latin2', 'Czech-Latin2', 'Czech-UTF8', 'Czech_Cesky-Latin2', 'Czech_Cesky-UTF8', 'Dagaare-UTF8', 'Dagbani-UTF8', 'Dangme-UTF8', 'Danish_Dansk-Latin1', 'Dendi-UTF8', 'Ditammari-UTF8', 'Dutch_Nederlands-Latin1', 'Edo-Latin1', 'English-Latin1', 'Esperanto-UTF8', 'Estonian_Eesti-Latin1', 'Ewe_Eve-UTF8', 'Fante-UTF8', 'Faroese-Latin1', 'Farsi_Persian-UTF8', 'Farsi_Persian-v2-UTF8', 'Fijian-Latin1', 'Filipino_Tagalog-Latin1', 'Finnish_Suomi-Latin1', 'Fon-UTF8', 'French_Francais-Latin1', 'Frisian-Latin1', 'Friulian_Friulano-Latin1', 'Ga-UTF8', 'Gagauz_Gagauzi-UTF8', 'Galician_Galego-Latin1', 'Garifuna_Garifuna-Latin1', 'German_Deutsch-Latin1', 'Gonja-UTF8', 'Greek_Ellinika-Greek', 'Greek_Ellinika-UTF8', 'Greenlandic_Inuktikut-Latin1', 'Guarani-Latin1', 'Guen_Mina-UTF8', 'HaitianCreole_Kreyol-Latin1', 'HaitianCreole_Popular-Latin1', 'Hani-Latin1', 'Hausa_Haoussa-Latin1', 'Hawaiian-UTF8', 'Hebrew_Ivrit-Hebrew', 'Hebrew_Ivrit-UTF8', 'Hiligaynon-Latin1', 'Hindi-UTF8', 'Hindi_web-UTF8', 'Hmong_Miao-Sichuan-Guizhou-Yunnan-Latin1', 'Hmong_Miao-SouthernEast-Guizhou-Latin1', 'Hmong_Miao_Northern-East-Guizhou-Latin1', 'Hrvatski_Croatian-Latin2', 'Huasteco-Latin1', 'Huitoto_Murui-Latin1', 'Hungarian_Magyar-Latin1', 'Hungarian_Magyar-Latin2', 'Hungarian_Magyar-UTF8', 'Ibibio_Efik-Latin1', 'Icelandic_Yslenska-Latin1', 'Ido-Latin1', 'Igbo-UTF8', 'Iloko_Ilocano-Latin1', 'Indonesian-Latin1', 'Interlingua-Latin1', 'Inuktikut_Greenlandic-Latin1', 'IrishGaelic_Gaeilge-Latin1', 'Italian-Latin1', 'Italian_Italiano-Latin1', 'Japanese_Nihongo-EUC', 'Japanese_Nihongo-SJIS', 'Japanese_Nihongo-UTF8', 'Javanese-Latin1', 'Jola-Fogny_Diola-UTF8', 'Kabye-UTF8', 'Kannada-UTF8', 'Kaonde-Latin1', 'Kapampangan-Latin1', 'Kasem-UTF8', 'Kazakh-Cyrillic', 'Kazakh-UTF8', 'Kiche_Quiche-Latin1', 'Kicongo-Latin1', 'Kimbundu_Mbundu-Latin1', 'Kinyamwezi_Nyamwezi-Latin1', 'Kinyarwanda-Latin1', 'Kituba-Latin1', 'Korean_Hankuko-UTF8', 'Kpelewo-UTF8', 'Krio-UTF8', 'Kurdish-UTF8', 'Lamnso_Lam-nso-UTF8', 'Latin_Latina-Latin1', 'Latin_Latina-v2-Latin1', 'Latvian-Latin1', 'Limba-UTF8', 'Lingala-Latin1', 'Lithuanian_Lietuviskai-Baltic', 'Lozi-Latin1', 'Luba-Kasai_Tshiluba-Latin1', 'Luganda_Ganda-Latin1', 'Lunda_Chokwe-lunda-Latin1', 'Luvale-Latin1', 'Luxembourgish_Letzebuergeusch-Latin1', 'Macedonian-UTF8', 'Madurese-Latin1', 'Makonde-Latin1', 'Malagasy-Latin1', 'Malay_BahasaMelayu-Latin1', 'Maltese-UTF8', 'Mam-Latin1', 'Maninka-UTF8', 'Maori-Latin1', 'Mapudungun_Mapuzgun-Latin1', 'Mapudungun_Mapuzgun-UTF8', 'Marshallese-Latin1', 'Matses-Latin1', 'Mayan_Yucateco-Latin1', 'Mazahua_Jnatrjo-UTF8', 'Mazateco-Latin1', 'Mende-UTF8', 'Mikmaq_Micmac-Mikmaq-Latin1', 'Minangkabau-Latin1', 'Miskito_Miskito-Latin1', 'Mixteco-Latin1', 'Mongolian_Khalkha-Cyrillic', 'Mongolian_Khalkha-UTF8', 'Moore_More-UTF8', 'Nahuatl-Latin1', 'Ndebele-Latin1', 'Nepali-UTF8', 'Ngangela_Nyemba-Latin1', 'NigerianPidginEnglish-Latin1', 'Nomatsiguenga-Latin1', 'NorthernSotho_Pedi-Sepedi-Latin1', 'Norwegian-Latin1', 'Norwegian_Norsk-Bokmal-Latin1', 'Norwegian_Norsk-Nynorsk-Latin1', 'Nyanja_Chechewa-Latin1', 'Nyanja_Chinyanja-Latin1', 'Nzema-UTF8', 'OccitanAuvergnat-Latin1', 'OccitanLanguedocien-Latin1', 'Oromiffa_AfaanOromo-Latin1', 'Osetin_Ossetian-UTF8', 'Oshiwambo_Ndonga-Latin1', 'Otomi_Nahnu-Latin1', 'Paez-Latin1', 'Palauan-Latin1', 'Peuhl-UTF8', 'Picard-Latin1', 'Pipil-Latin1', 'Polish-Latin2', 'Polish_Polski-Latin2', 'Ponapean-Latin1', 'Portuguese_Portugues-Latin1', 'Pulaar-UTF8', 'Punjabi_Panjabi-UTF8', 'Purhepecha-UTF8', 'Qechi_Kekchi-Latin1', 'Quechua-Latin1', 'Quichua-Latin1', 'Rarotongan_MaoriCookIslands-Latin1', 'Rhaeto-Romance_Rumantsch-Latin1', 'Romani-Latin1', 'Romani-UTF8', 'Romanian-Latin2', 'Romanian_Romana-Latin2', 'Rukonzo_Konjo-Latin1', 'Rundi_Kirundi-Latin1', 'Runyankore-rukiga_Nkore-kiga-Latin1', 'Russian-Cyrillic', 'Russian-UTF8', 'Russian_Russky-Cyrillic', 'Russian_Russky-UTF8', 'Sami_Lappish-UTF8', 'Sammarinese-Latin1', 'Samoan-Latin1', 'Sango_Sangho-Latin1', 'Sanskrit-UTF8', 'Saraiki-UTF8', 'Sardinian-Latin1', 'ScottishGaelic_GaidhligAlbanach-Latin1', 'Seereer-UTF8', 'Serbian_Srpski-Cyrillic', 'Serbian_Srpski-Latin2', 'Serbian_Srpski-UTF8', 'Sharanahua-Latin1', 'Shipibo-Conibo-Latin1', 'Shona-Latin1', 'Sinhala-UTF8', 'Siswati-Latin1', 'Slovak-Latin2', 'Slovak_Slovencina-Latin2', 'Slovenian_Slovenscina-Latin2', 'SolomonsPidgin_Pijin-Latin1', 'Somali-Latin1', 'Soninke_Soninkanxaane-UTF8', 'Sorbian-Latin2', 'SouthernSotho_Sotho-Sesotho-Sutu-Sesutu-Latin1', 'Spanish-Latin1', 'Spanish_Espanol-Latin1', 'Sukuma-Latin1', 'Sundanese-Latin1', 'Sussu_Soussou-Sosso-Soso-Susu-UTF8', 'Swaheli-Latin1', 'Swahili_Kiswahili-Latin1', 'Swedish_Svenska-Latin1', 'Tahitian-UTF8', 'Tenek_Huasteco-Latin1', 'Tetum-Latin1', 'Themne_Temne-UTF8', 'Tiv-Latin1', 'Toba-UTF8', 'Tojol-abal-Latin1', 'TokPisin-Latin1', 'Tonga-Latin1', 'Tongan_Tonga-Latin1', 'Totonaco-Latin1', 'Trukese_Chuuk-Latin1', 'Turkish_Turkce-Turkish', 'Turkish_Turkce-UTF8', 'Tzeltal-Latin1', 'Tzotzil-Latin1', 'Uighur_Uyghur-Latin1', 'Uighur_Uyghur-UTF8', 'Ukrainian-Cyrillic', 'Ukrainian-UTF8', 'Umbundu-Latin1', 'Urarina-Latin1', 'Uzbek-Latin1', 'Vietnamese-ALRN-UTF8', 'Vietnamese-UTF8', 'Vlach-Latin1', 'Walloon_Wallon-Latin1', 'Wama-UTF8', 'Waray-Latin1', 'Wayuu-Latin1', 'Welsh_Cymraeg-Latin1', 'WesternSotho_Tswana-Setswana-Latin1', 'Wolof-Latin1', 'Xhosa-Latin1', 'Yagua-Latin1', 'Yao-Latin1', 'Yapese-Latin1', 'Yoruba-UTF8', 'Zapoteco-Latin1', 'Zapoteco-SanLucasQuiavini-Latin1', 'Zhuang-Latin1', 'Zulu-Latin1']\n",
            "\n",
            " Afrikaans-Latin1 has following words:\n",
            " ['UNIVERSELE', 'VERKLARING', 'VAN', 'MENSEREGTE', 'Aanhef', 'AANGESIEN', 'erkenning', 'vir', 'die', 'inherente', 'waardigheid', 'en', 'die', 'gelyke', 'en', 'onvervreembare', 'reg', 'van', 'alle', 'lede']\n",
            "\n",
            " Afrikaans-Latin1 has 1807 words\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/udhr.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "# Download the 'inaugural' corpus\n",
        "nltk.download('inaugural')  # Download the corpus if you haven't already\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # This line downloads the required data for sentence tokenization\n",
        "\n",
        "# 1. Accessing basic information\n",
        "print(\"Number of files in the Inaugural Address Corpus:\", len(inaugural.fileids()))\n",
        "print(\"File IDs:\", inaugural.fileids())\n",
        "\n",
        "# 2. Raw text\n",
        "print(\"\\nFirst 100 characters of the 1789 inaugural address:\")\n",
        "print(inaugural.raw('1789-Washington.txt')[:100])\n",
        "\n",
        "# 3. Words\n",
        "words = inaugural.words('1789-Washington.txt')\n",
        "print(\"\\nFirst 10 words of the 1789 inaugural address:\")\n",
        "print(words[:10])\n",
        "print(\"\\nTotal word count in 1789 inaugural address:\", len(words))\n",
        "\n",
        "# 4. Sentences\n",
        "sentences = inaugural.sents('1789-Washington.txt') # This line should now work correctly\n",
        "print(\"\\nFirst 2 sentences of the 1789 inaugural address:\")\n",
        "print(sentences[:2])\n",
        "print(\"\\nTotal sentence count in 1789 inaugural address:\", len(sentences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PILYZRDd4IlJ",
        "outputId": "bd894ae2-46a1-4e2b-d112-48f343eb8e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the Inaugural Address Corpus: 60\n",
            "File IDs: ['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', '2021-Biden.txt', '2025-Trump.txt']\n",
            "\n",
            "First 100 characters of the 1789 inaugural address:\n",
            "Fellow-Citizens of the Senate and of the House of Representatives:\n",
            "\n",
            "Among the vicissitudes incident \n",
            "\n",
            "First 10 words of the 1789 inaugural address:\n",
            "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House']\n",
            "\n",
            "Total word count in 1789 inaugural address: 1538\n",
            "\n",
            "First 2 sentences of the 1789 inaugural address:\n",
            "[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], ['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.']]\n",
            "\n",
            "Total sentence count in 1789 inaugural address: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** b. Create and use your own corpora (plaintext, categorical)"
      ],
      "metadata": {
        "id": "M4ZfGKY0Zr3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "from nltk.tokenize import PunktSentenceTokenizer # import PunktSentenceTokenizer\n",
        "# Define the path to your corpus root directory\n",
        "corpus_root = r'/content/my_corpus' # Update this path as needed\n",
        "# Use a raw string for the path\n",
        "filelist = PlaintextCorpusReader(corpus_root, '.*')\n",
        "# Create an instance of PunktSentenceTokenizer\n",
        "sent_tokenizer = PunktSentenceTokenizer()\n",
        "print('\\n File list: \\n')\n",
        "print(filelist.fileids())\n",
        "print(filelist.root)\n",
        "# Display other information about each text\n",
        "print('\\n\\nStatistics for each text:\\n')\n",
        "print('AvgWordLen\\tAvgSentenceLen\\tno.ofTimesEachWordAppears OnAvg\\tFileName')\n",
        "for fileid in filelist.fileids():\n",
        "  try:\n",
        "    num_chars = len(filelist.raw(fileid))\n",
        "    num_words = len(filelist.words(fileid))\n",
        "# Use sent_tokenizer to tokenize the raw text into sentences\n",
        "    num_sents = len(sent_tokenizer.tokenize(filelist.raw(fileid)))\n",
        "    num_vocab = len(set([w.lower() for w in filelist.words(fileid)]))\n",
        "    if num_words > 0 and num_sents > 0 and num_vocab > 0:\n",
        "      avg_word_len = int(num_chars / num_words)\n",
        "      avg_sentence_len = int(num_words / num_sents)\n",
        "      avg_word_appearances = int(num_words / num_vocab)\n",
        "      print(f\"{avg_word_len}\\t\\t\\t{avg_sentence_len}\\t\\t\\t{avg_word_appearances}\\t\\t\\t{fileid}\")\n",
        "    else:\n",
        "      print(f\"Skipping {fileid} due to zero word count or sentence count.\")\n",
        "  except ZeroDivisionError as e:\n",
        "    print(f\"Error calculating statistics for {fileid}: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred while processing {fileid}: {e}\") #Fixed indentation of except blocks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkKHBN59Fo42",
        "outputId": "6fdc2016-f1ee-431b-b2e9-18f55a477733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " File list: \n",
            "\n",
            "['file1.txt', 'file2.txt', 'intro.txt', 'keypoints.txt']\n",
            "/content/my_corpus\n",
            "\n",
            "\n",
            "Statistics for each text:\n",
            "\n",
            "AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppears OnAvg\tFileName\n",
            "4\t\t\t6\t\t\t1\t\t\tfile1.txt\n",
            "5\t\t\t6\t\t\t1\t\t\tfile2.txt\n",
            "4\t\t\t15\t\t\t1\t\t\tintro.txt\n",
            "5\t\t\t12\t\t\t1\t\t\tkeypoints.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create 3/4 txt files, create a main folder in that uplaod files"
      ],
      "metadata": {
        "id": "-xJR3AN8GxtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** c.Study Conditional frequency distributions"
      ],
      "metadata": {
        "id": "in-qmMgSZLxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the Brown Corpus\n",
        "nltk.download('brown')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p66bFtDfYCEI",
        "outputId": "7169be61-40a4-4706-cb1e-4cdadb5543a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#process a sequence of pairs\n",
        "text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
        "pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "fd = nltk.ConditionalFreqDist((genre, word)\n",
        "  for genre in brown.categories()\n",
        "  for word in brown.words(categories=genre))\n",
        "genre_word = [(genre, word)\n",
        "  for genre in ['news', 'romance']\n",
        "  for word in brown.words(categories=genre)]\n",
        "print(len(genre_word))\n",
        "print(genre_word[:4])\n",
        "print(genre_word[-4:])\n",
        "cfd = nltk.ConditionalFreqDist(genre_word)\n",
        "print(cfd)\n",
        "print(cfd.conditions())\n",
        "print(cfd['news'])\n",
        "print(cfd['romance'])\n",
        "print(list(cfd['romance']))\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "# Download the 'inaugural' corpus\n",
        "nltk.download('inaugural') # Added this line to download the corpus data\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist((target, fileid[:4])\n",
        "  for fileid in inaugural.fileids()\n",
        "  for w in inaugural.words(fileid)\n",
        "  for target in ['america','citizen']\n",
        "  if w.lower().startswith(target))\n",
        "from nltk.corpus import udhr\n",
        "\n",
        "# Download the 'udhr' corpus\n",
        "nltk.download('udhr') # Added this line to download the corpus data\n",
        "\n",
        "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
        "'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
        "cfd = nltk.ConditionalFreqDist((lang, len(word))\n",
        "  for lang in languages\n",
        "  for word in udhr.words(lang + '-Latin1'))\n",
        "cfd.tabulate(conditions=['English', 'German_Deutsch'],samples=range(10), cumulative=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n7ot8_zYf6-",
        "outputId": "faa3cfc1-1183-4cba-f8cc-ef6bbd2cc1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170576\n",
            "[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')]\n",
            "[('romance', 'afraid'), ('romance', 'not'), ('romance', \"''\"), ('romance', '.')]\n",
            "<ConditionalFreqDist with 2 conditions>\n",
            "['news', 'romance']\n",
            "<FreqDist with 14394 samples and 100554 outcomes>\n",
            "<FreqDist with 8452 samples and 70022 outcomes>\n",
            "[',', '.', 'the', 'and', 'to', 'a', 'of', '``', \"''\", 'was', 'I', 'in', 'he', 'had', '?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him', 'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', 'out', 'were', 'up', 'all', 'from', 'could', 'me', 'like', 'been', 'so', 'there', 'they', 'one', 'about', 'my', 'an', 'or', 'is', 'this', 'It', 'them', 'if', 'into', 'But', 'And', 'down', 'when', 'back', 'no', 'what', 'did', 'their', 'do', 'by', 'only', 'your', 'thought', 'which', 'You', \"didn't\", 'then', 'just', 'little', 'time', 'too', 'get', 'who', 'got', 'before', 'know', 'over', 'man', 'because', 'more', 'never', 'way', 'now', 'went', 'we', \"I'm\", 'eyes', 'go', 'came', 'see', 'can', 'old', 'come', 'even', 'are', 'looked', 'other', 'They', 'its', 'knew', 'some', 'much', 'around', 'any', 'There', 'here', 'long', 'than', 'good', 'away', 'felt', 'day', 'own', 'still', 'made', 'take', \"don't\", 'say', 'going', 'how', 'something', 'after', 'through', ':', 'off', 'think', 'In', 'right', 'night', 'where', 'look', 'those', 'again', 'himself', \"I'll\", 'thing', 'first', 'might', 'seemed', 'life', 'very', 'What', \"wasn't\", 'always', 'left', 'make', 'young', 'put', 'being', 'people', 'while', 'took', 'two', 'turned', 'A', 'nothing', 'saw', 'told', 'head', \"couldn't\", 'home', 'asked', 'place', 'room', 'must', 'His', 'mother', 'face', 'wanted', 'last', 'Phil', 'door', 'next', 'will', 'against', 'anything', 'us', 'Then', 'No', 'herself', 'enough', 'morning', 'let', 'Mrs.', 'John', 'once', 'This', 'boy', 'really', 'well', 'tell', 'When', 'few', 'stood', 'want', 'looking', 'course', 'house', 'big', 'feel', 'hand', 'ever', 'woman', 'why', 'Well', 'find', 'until', 'cold', 'kind', 'water', 'years', 'voice', \"wouldn't\", 'son', 'All', 'Mr.', 'along', \"I'd\", 'black', 'gave', 'sat', 'work', 'better', 'should', 'days', 'love', 'called', 'new', 'For', 'heard', 'small', 'We', 'hands', 'these', 'without', 'same', 'white', 'hair', 'sure', 'great', 'things', 'Lucy', 'church', 'men', 'That', 'else', 'though', 'At', 'Her', 'done', 'found', \"hadn't\", 'Now', 'both', 'Just', \"It's\", 'give', 'Why', 'If', 'Miss', 'Mike', 'everything', 'many', \"I've\", 'moment', 'walked', 'myself', 'job', 'Cady', 'kept', 'girl', 'clothes', 'keep', 'has', 'world', 'another', 'most', 'baby', 'stopped', 'beautiful', 'together', 'our', 'gone', 'Yes', 'pale', 'talk', 'Linda', 'Johnnie', 'each', 'light', 'having', 'money', \"can't\", 'leave', 'thinking', 'Oh', \"Don't\", 'end', 'call', 'field', 'help', 'alone', 'mind', 'Myra', 'Theresa', 'Not', 'smiled', 'began', 'sun', 'sound', 'Anne', 'father', 'every', 'trying', 'bed', 'whole', 'family', 'idea', 'God', 'wrong', 'Wally', 'women', 'toward', 'Maggie', 'started', 'stay', 'quite', 'tried', 'wish', 'between', \"she'd\", 'suddenly', 'slowly', 'Spencer', 'Bobbie', 'Martin', 'Cousin', 'Eddie', 'Deegan', 'early', \"that's\", 'wife', '(', ')', 'yet', 'such', 'feet', 'used', 'feeling', 'My', 'business', 'weeks', 'George', 'snake', 'watching', 'seen', 'children', 'high', 'doing', \"isn't\", 'am', 'already', 'Henrietta', 'Owen', 'dark', 'Alexander', 'As', 'car', 'Susan', 'Freddy', 'since', 'happened', 'pretty', 'taking', 'care', 'coffee', 'lived', 'table', 'blue', 'body', 'sitting', 'sorry', 'turn', 'full', 'almost', 'brought', 'across', 'air', 'later', 'letter', 'Cathy', 'Dolores', 'given', 'part', 'matter', 'nice', 'fine', 'saying', 'So', \"won't\", 'open', 'week', \"That's\", 'Sam', 'ball', 'tractor', 'shorts', 'William', 'Old', 'above', 'front', 'making', 'side', 'pink', 'rest', 'lot', 'child', 'set', 'three', 'half', 'talking', 'How', 'rather', 'word', 'couple', 'true', 'clear', 'heart', 'bright', 'real', 'Nadine', 'Richard', 'stop', 'upon', 'hear', 'One', \"You're\", 'close', 'picked', 'hard', 'run', 'getting', \"he'd\", 'loved', 'hours', 'Of', 'beside', 'breakfast', 'held', 'chance', 'standing', \"you're\", 'Rachel', 'waited', 'Hanford', 'Willis', 'To', 'Or', 'past', 'dead', 'behind', 'town', 'waiting', 'also', 'perhaps', 'Eugenia', 'New', 'second', 'need', 'talked', 'red', 'caught', 'moved', 'far', 'hot', 'school', 'age', 'girls', 'bad', 'maybe', 'Poor', 'Anniston', 'evening', 'street', 'remember', 'stared', 'times', 'words', 'dropped', 'instead', 'American', 'under', 'fell', 'ought', 'use', \"'\", 'Doc', 'year', 'yourself', \"He's\", 'Japanese', 'Tommy', 'believe', 'understand', 'soon', 'listening', 'rock', 'apartment', 'Edythe', 'refrigerator', 'Man', 'name', 'With', 'fingers', 'yellow', 'color', 'Even', 'heavy', 'arms', 'lost', 'opened', 'company', 'office', 'York', 'everybody', 'laughed', 'floor', \"it's\", 'doctor', 'ran', 'live', \"haven't\", 'read', 'coming', 'note', 'On', 'certain', 'remembered', 'seeing', 'friends', 'wondered', 'swimming', 'probably', 'realized', 'Cromwell', 'large', 'boys', 'straight', 'quietly', 'Carla', 'Jim', 'liked', 'sounds', 'tiny', 'Did', 'hold', 'less', \"doesn't\", 'death', 'afternoon', 'Do', 'neck', 'sick', 'kitchen', 'bedroom', 'bit', 'brown', 'shoes', 'meeting', 'fire', 'spoke', 'best', 'hour', 'party', 'met', 'walking', 'rain', 'gray', 'short', 'move', 'green', 'Navy', 'Maybe', 'least', 'college', 'added', 'guy', 'interest', 'kill', 'late', 'Charlie', 'catcher', 'Frankie', 'six', 'tongue', 'nobody', 'thin', 'anyone', 'silent', 'steps', 'inside', 'running', 'ten', 'months', 'answer', 'within', 'eye', 'knowing', 'mouth', 'watch', 'tired', 'Chris', 'living', 'worry', 'during', 'outside', 'war', 'wore', 'meet', 'strange', 'stayed', 'dressed', 'lips', 'simply', 'lay', 'meant', 'handsome', 'sent', 'surprised', 'smile', 'able', 'someone', 'After', 'does', 'stick', 'different', 'today', 'hall', 'reason', 'may', 'finally', 'group', 'among', 'speak', 'shouted', 'ready', 'taken', 'afraid', 'colors', 'Quint', 'Pete', 'Lucille', 'Vivian', 'country', 'except', \"shouldn't\", 'known', 'city', 'low', 'walls', 'alive', 'sleep', 'case', 'leaned', 'wet', \"She's\", 'warm', 'wide', 'ones', 'seem', 'Grandma', \"Let's\", 'government', 'sort', 'forever', 'teeth', 'anyway', 'married', 'yes', 'followed', 'drove', 'line', 'hope', 'promise', 'drank', 'please', 'try', 'husband', 'wished', 'wait', 'till', 'sea', 'passed', 'clean', 'drink', 'ahead', 'cry', 'sometimes', 'mean', 'visit', 'tomorrow', 'book', 'walk', 'themselves', 'drew', 'wonder', 'needed', 'innocent', 'ask', 'Your', 'hell', 'Via', 'Paris', 'beach', 'manager', 'pitcher', 'Dave', 'tone', 'Yet', 'thick', 'present', 'Once', 'carry', 'paid', 'taste', \"man's\", 'Perhaps', 'spent', 'rose', 'window', 'beyond', 'threw', 'either', 'bring', 'whose', 'beginning', 'near', 'corner', 'hearing', 'food', 'moments', 'strong', 'changed', 'stairs', 'dress', 'settled', \"weren't\", 'supposed', 'Something', 'start', 'trip', 'person', \"she's\", 'hardly', 'class', 'others', 'imagine', 'show', 'nodded', 'beard', 'possible', 'answered', 'regular', 'jacket', 'turning', 'trouble', 'Nobody', 'minutes', 'religion', 'luck', 'game', 'longer', 'Captain', 'paper', 'knife', 'stand', 'returned', 'anger', 'Nothing', 'bench', 'ago', 'cool', 'crazy', 'hate', 'shut', \"you've\", 'decided', \"What's\", 'ugly', 'scared', 'president', 'Elec', 'Warren', 'club', 'biwa', 'Partlow', 'broken', 'sweet', 'rising', 'dog', 'smelled', 'wall', 'shop', 'Only', 'top', 'pay', 'fifteen', 'order', \"There's\", 'dream', 'quickly', 'ground', 'happy', 'shook', 'strength', 'plenty', 'Evans', 'laughing', 'working', 'truth', 'bottle', 'evil', 'burst', 'wearing', 'poor', \"you'll\", 'nearly', 'forth', 'Robards', 'Let', 'dear', 'difficult', 'change', 'marriage', 'forward', 'stomach', 'special', 'spirits', 'summer', \"We'll\", 'notice', 'Doaty', 'flowers', 'play', 'pulled', 'sense', 'mine', 'earth', 'necessary', 'sky', 'impossible', 'hospital', 'English', 'faces', 'image', 'considered', 'Buzz', 'worked', 'happen', 'fashion', 'bag', 'shirt', 'Is', 'quiet', 'guess', 'fortune', 'doubt', 'Here', 'four', 'building', 'shrugged', 'Gratt', 'somebody', 'asleep', 'picture', 'painting', 'Salter', 'Rome', 'Look', 'lodge', 'mad', 'sharp', 'phone', 'Dad', 'Cooper', \"John's\", 'Alice', 'Charlotte', 'Ryusenji', 'temple', 'Hamrick', 'neither', 'pushed', 'distance', 'touch', 'shoulders', 'climbed', 'sister', 'Italy', 'foot', 'center', 'trees', 'glass', 'porch', 'holding', 'named', 'smoke', 'older', 'leg', 'pointed', 'showed', 'lines', 'somewhat', 'shining', 'space', 'figure', 'pleasure', 'flesh', 'became', 'comfortable', 'carried', 'round', 'figures', 'minute', 'twenty', 'stock', 'fast', 'somehow', 'begged', 'hoped', 'die', 'anybody', 'Jenny', 'nose', 'weather', 'marry', 'died', 'peace', 'often', 'speaking', 'indeed', 'concern', 'worried', 'enjoyed', 'glad', 'folks', 'send', 'funny', 'safe', 'daughter', 'garden', 'duty', 'whatever', 'narrow', 'An', 'understood', 'Come', 'eating', 'arm', 'grow', 'leaving', 'letters', 'watched', 'main', 'mirror', 'says', 'wonderful', 'ship', 'Riverside', 'dinner', 'explained', 'suppose', 'desk', 'hotel', 'conversation', 'smiling', 'dollars', 'slightly', \"he's\", 'studied', 'carrying', 'entered', 'Heiser', 'further', 'seven', 'From', 'grinned', 'fight', 'hurt', 'naked', 'suit', 'whether', 'wants', 'Donald', 'By', 'Burns', 'reading', 'fog', 'length', 'busy', 'Julia', 'Julie', 'Sabella', 'Mousie', 'Ken', 'fall', 'Emma', 'Kirby', 'sweater', 'lotion', 'dugout', 'Fudo', 'College', 'leadership', 'David', 'rang', 'month', 'parents', 'orange', 'beneath', 'become', 'onto', 'direction', 'soul', 'guessed', 'memory', 'houses', 'cemetery', 'rich', 'sit', 'service', 'pocket', 'pipe', 'chest', 'sign', 'reached', 'moving', 'Suddenly', 'sight', 'skin', 'sake', 'rolled', 'slid', 'glance', 'expected', 'closer', 'Stuart', 'social', 'insisted', 'keeping', 'envelope', 'Every', 'bread', 'sighed', 'jobs', 'wondering', 'grew', 'desperate', 'remained', 'hated', 'worse', 'Lord', 'suspected', 'Tolley', \"He'd\", 'fact', 'laugh', 'none', 'write', \"You've\", 'spring', 'Dr.', 'damn', 'horse', 'horses', 'Gunny', 'Jen', 'quick', 'suggested', 'pleased', 'tears', 'unhappy', 'giving', \"you'd\", 'return', 'driving', 'shame', 'Have', 'Club', 'ideas', \"Doaty's\", 'knowledge', 'curious', 'music', 'stone', 'sad', 'growing', 'Outside', 'Go', 'sounded', 'interested', 'cottage', 'terrible', \"Can't\", 'places', 'reasons', 'Probably', 'limp', 'pick', 'defense', 'Who', 'voices', 'easy', 'grown', 'soft', 'ward', 'approach', 'stag', 'admit', 'finished', 'road', 'played', 'bottom', 'glasses', 'telling', 'raised', 'books', 'usual', 'five', 'appeared', 'blonde', 'dance', 'Somers', 'Gansevoort', 'sir', \"Spencer's\", 'missing', 'Are', 'cannot', 'coat', 'teacher', 'fighting', 'Sometimes', 'deep', 'Out', 'hit', 'kid', 'Shafer', 'wedding', 'drive', 'ham', 'truck', 'miserable', 'drunk', 'looks', 'closet', 'advertising', 'rooms', 'kids', 'studio', 'younger', 'bitter', 'attention', 'guests', 'habit', 'Was', 'cried', 'decision', 'funeral', 'bathroom', 'Pope', 'breath', 'whip', 'yours', 'future', 'wind', 'flat', 'Will', 'dolls', 'guest', 'Stop', 'Victoria', 'honest', 'Gladdy', 'cut', 'beauty', 'stake', 'Alma', 'pill', 'peaked', 'team', 'Lee', 'Rossoff', 'student', 'Tokyo', 'interests', 'science', 'Adam', 'nor', 'streets', 'missed', 'unusual', 'youth', 'Sunday', 'passing', 'Street', 'wine', 'passion', 'cared', 'park', 'playing', 'cards', 'neighborhood', 'wood', 'owned', 'whom', 'price', 'step', 'Pompeii', 'ride', 'calling', 'stuff', 'Laura', 'toes', 'mood', 'milk', 'smooth', 'brother', 'reach', 'filled', 'excitement', 'proud', 'Little', 'fat', 'developed', 'laid', \"father's\", 'bought', 'plain', 'stove', 'snap', \"let's\", 'sleeping', 'fair', 'relief', 'furnace', 'giant', 'heater', 'camp', 'worries', 'proved', 'burned', 'mentioned', 'Kizzie', 'pictures', 'simple', 'Frank', \"They're\", 'fault', 'exactly', 'lie', 'parties', 'touching', 'fool', 'animal', 'sympathy', 'humor', 'winter', 'rise', 'Those', 'box', 'friendly', 'land', 'hat', 'Adelia', 'Their', 'dancing', 'Papa', 'circle', 'question', 'birds', 'chair', 'nature', 'slipped', 'human', 'thirty', 'impulse', 'fellow', 'captain', 'size', \"there's\", 'harm', 'beat', 'beer', \"You'll\", 'led', 'fish', 'edge', 'gathering', 'principal', 'blood', 'average', 'sunburn', 'Japan', 'putting', 'San', 'patients', 'tight', 'anywhere', 'check', 'angry', 'promised', 'Actually', 'mast', 'chief', 'Base', 'telephone', 'Whitey', \"we're\", 'Parker', 'nervous', 'wildly', 'aware', 'forget', 'helped', 'lawyer', 'explain', 'easily', \"o'clock\", 'eat', 'announced', 'redhead', 'checks', 'ordered', 'Philip', 'letting', 'prisoners', 'listen', 'Three', 'rushed', 'mass', 'study', 'market', 'stepped', 'Europe', 'brilliant', 'follow', 'Reuveni', 'weight', 'difference', 'dying', 'While', 'lean', 'store', 'written', 'handle', 'eleven', 'various', 'actually', 'usually', 'paint', 'Askington', 'profession', 'national', 'twelve', 'point', 'extreme', 'Ah', 'religious', 'path', 'itself', 'friend', 'drop', 'broad', 'pair', 'aside', 'wake', 'stuck', 'nephew', 'happiness', 'stumbled', 'Walitzee', 'pain', 'shouting', 'darkness', 'hollow', 'ears', 'discovered', 'Mother', 'mention', 'Please', 'goes', 'character', 'sand', 'chosen', 'bathing', 'snapped', 'Bill', 'tonight', 'John-and-Linda', 'pool', 'Jack', 'Janice', 'Thom', 'gotten', 'tough', 'lots', 'Joe', 'carefully', 'Tuxapoka', 'Can', 'League', 'salami', 'Tom', 'plate', 'players', 'god', 'Acala', 'furrow', 'snakes', 'motivation', 'electronics', 'situation', 'A-Z', 'Zenith', 'Richert', 'disliked', 'bell', 'noon', 'breaking', 'buzzing', 'screen', 'June', 'below', 'female', 'heat', 'although', 'quarters', 'Saturday', 'hundred', 'suitcase', 'rode', 'swayed', 'Rose', 'spun', 'hatred', 'square', 'hill', 'Love', 'fear', 'coolness', 'brief', 'cane', 'remove', 'Hey', 'swung', 'interrupted', \"ain't\", \"boy's\", 'due', 'shaved', 'bear', 'flying', 'dry', 'deal', 'juice', 'washing', 'odd', \"baby's\", \"She'll\", 'nine', 'interesting', 'ashamed', 'wishes', 'appearance', 'grim', 'buildings', 'poverty', 'regarded', 'East', 'circumstances', 'bills', 'soles', 'dropping', 'cracks', 'range', 'important', 'ice', 'talent', 'Christian', 'especially', 'Hope', 'crisp', 'Laban', 'Roy', 'Zion', \"it'll\", 'favorite', 'unable', 'opening', 'pot', 'Surely', 'Quinzaine', 'poured', 'bother', 'Dunne', 'ways', 'manner', 'danger', 'curled', 'lack', 'expensive', 'showing', \"'em\", 'mighty', 'legs', 'More', 'welcome', 'nurse', 'fit', 'crowd', 'gets', 'See', 'Mama', 'Charles', 'choice', 'awake', 'forgotten', 'Everything', 'closed', 'swelling', 'bare', 'downstairs', 'Rosa', 'tied', 'danced', 'branches', 'offered', 'plan', 'straightened', 'tissues', 'island', 'blame', 'defend', 'ancient', 'bent', \"day's\", 'wild', 'covered', 'hoping', 'tourist', 'trade', 'Most', 'lovely', 'Diego', 'serious', 'States', 'involved', 'drinking', 'obviously', 'lieutenant', 'rage', 'Take', 'training', 'accepted', 'language', 'Two', 'experienced', 'America', 'male', 'terribly', 'key', 'natural', 'certainly', 'member', 'opinion', 'entirely', 'delight', 'hills', 'taxi', 'smart', \"one's\", 'Listen', 'Gertrude', 'frowned', 'lay-sisters', 'effect', 'admitted', 'miles', 'Pacific', 'plays', 'excuse', 'card', 'ate', 'Although', 'crowded', 'combination', 'won', 'champagne', 'dirty', 'arranged', 'authentic', 'Five', 'everyone', 'fourteen', \"We're\", 'broke', 'lump', 'murder', 'cabin', 'news', 'Where', 'deck', 'brace', 'sending', 'signal', 'begin', 'placed', 'joined', 'aboard', 'force', 'number', 'Besides', 'lead', 'positive', 'lose', 'dignity', 'plot', 'agreed', 'hesitate', 'guilty', 'Some', 'alley', 'trousers', 'tightly', 'swinging', 'Like', 'stupid', 'Israel', 'chose', 'nerve', 'relaxed', 'refugee', 'continued', 'skinny', 'imagined', 'gang', 'board', 'maid', 'thinks', 'tire', 'crying', 'damned', 'diamond', 'makes', 'means', 'share', 'gift', 'St.', 'courage', 'chill', 'opposite', 'sales', 'sold', 'doorway', 'painted', 'paintings', 'easel', 'admired', 'joy', 'pure', 'eighteen', 'lemon', 'Dolly', 'assumed', \"women's\", 'lunch', 'equal', 'final', 'release', 'Everyone', 'value', 'advice', 'darling', 'Father', 'arrived', 'knees', 'fountain', 'midnight', 'woke', \"mother's\", 'worrying', 'lady', 'Agnese', 'folded', 'experience', 'Early', 'Spring', 'Pile', 'Clouds', 'spat', 'dreamed', 'damp', 'staring', 'circles', 'Rawlings', 'Millie', 'slept', 'plane', 'vivid', \"aren't\", 'receiver', 'packed', \"Myra's\", 'widow', 'Sure', 'rocks', 'perfect', 'waves', 'yelled', 'cough', 'Better', \"we'll\", 'earlier', 'ruined', 'Chandler', 'steel', 'bride', 'suits', 'blessing', 'managed', 'Francie', 'possibly', 'throw', 'cars', 'position', \"we'd\", 'bureau', 'story', 'anyhow', 'Dick', 'Seven', 'Bancroft', 'Groggins', 'guys', 'sex', 'snow', 'Stubblefield', 'arched', 'Stubblefields', 'pills', 'Somebody', 'spread', 'Always', 'inning', 'bat', 'dressing', 'seated', 'Ricco', 'gonna', 'towards', 'baseball', 'Samuel', 'plow', 'killed', 'soil', 'Good', 'engineer', \"hasn't\", 'Gerry', \"Freddy's\", 'Allstates', 'Wisconsin', 'Ticonderoga', 'boredom', 'smell', 'ripe', 'wrapped', 'dogs', 'kiss', 'dollar', 'bill', 'noticed', 'Drexel', 'higher', 'thinner', 'stockings', 'miniature', 'wrinkled', 'station', 'intervals', 'companion', 'divided', 'ash', 'concrete', 'wooden', 'doors', 'thrown', 'iron', 'Above', 'mysterious', 'form', 'forked', 'gossip', 'silently', 'fed', 'devil', 'determined', 'belly', 'dreaming', 'Nor', 'fist', 'bowing', 'smoked', 'fence', 'torn', 'fold', 'sullen', 'quality', 'fully', 'chin', 'aged', 'goat', 'boiling', 'taut', 'warmth', 'roof', 'player', 'shadow', 'pass', 'raise', 'sandals', 'examining', 'belonged', 'sink', 'shrill', \"Maggie's\", 'complexion', 'sunny', 'activity', 'catch', 'lugged', 'besides', 'machine', 'dresses', 'Evadna', 'Mae', 'hang', 'careful', 'block', 'tea', 'tables', 'serve', 'type', 'happier', 'typewriter', 'whenever', 'waste', 'City', 'icy', 'grip', 'physical', 'climate', 'eternal', 'agencies', 'facilities', 'depression', 'dragging', 'West', 'meanwhile', 'theme', 'personal', 'dentist', 'minor', 'twice', 'coal', 'loosened', \"Grandma's\", 'wretched', 'mainly', 'shivering', 'upper', 'beds', 'mouthful', 'lives', 'brains', 'superior', 'dragged', \"Didn't\", \"Tolley's\", 'empty', 'kissed', 'decide', 'oh', 'Swift', 'Under', \"God's\", 'tore', 'shall', 'gather', 'envy', 'Another', 'effort', 'Mare', 'bless', 'Never', 'mark', 'racing', 'triumph', 'understanding', 'silly', 'Night', 'attentive', 'triplets', 'disappointment', 'pike', 'fetch', 'deeply', 'wear', 'thanks', 'silver', 'Kiz', 'chickens', 'babies', 'lift', 'French', 'pupils', 'gay', 'enormously', 'Blackwell', 'heavily', 'Thank', 'easier', 'moral', 'dawn', 'lying', 'ability', 'Before', 'grave', 'impatient', 'comfort', 'permitted', 'Feeling', 'yesterday', 'bouquet', 'Folly', 'impression', 'eager', 'village', 'ear', 'estimate', 'choose', 'coffin', 'system', 'sharply', 'Next', 'spoken', 'actions', 'Island', 'comes', 'visiting', 'nod', 'lifted', 'hungry', 'lively', 'fallen', 'savage', 'moonlight', 'clouds', 'stolen', 'Momoyama', 'tall', 'bridge', 'tilted', 'considering', 'request', \"doctor's\", 'suffered', 'Such', 'tours', 'Fleet', 'Yokosuka', 'Beach', 'sports', 'Hong', 'Kong', 'marked', 'applied', 'foreign', 'charm', 'date', 'officer', 'decency', 'thumb', 'muttered', 'Everybody', 'charming', 'addressed', 'Spanish', 'United', 'steak', 'sandwich', 'cup', 'remain', 'becoming', 'slight', 'cheerful', 'naturally', 'report', 'Perry', 'convinced', 'executive', 'members', 'draw', 'enjoy', 'spare', 'Pagan', 'Room', 'Friday', 'Thanks', 'rid', 'Our', 'bringing', 'freezing', 'resent', 'pitch', 'retreated', 'knocking', 'proper', 'active', 'whiskey', 'bar', 'showered', 'Golden', 'Calf', 'dice', 'test', 'management', 'dimly', 'lit', 'hopeful', 'cowboy', 'steady', 'build', 'repeating', 'Wrangler', 'spend', 'Sparky', 'loss', 'Hurrays', 'spirit', 'exhibition', 'wheel', 'boss', 'success', \"How's\", 'honey', 'hurry', 'throat', \"name's\", 'offer', 'planned', 'address', 'arrest', 'Greek', 'Somehow', 'handspikes', 'McKinley', 'reported', 'crash', 'heads', 'Small', 'violently', 'swear', 'evident', 'turmoil', 'ideal', 'battle', 'irons', 'followers', 'Wilson', 'Wales', 'useless', 'informed', 'truly', 'paused', 'stands', 'suspicious', 'lowered', 'act', 'failed', 'foolish', 'reward', 'someday', 'shake', 'guilt', 'painful', 'recognized', 'red-haired', 'shocked', 'favored', 'constant', 'favor', 'whispered', 'continue', 'frightful', 'approached', 'glancing', 'fixed', 'paot', 'sides', 'swallowed', 'windows', 'flight', 'peering', 'Yiddish', 'ghettos', 'noses', 'Since', 'worn', 'hung', 'rigid', 'rubbed', 'glanced', 'skirt', 'horrible', 'orthodox', 'amazed', 'section', 'visited', 'freedom', 'sought', 'intimate', 'pace', 'plans', 'begun', 'swing', 'Wednesday', 'nights', 'headlights', 'ceiling', 'complete', 'casual', 'waving', 'listened', 'Memphis', 'mayor', 'balloon', 'mud', 'changing', 'gives', 'Because', 'particularly', 'reflecting', 'thoughts', 'toilet', 'lock', 'encourage', 'bangs', 'break', 'drag', 'clutching', 'cooling', 'overhead', 'magazine', 'delicate', 'details', 'sheet', 'instance', 'covers', 'research', 'armor', 'McKenzie', 'art', 'third', 'cost', 'middle', 'Monmouth', 'page', 'artist', 'drawn', 'golden', 'prominent', 'slick', 'Life', 'photograph', 'library', 'gorgeous', 'studying', 'classes', 'painter', 'embrace', 'common', 'Pendleton', 'modern', 'leaders', 'ugliness', 'Tuesday', 'glove', 'Had', 'signed', 'thank', 'remark', 'weekend', 'river', 'upstairs', 'Harvie', 'disturbed', 'shudder', 'outcry', 'knows', 'absurd', 'connection', 'thousands', 'hanging', 'sacred', 'traffic', 'Again', 'Uhhu', 'wanting', 'history', 'Would', 'lower', 'snuggled', 'somewhere', 'Ferraros', 'tennis', 'Signora', 'North', 'lumber', 'boxcar', 'scornful', 'trembling', 'hideous', 'images', 'pressed', 'Tell', 'wolf', 'upright', 'nagging', 'sticks', 'noise', 'screaming', 'demons', 'Get', 'sank', 'happens', 'enter', 'brain', 'bore', 'thousand', 'whine', 'meaning', 'Move', 'hoarse', 'cave', 'single', 'grace', 'vanished', 'Hell', 'ridge', 'grotesque', 'totally', 'shape', 'committed', 'dive', 'jumped', 'Class', 'ghost', 'remote', 'bandages', 'mess', 'gentleman', 'Strange', 'cheekbones', 'leading', 'Uncle', 'ridiculous', 'general', 'crossed', 'Joan', 'win', 'Okay', \"Jim's\", \"She'd\", 'registered', 'awfully', 'grateful', 'appreciated', 'nervously', 'troubled', 'hers', 'yell', 'sharing', 'silence', 'good-by', 'cowbirds', 'solitary', \"child's\", 'row', 'surrounded', 'tension', 'bet', 'realize', 'hunting', 'buy', \"they'd\", 'tourists', 'curve', 'Quintus', 'intended', 'nightmare', 'Big', 'Mister', 'metal', 'ocean', 'shot', 'duck', \"Isn't\", 'Latin', 'waiter', 'Part', 'glared', 'skipped', 'backward', 'unbearable', 'smashed', 'birthday', 'attractive', 'rarely', 'surely', 'Longue', 'Vue', \"Linda's\", 'ceremony', \"who'd\", 'brave', 'keeps', 'generous', 'Washington', 'successful', 'save', 'stream', 'anxious', 'steels', 'Lovejoy', 'Funk', 'Shirley', \"Nadine's\", 'flushed', 'Roberts', \"Wally's\", 'dozed', 'figured', 'thankful', 'fired', 'cent', 'incredible', 'practically', 'fresh', 'Fairview', 'secret', 'Chief', 'elevator', 'shock', \"woman's\", 'scar', \"We'd\", 'dull', 'slide', 'private', 'Suppose', 'refused', 'tremble', 'acceptance', 'solution', 'deeper', 'recent', 'cognac', 'sober', 'wisdom', 'wise', 'toothbrush', 'bath', 'subject', 'plants', 'Carraway', 'scrawled', 'tense', 'agony', 'cat', 'ladies', 'rides', 'batting', 'affairs', 'driven', 'touched', 'Ronald', 'burning', 'reasonable', 'veranda', \"Kirby's\", 'flu', 'Peony', \"what's-his-name\", 'several', 'Be', 'worst', 'closely', 'wrote', 'absorbed', 'puny', 'base', 'tossed', 'ballplayers', \"Mike's\", 'dirt', \"Richard's\", 'Book', 'Dead', 'Asian', 'basement', 'flames', 'rush', 'searching', 'Fudomae', \"Charlotte's\", 'recall', 'tale', \"brother's\", 'plowed', 'jar', 'brutality', 'Snakes', 'likes', 'push', 'production', 'treasurer', 'firm', 'equipment', 'Anthea', 'expenditure', 'Cap', 'General', 'financing', 'Arthur', \"William's\", 'Herberet', 'plant', 'Ham', \"We've\", 'whirling', 'bats', 'lights', 'obeyed', 'commanded', 'confession', 'ignored', 'blind', 'Without', 'directly', 'Torino', 'Eh', 'stage', 'slapped', 'thighs', 'caressed', 'smoothness', 'rosaries', 'sweeter', 'condemned', 'fig', 'consumed', 'questioned', 'wave', 'exhausted', 'farewell', 'altered', \"Man's\", 'climbing', 'glued', 'stretched', 'roared', 'stucco', 'Sameness', 'sloping', 'upward', 'porches', 'screeched', 'housed', 'scent', 'lawn', 'hedge', 'supporting', 'facade', 'everywhere', 'blocks', 'fortresses', 'property', 'privacy', 'shaded', 'grape', 'licked', 'sinister', 'Aunt', 'provided', 'tribute', 'restless', 'Americans', 'brushing', 'drying', 'tanned', 'arch', 'swept', 'features', 'dreams', 'lapping', \"goat's\", 'sphere', 'Christ', 'jowls', 'swollen', 'teats', 'straining', 'storm', 'finger', 'reassured', 'pretended', \"Concetta's\", 'disfigured', 'People', 'possession', 'rapidly', 'huge', 'basket', 'household', \"wife's\", 'contributing', 'upkeep', 'soap', 'desperation', 'seems', 'baked', 'nursery', 'flannel', 'extra', 'list', 'assets', 'yard', 'elaborate', 'typing', 'bitterly', 'numbers', 'stark', 'cities', 'greedy', 'Western', 'sunshine', 'mountains', 'endless', 'resources', 'huddled', 'offices', 'newspaper', 'protected', 'rates', 'west', 'doubtless', 'Shivering', 'argued', 'song', 'problems', 'dozen', 'fuel', 'gas', 'fifty', 'catastrophe', 'Abernathy', 'ravenous', 'depths', 'widened', 'Presently', 'depend', 'removed', 'installed', 'freak', 'undressed', 'plunged', 'grimly', 'Plenty', 'ruin', 'prove', 'pneumonia', 'span', 'bedspread', 'trusted', 'deliberately', 'joke', \"they're\", 'Mamma', 'pin', 'Pictures', 'camera', 'Indeed', 'marrying', 'Mt.', 'Pleasant', 'Wait', 'hooked', 'Hurry', 'Fairbrothers', 'Make', 'congregation', 'cruel', 'unfair', 'rules', 'thunder', 'preserved', 'thrusting', 'burden', 'remaining', 'sons', 'necessity', 'clay', 'pieces', 'flame', 'haunting', \"Fair's\", 'graves', 'toast', 'absent', 'adding', 'Simply', 'Whipsnade', 'Mist', 'scream', 'assured', 'importance', 'furnishings', 'fail', 'spiritual', 'irritating', 'token', 'barn', 'bite', 'scarf', 'nerves', 'foal', 'laments', 'afterwards', 'robbed', 'mare', 'greatness', 'conflict', 'intensely', 'indifference', 'Tillie', 'aversion', 'chances', 'veterinarian', 'rubbing', 'cook', 'carpet', 'allowed', 'hon', 'cute', 'firmly', \"an'\", 'elegant', 'Sis', 'changes', 'rings', 'trips', 'frame', 'cats', 'Though', 'collar', 'holy', 'reserved', 'regretted', 'swell', 'dare', 'teaching', 'hop', 'hesitation', 'momentary', 'lonesome', 'relish', 'uncomfortably', 'murmured', 'loving', 'extraordinary', 'Hetty', 'tempted', 'childhood', 'School', 'taught', 'profound', 'distaste', 'tireless', 'one-two-three', 'supper', 'balls', 'sighing', 'ignore', 'regard', 'temptation', 'sin', 'creep', 'mission', 'sentimental', 'enclosed', 'grass', 'dew', 'bird', 'sang', 'engine', 'barefoot', 'neat', 'subdued', 'soberly', 'reassurance', 'tame', 'daisies', 'scented', \"lady's\", 'stirring', 'coldly', 'recalling', 'slow', \"dryin'\", \"You'd\", \"Y're\", 'sprawled', 'boots', 'emphasizing', 'aspects', 'whatsoever', 'approval', 'Titus', 'sisters', 'senses', 'gently', 'impressed', 'boat', 'observed', 'laws', 'Great', 'stones', 'grand-daughter', 'heaven', 'graveyard', 'patiently', 'gentle', 'weak', 'wicked', 'crossing', 'rolling', 'roots', 'humming', 'Both', 'smaller', 'gold', 'shade', 'angel', 'tomb', 'dared', 'Miyagi', 'strain', 'ancestors', 'Asia', 'sing', 'tribal', 'hairy', 'warmed', 'South', 'occasions', 'taller', 'lucky', 'melody', 'alien', 'commission', 'granted', 'preserve', 'health', 'rule', \"Officers'\", 'hamburger', \"Navy's\", 'Coast', 'Seventh', 'liberty', 'Sooner', 'Long', 'desperately', 'Bar', 'tinkling', 'trifle', 'practice', 'tweed', 'unmarried', 'slender', 'license', 'therefore', 'quaint', 'heels', 'furiously', 'jerked', 'Uh', 'hint', 'Orient', 'beauties', 'states', 'similarly', 'Four', 'murdering', 'groomed', 'exceptionally', 'Seeing', 'pride', 'learning', 'perfectly', 'singing', 'incident', 'Back', 'uncle', 'literally', 'translated', 'strangers', \"captain's\", 'discipline', 'riot', 'Subic', 'Walt', 'expert', 'Doolittle', 'awful', 'practical', 'jokes', 'sobering', 'cockroaches', 'security', 'uniform', 'confines', 'naval', 'establishment', 'suited', 'dug', 'Naval', 'sonofabitch', 'exclaimed', 'affection', 'dock', 'dusk', 'exercise', 'golf', 'clubs', 'Sierras', 'Toodle', 'Williams', 'Imagine', 'pleasant', 'territory', 'dinners', 'timid', 'lobby', 'disappointed', 'Alaska', 'Eskimos', 'highball', 'apparently', 'satisfied', 'reaction', 'winking', 'outsiders', 'remarks', 'curiously', 'travel', 'salesmen', 'prayer', 'personnel', 'cosmetics', 'appointment', 'calm', 'dining', 'mail', 'bulletins', 'issued', 'asking', 'glisten', 'River', 'adventurous', 'flowerpot', 'spinning', 'seventeen', 'surprisingly', 'gamblers', 'caring', 'gazed', 'mused', 'cleaned', 'entrance', 'support', 'period', 'blackjack', 'parts', 'Nice', 'pushing', 'neglected', 'glorified', 'den', 'waned', 'pistol', 'cash', 'withdrew', 'customers', 'points', 'glamorous', 'frightened', 'oil', 'Tahoe', 'brings', 'properly', 'During', 'opportunity', 'communicate', 'glances', 'dangers', 'chilling', 'heavers', 'Green', 'information', 'sprang', 'revealed', 'topgallant', 'crew', 'struck', 'bellowed', 'response', 'shackled', 'Adrien', 'Deslonde', \"Alexander's\", 'intention', 'confusion', 'terror', 'potential', 'ominous', 'tones', 'Midshipman', 'Tillotson', 'brandishing', 'weapon', 'flush', 'boast', 'investigation', 'questioning', 'court', 'loose', 'flag', 'self-confident', 'appointed', 'murdered', 'harbor', 'instinct', 'Anything', 'midshipmen', 'responsibility', 'uncertain', 'trust', 'wept', 'dedicated', 'seek', 'sobbed', 'fame', 'career', 'safely', 'boatswain', 'squinting', 'shifted', 'inevitable', 'prisoner', 'shackles', 'action', 'fantastic', 'apprentices', 'awaited', 'fur', 'erect', 'cloth', 'dingy', 'heel', 'plodding', 'schools', 'occupied', 'ajar', 'steep', 'baggy', 'skullcap', 'greeted', 'flicked', 'preceded', 'corridor', 'classroom', 'benches', 'shrilled', 'interpretation', 'Books', 'chanted', 'Hebrew', 'tune', 'Each', 'rocked', 'prayed', 'portion', 'rapping', 'incessantly', 'freckles', 'barely', 'accompanied', 'pitched', 'shuddered', 'coin', 'handed', 'thanked', 'Does', 'appear', 'resented', 'faced', 'clicked', 'dangerous', 'slyly', 'wherever', 'helping', 'countries', 'farms', 'clasped', 'clamped', 'relatives', 'retrieve', 'imprisoned', 'hanged', 'energy', 'bright-eyed', 'Anyway', 'upset', 'waking', 'poling', 'fork', 'fellowship', 'pine', 'Patrol', 'gripping', 'blow', 'underwear', 'pregnant', 'dizzy', 'normal', 'daddy', 'raining', 'spectator', 'eggs', 'sheds', 'unlocked', 'discover', 'believed', 'resting', 'works', 'sees', 'plastered', 'afford', 'fill', 'Christmas', 'corn', 'factories', 'uniforms', 'bands', 'symphony', 'sophisticated', 'concert', 'virtuoso', 'nodding', 'cheeks', 'drought', 'August', 'clerk', 'thirty-four', 'conditions', 'Herman', 'connections', 'spot', 'drawings', 'jackets', 'furnished', 'daring', 'cops', 'leather', 'accurate', 'editor', 'including', 'ages', 'transparent', 'furniture', 'costume', 'inner', 'vision', 'reminded', 'bottoms', 'bottles', 'exciting', 'pants', 'shared', 'cheap', 'pressure', 'awoke', 'income', \"Monmouth's\", 'ring', 'richness', 'rent', 'forty-four', 'speech', 'included', 'series', 'Jackson', 'atmosphere', 'vast', 'artists', 'inward', 'title', 'fascinated', 'intelligent', 'replied', 'Manhattan', 'Art', 'scholarship', 'sixty', 'impressionist', 'stimulation', 'farther', 'picking', 'greater', 'bunch', 'cafe', 'clock', 'lonely', 'mostly', 'worship', 'registration', 'Ida', 'accusing', \"Via's\", 'Walter', 'tear', 'killing', 'enjoying', 'chicken', 'frozen', 'wiser', 'foggy', 'crept', 'discussed', 'annual', 'accounts', 'testimony', 'Thaxter', 'driveway', 'nowhere', 'screamed', 'search', 'closets', 'telephoning', 'recalled', 'alarm', 'conversations', 'cliff', 'romantic', 'enormous', 'account', 'vigilance', 'pity', 'void', 'self-pity', 'triumphantly', 'haunted', 'coarse', 'insulting', 'harsh', 'altogether', 'free', 'Ellen', 'minister', 'glistening', 'doll', 'stranger', 'burial', 'parlor', 'surface', 'roses', 'knelt', 'Heaven', 'grief', 'Together', 'bachelor', 'downtown', 'needs', 'purse', 'lipstick', 'solemnly', 'Alberto', 'lies', 'using', 'falling', 'exasperation', 'respect', 'shoe', 'Italian', 'outfit', 'ruins', 'amusing', 'scandal', 'bells', 'ringing', 'Steps', \"Peter's\", 'shaking', 'Bible', 'Chapel', 'Could', 'honestly', 'imagination', 'Being', 'strict', 'Ferraro', 'lap', 'utterly', 'bobbing', 'Canada', 'bush', \"Sam's\", 'boxcars', 'Holy', 'assistance', 'doctors', 'saint', 'cardinals', 'Signor', 'Raymond', \"Carla's\", 'dreamy', 'Dookiyoon', 'Shades', 'stiff', 'facing', 'contempt', 'bleeding', 'Going', 'queer', 'lodges', 'Standing', 'indignant', 'begging', 'hunger', 'bearing', 'indignation', 'shower', 'sting', 'pack', 'teach', 'Keep', 'worrisome', 'smelling', 'whisky', 'TuHulHulZote', 'concerns', 'twisted', 'moon', 'crack', 'settling', 'motionless', 'bark', 'rough', 'edges', 'lash', 'despair', 'settle', 'Turning', 'prophesied', 'Sarpsis', 'wings', 'file', 'hurtling', 'maleness', 'parade', 'nation', \"men's\", 'hearts', 'mounted', 'disheveled', 'hoofs', 'roll', 'monster', 'plunging', 'terrifying', 'pulse', 'clutch', 'downward', 'leaves', 'volume', 'miracle', 'confused', 'Hello', 'obvious', 'wire', 'pretense', 'acute', 'Bentley', 'robe', 'Monday', 'vacuum', 'cleaner', 'expanding', 'shivered', 'wound', 'Alex', 'debt', 'Too', 'adhesive', 'tape', 'however', 'Hall', 'hallway', 'wreck', 'reply', 'gazing', 'brow', 'considerate', 'clearly', \"It'll\", 'disappeared', 'footsteps', 'Pietro', 'forced', 'affectionate', 'respected', 'theater', 'Someone', 'jury', 'struggle', 'tactful', 'facts', 'routine', 'strip', 'constructed', 'secretary', 'train', 'peculiar', 'Websterville', 'anxiety', 'finds', 'Remember', 'keys', 'horizon', 'unhappily', 'startled', 'selfish', 'bundle', 'instant', 'warning', \"Susan's\", 'entire', 'dust', 'breeze', 'darkened', 'radiant', 'nests', 'baffled', \"people's\", 'airy', \"Greg's\", 'faintly', 'lavender', 'buried', 'pause', 'wallpaper', 'hesitated', 'Christmastime', 'speechless', 'teddy', 'delivered', 'jammed', \"Quint's\", 'cottages', 'V-shaped', 'inlet', 'seaweed', 'whiskers', 'blades', 'controlling', 'Fearless', 'piece', 'Fortman', 'Stuck-up', 'loud', 'Gord', 'sweat', 'Aw', 'hole', 'shoulder', 'choppy', 'peppermints', 'Ever', 'Gordon', 'tail', 'soup', 'shaken', 'mackerel', 'yodeling', 'Same', 'King', 'Which', 'mold', 'fought', 'smothered', 'occasion', 'tonsil', 'giggles', 'swam', 'plastic', \"he'll\", 'Fifteen', 'Yeah', 'rope', 'lung', 'caused', 'Jaguar', 'thoroughly', 'chapter', 'community', \"Wasn't\", 'Mountains', 'Dartmouth', 'engagement', 'events', 'jolt', 'Cleveland', 'wishful', 'absolutely', 'shallow', 'frivolous', 'scatterbrained', 'magnificent', 'example', 'correct', 'Others', 'trace', \"Edythe's\", 'glaring', 'graham', 'crackers', 'problem', 'match', \"Bobbie's\", 'decent', 'Smith', '&', 'salesman', 'branch', 'contacts', 'Murkland', 'harder', 'feels', 'opportunities', 'surprise', 'degree', 'sweetly', 'falls', 'staying', 'refuse', 'Pack', 'davenport', 'instantly', 'strode', 'nails', 'state', 'bubble', 'miserably', 'coy', 'unbelievable', 'tide', \"They'd\", 'spoiled', 'bum', 'application', 'logic', 'thousandth', 'complain', 'Hodges', \"mustn't\", \"Gladdy's\", 'credit', 'Michelson', 'promising', 'chatter', 'rounds', 'sights', 'chart', 'diagnosis', 'familiar', 'network', 'violent', 'patient', 'control', 'examination', 'penetrating', 'smear', 'Doctor', 'permission', 'shiver', 'amazement', 'cancer', 'echo', 'outta', 'nitrogen', 'contented', 'mouthpiece', 'aloud', 'vacation', 'Valery', 'army', 'Hun', 'Jour', 'et', 'Nuit', 'Montmartre', 'cheese', 'drained', 'Tropic', 'breathing', 'topcoat', 'umbrella', 'hustler', 'Hardly', 'lied', 'wager', 'Monsieur', 'sadly', 'cynicism', 'rotten', 'thirty-five', 'forty', 'sensed', 'Bon', 'jour', 'hips', 'brushed', 'innocently', 'brush', 'Stephen', 'cups', 'Louvre', 'Express', 'leaning', 'Today', 'Colosseum', 'prescribed', 'noisy', 'frightening', 'Shoals', 'smartly', 'splendid', 'staircase', 'display', 'azalea', 'workmen', 'azaleas', 'blooms', 'purple', 'learned', 'warming', 'cigarette', 'confidence', 'drawer', 'replaced', \"Elec's\", 'grandchildren', \"Emma's\", 'contorted', 'solemn', \"George's\", 'persuade', 'possibility', 'arrangements', 'separate', 'specialized', 'alligator', 'Finally', 'scurried', 'supremely', 'exist', 'Elsie', 'motion', 'Westfield', 'Dolan', 'Young', \"Christians'\", 'faith', \"guy's\", 'willing', 'kitten', 'honorable', 'hopelessly', 'surrendered', 'motel', 'desire', 'awareness', 'Mmm', 'elbow', 'maturity', 'Boston', 'blond', 'separated', 'Need', 'invisible', 'appeal', 'Unit', 'Number', 'lightly', 'victory', 'Nassau', 'Bobbsey', 'Twins', 'variety', 'presence', 'onion', 'wrapping', 'feminine', 'judgment', 'following', 'chic', 'subconsciously', 'collection', 'Larkspur', 'Clearly', 'ninety-six', 'browny-haired', 'sprinkling', 'sympathetic', 'tan', 'Third', \"Deegan's\", 'bastard', 'spectators', 'fielder', 'ballplayer', 'built', 'mound', 'arose', \"Phil's\", 'sweatshirt', 'jaw', 'locker', 'banged', 'professional', 'leagues', 'abruptly', 'shaky', 'product', 'numerous', 'Through', 'seldom', 'studies', 'acquired', 'freighter', 'eaten', 'pretentious', 'throughout', 'Ceecee', 'statue', 'paced', 'sensation', 'hillside', 'demon', \"Fudo's\", 'refusing', 'plowing', 'stubble', 'area', 'lath', 'panic', 'tapered', 'frighten', 'trailed', 'delicately', 'diamonds', 'gracefully', 'recently', 'solid', 'clods', 'pirouette', 'brutal', 'eagerly', 'scholastic', 'record', \"son's\", 'organization', 'Ivy', 'grades', 'elected', 'slammed', 'forum', \"David's\", 'Products', 'Half', 'colleges', 'Hear', 'surprising', 'bass', 'Crazy', 'Horse', 'Colts', 'generally', 'garments', 'resistance', 'stubborn', 'contest', 'lace', 'chain', 'forgot', 'Bryan', 'corporation', 'contracts', \"Willis'\", 'partner', 'power', 'conversion', 'allied', 'aircraft', 'jet', 'automobile', 'promotion', \"Hamrick's\", 'Motors', 'gantlet', 'tower', 'Mass', 'repetitive', 'monotonous', 'unimportant', 'bees', 'tomato', 'paste', 'sour', 'aluminum', 'trays', 'fly-dotted', 'cheesecloth', 'surging', 'bodies', 'dived', 'blackness', 'amber', 'bay', 'Filippo', 'Rossi', 'Signore', 'elders', 'Youth', 'fur-piece', 'wiggled', 'satin-covered', 'buttocks', 'clutched', 'straw', 'strutted', 'streetcars', 'bigger', 'fancy', 'airs', 'ours', 'speeches', 'Dante', 'actresses', 'Henh', 'Calloused', 'polished', 'excitedly', 'puckered', 'chins', 'hairs', 'sprouted', 'tweezed', 'Mauve-colored', 'mouths', 'Puttana', 'fleshy', 'suppleness', 'seams', 'shooing', 'fleas', 'hopped', 'sundials', 'variegated', 'expression', 'flown', 'withered', 'streetcar', 'contemptuous', 'purpose', 'opposition', 'outskirts', 'Philadelphia', 'Bari', 'Chieti', 'Ash', 'Road', 'elevated', 'trains', \"city's\", 'half-hour', 'creek', 'Schuylkill', 'aloneness', 'framed', 'ginkgo', 'lined', 'two-story', 'slashed', 'manure-scented', 'lawns', 'wicker', 'swings', 'rusted', 'hinges', 'stable-garage', 'rot', 'evenings', 'Sundays', 'reeked', 'dregs', 'squatted', 'sidewalk', 'grating', \"Bartoli's\", 'second-story', 'showroom', 'angels', 'surveyed', 'sameness', 'perched', 'slant', 'paved', 'alleyways', 'tunneled', 'core', 'intimacy', 'backyards', 'fences', 'blended', 'courtyards', 'vines', 'Waiting', 'pact', 'heritage', 'ended', 'white-columned', 'eight-thirty', 'local', 'pound', 'gloved', 'vest', 'gloves', 'unconcerned', 'gilded', 'lithe', 'straddled', 'railing', 'loosely', 'creaking', 'pales', 'artfully', 'tapering', 'tips', 'glowed', 'sinewy', 'swirled', 'childlike', 'softness', 'fragile', 'harshness', 'belied', 'lyric', 'contours', 'downcast', 'possessed', 'clouded', 'screeching', 'rail', 'daydreaming', 'wetness', 'claret', 'feasting', 'salt', 'sallow', 'time-cast', 'encrusted', 'marbleized', 'unfalteringly', 'git', 'Soothing', 'whiskered', 'expectantly', 'undulated', 'gradually', 'covering', 'squirted', 'savored', 'earthy', 'operated', 'skilled', 'unity', 'bagpipe', 'pressing', 'pulling', 'delighting', 'evasive', 'cloud', 'Its', 'fluttering', 'soutane', 'sensing', 'portentous', \"dog's\", 'Time', 'meantime', 'stained', 'ocher', \"Pompeii's\", 'edged', 'clapping', 'ecstatic', 'released', 'haunches', 'ticks', 'biggest', 'Niobe', 'neatest', 'Concetta', 'laced', 'Romeo', 'idiot', \"Romeo's\", 'grasp', 'impetuous', 'rotundity', 'throbbed', 'fatigue', 'earnest', 'quench', 'outdistanced', 'recovery', 'highly', 'fuzz', 'flaxen', 'rages', 'digesting', 'peaches', 'cream', 'disposition', 'amounts', 'stated', 'lifting', 'coaxed', 'rackety', 'washer', 'daily', 'chores', 'participating', 'Worry', 'produce', 'salary', 'Clifton', 'preferred', 'bankruptcy', \"family's\", 'vitamins', 'pored', 'squeeze', 'pennies', 'consulted', \"Woman's\", 'Exchange', 'goods', 'starched', 'nineteen', 'nuzzled', 'ironed', 'wrapper', \"Best's\", 'Liliputian', 'Bazaar', 'sensible', 'pencil', 'jotting', 'mothers', 'cribs', 'playroom', 'entail', 'salads', 'beans', 'gypsy', 'fortunes', 'clearing', 'chattering', 'contests', 'November', 'miseries', 'countless', 'temperature', 'zero', 'tangible', 'pall', 'Mile', 'High', 'locking', 'harshened', 'outlines', 'realism', 'resembling', 'habitual', 'sprawling', 'bumptious', 'open-handed', 'basking', 'stored', 'riches', 'jobless', 'employment', 'parks', 'exclusive', 'created', 'depressions', 'markets', 'congested', 'populations', 'centralization', 'industries', 'discriminatory', 'freight', 'popularly', 'creating', 'penetrated', 'spending', 'remedies', 'millions', 'urgent', 'Abernathys', 'stretch', 'mortgage', 'taxes', 'overshoes', 'leaks', 'emergencies', 'termed', 'major', 'disaster', 'maw', 'appeased', 'hurling', 'tons', 'Cold', 'innumerable', 'sprung', 'frames', 'attic', 'shingles', 'capacity', 'fireplaces', 'electric', 'gaiety', 'infected', 'stormbound', 'shipwrecked', 'circumstance', 'anxieties', 'cling', 'survive', 'pipes', 'ankles', 'swabbed', 'bathrooms', 'groaning', 'polar', 'regions', 'mining', 'joys', 'emergency', 'wallow', 'willed', \"Hope's\", 'tipped', 'overturning', 'Thrifty', 'Unusual', 'tenant', 'simplify', 'California', 'jilted', \"sun's\", \"Brace's\", 'sneaky', 'consorting', 'tenants', 'understands', 'Labans', 'valiant', 'pitiable', 'Returning', 'log-house', 'Jonathan', 'stormy', 'Him', 'Put', 'asunder', 'Absolution', 'telegraph', 'message', 'sealed', 'pigeonhole', 'resist', 'deception', 'peculiarly', 'confess', 'lacerate', 'Reaching', 'relic', 'pioneer', 'pyre', 'curl', 'sealing', 'wax', 'melting', 'bubbling', 'feathery', 'beloved', 'gathers', 'fruit', 'subtle', 'genius', 'Swinburne', 'gifted', 'satisfactions', 'Beyond', 'greening', 'Grand', \"parents'\", 'grandsons', 'thimble', 'Brace', 'easygoing', 'blacksmith', 'preacher', 'Howdy', 'careless', 'Oscar', 'P.GA', \"C'un\", 'Major', 'plain-out', 'carte', 'blanche', 'weakness', 'devotion', \"Robards'\", 'Queen', 'mares', 'geldings', 'what-nots', 'reverent', \"Racin'\", 'revolting', 'pearl', 'stamp', 'imperiously', 'stall', 'invariably', 'Nerves', 'nip', 'Stand', 'oneasy', 'Quit', \"sor'l\", 'lip', 'January', \"musn't\", 'annoy', 'Listening', 'predictions', 'procession', 'foals', 'symbolized', 'unpleasant', 'treated', 'acknowledge', 'condition', 'disgusting', 'Human', 'birth', 'novelty', 'midwife', \"Jenny's\", 'former', 'admirer', 'riding', 'quarts', 'liniment', 'fussing', 'bran', 'mash', 'charlotte', 'russe', 'tracking', 'manure', 'jiffy', \"trippin'\", \"ever'\", 'Rhyme', 'Arcilla', 'Flotilla', 'Edmonia', 'Jennifer', 'Kezziah', \"this'll\", 'boy-name', \"Mare's\", 'Handsomest', 'colt', 'Kentucky', 'Strong', 'Royal', 'Jesus', 'distressed', 'peach', 'Home', 'trimmed', 'callers', 'Shawnee', 'Rakestraw', 'criticisms', 'Heavenly', 'Rest', 'boarding', 'inclement', 'Spa', 'snippy', 'namesake', 'teething', 'cordial', \"Tillie's\", \"Nick's\", 'posts', 'pups', 'to-do', 'dollies', 'finest', 'spa', 'entertain', 'nigs', 'invite', \"ever'body\", 'Galt', 'House', 'Jockey', 'affair', 'dang', 'oystchers', \"bar'l\", \"oystchers'll\", 'perk', 'downright', \"Roy's\", 'buggy', 'Eph', 'Showers', 'preach', 'mountain', 'politely', 'gateway', \"Them's\", 'purtiest', 'babes', \"comin'\", 'forgiven', 'absolution', 'testament', 'proof', 'godless', 'wishing', 'garnet', 'counsel', 'inevitably', 'unfavorable', 'frequently', 'circumspect', \"Hetty's\", 'flair', 'drama', 'summers', 'Maneret', 'excellently', 'austere', 'lessons', 'Polish', 'nobleman', 'Craddock', 'supervising', 'wand', 'everlasting', 'teas', 'fetes', 'jocular', 'possessive', 'planets', 'revolved', 'corruption', 'interruption', 'assume', 'spared', \"Charles'\", 'vulnerable', 'argue', 'morality', 'consisted', 'finding', 'oppressive', 'Impossible', 'virtue', 'colloquy', 'sill', 'artificial', 'blossom', 'leaf', 'pilgrim', 'avoided', 'graveyards', 'remembering', 'lacy', 'enchanting', 'wildness', 'Leaning', 'tangle', 'rosebush', 'honeysuckle', 'bloom', 'spray', 'thorns', 'dewdrops', 'buds', 'Valentine', 'balanced', 'nondescriptly', 'artless', 'decidedly', 'une', 'femme', \"d'un\", 'unbelievably', 'protective', 'bun', 'jug', 'equivalent', 'wilderness', 'yielded', 'patchwork', 'william', 'bedstraw', 'grasses', 'haranguing', 'poodle', 'gleefully', 'supplicating', 'prayerful', 'forepaws', 'Rummaging', 'comply', 'slumbered', 'shuttered', 'shops', 'inhabitants', 'eying', 'frankest', 'curiosity', 'bowed', 'princess-in-a-carriage', 'acknowledgments', 'cautious', 'reflective', 'sap', 'plainly', 'cupped', 'repetition', 'announcing', 'nettled', 'unenthusiastic', \"Summer's\", 'scowled', 'vague', 'hospitality', \"They'll\", \"takin'\", 'pleasantly', \"soon's\", 'doubtfully', 'soldierly', 'ceased', 'pointer', 'finer', 'text', \"f'r\", 'happily', 'rests', 'womanly', 'Rests', 'pacifies', 'dad', 'link', 'nineties', 'Blackwells', 'silenced', 'scanty', 'deep-set', 'mariner', 'awed', 'fishing-boat', 'powerful', 'shriveled', \"well's\", 'Ran', 'lawful', 'wedded', \"leavin'\", 'cheerfully', 'Left', 'Any', 'dryly', 'knowingly', 'liking', 'connivance', 'Selma', \"Cotter's\", 'censure', 'praised', 'big-boned', 'drab-haired', 'apron', 'print', 'vaguely', \"gran'dad\", \"Y'r\", \"dam'\", 'porridge', 'Milk', 'sops', 'claws', 'clattered', 'lowly', 'wearily', 'piously', 'Beer', 'affect', 'ingenious', 'hypocrisy', 'forgave', 'stint', 'malevolence', \"Blackwell's\", 'somewheres', \"Ma'am\", 'Delia', 'cruelty', 'tyranny', 'mumbled', 'shuffling', 'mightily', 'buffeted', 'gabble', 'indoors', 'swearing', \"journey's\", 'dotted', 'bushes', 'sheep', 'blossoms', 'scant', 'clawing', 'stony', 'darted', 'rustle', 'placid', 'terrestrial', 'mindless', 'spire', 'lessened', 'headstones', 'Dorothy', 'Tredding', 'nearest', 'sea-damp', 'tracing', 'indecipherable', 'carved', 'padded', 'moss', 'parasol', 'rock-carved', 'nearby', 'scudding', 'fearing', 'ghosts', 'Prefecture', 'northeast', 'Honshu', 'traces', 'Ainu', 'Ainus', 'primitive', 'Southern', 'Apparently', 'Caucasian', 'skins', 'bearded', 'pitiful', 'subsist', 'chants', 'sadness', 'Indians', 'assimilated', 'Akita', 'prefectures', 'occasionally', 'strikingly', 'coloring', 'improved', 'tawny', 'honey-in-the-sun', 'tint', 'invaders', 'fortunate', 'exquisitely', 'willowy', 'susceptible', 'vegetable', 'diet', 'forebears', 'intriguingly', 'rebellious', 'requests', 'Bremerton', 'Lakes', 'Pensacola', 'threat', 'resign', 'Anywhere', 'assigned', 'psychopathic', 'depressingly', 'cases', 'teamed', 'chaplain', 'counselor', 'mental', 'salvage', 'firms', 'operate', 'hulks', 'warships', 'sunk', 'inshore', 'setting', 'nerve-shattering', 'blasts', 'psychiatry', 'off-duty', 'sustain', 'dinnertime', 'cocktail', 'crossroads', 'Ships', 'rotated', 'six-month', \"Fleet's\", 'port', 'maintenance', 'shore', 'ships', 'officers', 'good-looking', 'cubes', 'tipsy', 'consequently', 'tailored', 'carriers', 'aviator', 'fifty-fifty', 'overseas', 'implied', 'excused', 'intimated', 'Harro', 'girl-san', 'catchee', 'boy-furiendo', 'likee', 'beg', 'pardon', 'brand', 'nice-looking', 'open-mouthed', 'blushing', 'blush', 'gesture', 'slang', 'fly-boy', 'lasting', 'satisfaction', \"nurses'\", 'brunettes', 'Moorish', 'Balkan', 'endowed', 'blessed', 'halfway', 'humiliated', 'self-consciously', 'waitresses', 'counter', 'Yuki', 'Kobayashi', 'Bifutek-san', 'Kohi', 'Futotsu', 'whitehaired', 'doting', 'commander', 'appealing', \"Tommy's\", 'rebelliously', 'civilian', 'transient', 'increasingly', 'Nurse', 'Corps', 'congenial', 'after-duty', 'raucous', 'poorly', 'companionship', 'put-upon', 'repeated', 'Eating', 'indigestion', 'bicarbonate', 'soda', 'sulked', 'crude', 'Oyajima', 'kimono', 'faculty', 'hostess', 'gown', 'kotowaza', 'proverb', 'Tanin', 'yori', 'miuchi', 'Relatives', 'thicker', \"Doolittle's\", 'scheduled', 'dispensed', 'ordinarily', 'immature', 'tactics', 'rabble', 'rousing', 'belief', 'fun-loving', 'Guns', 'Appleby', 'perils', 'horseplay', 'stabilizing', 'influence', 'overdeveloped', 'wisecracked', 'indoctrination', 'slaughtering', 'ethyl', 'chloride', 'Armed', 'Forces', 'heights', 'eventually', 'retired', 'restriction', 'Bustard', 'deprived', 'Boats', 'McCafferty', 'restricted', 'thoughtful', 'medical', 'supplies', 'Supply', 'inspect', 'caves', 'shipmate', 'connect', 'exchange', 'Gresham', 'commissary', 'Grab', 'errand', 'salute', 'gangway', 'Petty', 'excursion', 'boating', 'hiking', 'virile', 'strenuous', 'fashionable', 'Hotel', 'frog', 'packing', \"Buzz's\", 'Wow', 'Strippers', 'scrumptious', 'all-lesbian', 'band', 'Hi', 'Willows', 'notes', 'Tough', 'Reno', 'instigator', 'victims', 'rap', 'Ahah', 'lush', 'divorcee', 'scrawny', 'pajamas', 'moons', 'semi-professionally', 'sponsoring', 'courageous', 'sparkled', 'dully', 'hardships', 'undergo', 'Smug', 'smug', 'sappy', 'twitch', 'region', 'lewd', 'Eskimo', 'sickly-tolerant', 'canvassing', 'Forebearing', 'Several', 'wandering', 'corridors', \"strangers'\", 'difficulties', 'pairs', 'hundred-and-fifty', 'Northwest', 'overexcited', 'remonstrated', 'secular', 'bibles', 'publishing', 'appliances', 'waspishly', 'appliance', 'heatedly', 'blinking', 'seller', 'Leave', 'bucking-up', 'crestfallen', 'race', 'indecisively', 'receive', 'enthusiastic', 'statement', 'overdue', 'alimony', 'Collector', 'Internal', 'Revenue', \"year's\", 'exemptions', 'Virginia', 'braced', 'crest', 'white-topped', 'Truckee', 'spangle', 'elated', 'rattling', 'roulette', 'neon', 'automatically', 'mural', 'depicted', 'settlers', 'wagons', 'animated', 'expectancy', 'splendidly', 'stern', 'Donner', 'starving', 'heavily-upholstered', 'one-arm', 'bandits', 'cunningly', 'slots', 'bested', 'mechanical', 'devices', 'depending', 'dried-up', \"blonde's\", 'stables', 'martingale', 'hit-and-miss', 'five-seventeen', 'logging', 'twelve-hour', \"roulette's\", 'Incidentally', 'famous', 'ranch', 'Bar-H', 'collect', 'rack', 'single-foot', 'fastest', 'Washoe', 'County', 'publicity', 'campaign', 'TV', 'hero', 'trained', 'Hoot', 'Gibson', 'pinto', 'photographs', 'wonderfully', 'conclusively', 'jinx', 'gambling', 'marking', 'keno', 'featured', 'attraction', 'ogled', 'shill', 'prejudice', 'shills', 'flipping', 'discreetly', 'chips', 'premium', 'announcement', 'whereby', 'feature', 'floorshow', 'A.M.', 'starring', 'Adele', 'Body', 'Brenner', 'schoolgirls', 'brides', 'orchids', 'chuck-a-luck', 'Hawaiian', 'sun-tan', 'shorter', 'fatter', 'Lake', 'Cal-Neva', 'instigating', 'guarantees', 'stickman', 'pit', 'crossroading', 'faro', 'allergic', 'pollen', 'okay', 'crap', 'pyramid', 'consecutive', 'platinum', 'bursting', 'sun-suit', 'well-fed', 'prosperous', 'propositioned', 'Stake', 'tango', 'discuss', 'merger', 'Sounds', 'Gisele', 'Scotch', 'Named', 'ballet', 'Sylphide', 'affected', 'Answer', 'Very', 'procedure', 'cold-bloodedly', 'chide', 'lapse', 'manufacturing', 'steal', 'aft', 'pretence', 'Hostile', 'flashed', \"Captain's\", 'arrests', 'E.', 'Andrews', 'oldest', 'glories', 'identified', 'mutineer', 'tap', 'Oliver', 'breathless', 'wild-eyed', 'holystones', 'mysteriously', 'customary', 'articles', 'souvenirs', 'African', 'battle-ax', 'sharpened', 'overheard', 'disappearance', 'ladder', 'aimless', 'milling', 'well-trained', 'well-organized', 'horror', 'orders', 'alert', 'questioningly', 'instruction', 'hastened', 'weather-royal', 'consultation', 'mistaken', 'aiding', 'obey', 'bloodshed', 'strategy', 'reasoned', 'defeated', 'unwillingness', 'sanction', 'originated', 'stalked', 'openly', 'morose', 'muster', 'expressing', 'displeasure', 'communicating', 'hiding', 'insolently', 'disobeyed', 'Seaman', 'attacked', 'snarling', 'McKee', 'brig', 'loyal', 'villainous', 'unaffected', 'crime', 'mutiny', 'fears', 'requires', 'omniscient', 'select', 'rely', 'staunch', 'modest', 'ventured', 'guard', 'increased', 'inquiry', 'punished', 'perform', 'workable', 'extravagant', 'agree', 'thoughtfully', 'conspiracy', 'ferocious', 'anarchy', 'Implements', 'available', 'hasty', 'combat', 'ourselves', 'sentinel', 'brothers', 'labor', 'accordance', 'implacable', 'protection', 'honor', 'yearned', 'passionately', 'strive', 'gates', 'eighteen-year-old', 'sailed', 'shed', 'desired', \"egotist's\", 'precious', 'satisfy', 'egotist', 'served', 'Dear', 'dried', 'assurance', 'Stern-faced', 'inspected', 'satisfying', 'averted', 'glimpsed', 'mild-mannered', 'respectful', 'seeming', 'lethargy', 'gaze', 'calmness', 'detachment', 'unawareness', 'implicit', 'naive', 'thick-skulled', 'sudden', 'clarity', 'kinship', 'stupidity', 'quarreling', 'handcuffs', 'ankle', 'follows', 'misfortune', 'towering', 'symbol', 'authority', 'tragic', 'lad', 'forged', \"other's\", 'undoing', 'deny', 'hazel', 'accept', 'gain', 'defending', 'contemplation', 'logical', 'untruth', 'interpreted', 'prior', 'significance', \"Cromwell's\", 'explanations', 'Rogers', 'geometry', 'wardroom', 'element', 'absurdity', 'percentage', 'steered', 'Torah', 'Bits', 'trash', 'roadway', 'warmish', 'foul', 'strides', 'double-breasted', 'material', 'buckles', 'brim', 'crown', 'trim', 'pinkish-white', 'conscious', 'long-sleeved', 'stride', 'pivoting', 'twisting', 'discolored', 'boarded', 'synagogues', 'shabby', 'clasping', 'unclasping', 'paunch', 'bare-armed', 'dripped', 'poised', 'surly', 'half-closed', 'Rapping', 'translation', 'Moses', 'previously', 'grandfather', 'great-grandfather', 'upturned', 'cropped', 'ringlets', 'framing', 'yellowed', 'prayerbooks', 'penalty', 'distraction', 'guttural', 'ghetto', 'curls', 'traveled', 'partially', 'texts', 'memorized', 'singsonged', 'off-key', 'baritone', 'spurred', 'tapping', 'defined', 'rhythm', 'backed', 'clucked', 'high-pitched', 'devote', 'Except', 'Shabbat', 'praying', 'numb', 'prickly', 'asks', 'fingered', 'pruta', 'Sabras', 'roads', 'Messiah', 'convictions', 'immortal', 'heroic', 'twinkling', 'respectfully', 'kibbutzim', 'Aliah', 'immigrants', 'Ready', \"Me'a\", \"She'arim\", 'Jewish', 'anyplace', 'gaining', 'rebuffed', 'slowed', 'cobblestones', 'pursed', 'Trouble', 'middle-aged', 'strut', 'numerals', 'branded', 'concentration', 'Often', 'despondent', 'wandered', 'official', 'Mandate', 'arrested', 'direct', 'complained', 'intend', 'proposal', 'straighten', 'beaten', 'chariot', 'south', 'Forked', 'Deer', 'braving', 'wastes', 'dumped', 'barge', 'Ethiopians', 'tooling', 'sweeping', 'expansiveness', 'courts', 'cabins', 'bulb', 'unfrosted', 'streetlight', 'swooping', 'bugs', 'bouts', 'hopscotch', 'moths', 'pinging', 'cruising', 'insect', 'Highway', 'feelers', 'sagged', 'overhearing', 'curtain', 'rehearsal', 'Mattie', 'microphone', 'Toonker', 'Burkette', 'yanking', 'leak', 'burn', 'parachute', 'Starkey', 'Poe', 'draining', 'punctured', 'muddy', 'pumps', 'squat', 'son-of-a-bitch', 'hating', 'raped', 'tool', 'rape', 'nestled', 'breast', 'tramp', 'skull', 'drives', 'delivers', 'berry', 'crates', 'owns', 'wells', 'mines', 'mild-voiced', 'little-town', 'big-town', 'level', 'Houdini', 'mile', 'chump', 'too-shiny', 'radio', 'dawns', 'seat', \"Shafer's\", 'substitute', 'Louis', 'Damn', 'acres', 'whitewashed', 'bordering', 'grounds', 'envisioned', 'Homes', 'federal', 'highway', 'peaceful', 'bandstand', 'flash', 'instruments', 'spellbound', 'piano', 'rehearsing', 'governor', 'swelled', 'goose', 'bumps', 'rippled', 'Tennessee', 'farmer', 'rained', \"rat's\", 'ass', 'furrowed', 'powdery', 'droughts', 'predicting', 'festival', 'Factory-to-You', 'maids', 'youngest', 'unventilated', 'allow', 'malingering', 'primping', 'sincere', 'scrubbing', 'boxed-in', 'public', 'count', 'rapped', 'jerk', 'whichever-the-hell', 'six-thirty', 'Saturdays', 'chairs', 'fans', 'porter', 'janitor', 'Among', 'handled', 'commissions', 'uptown', 'lettering', 'specialty', 'complicated', 'pen-and-ink', 'medieval', 'hundreds', 'crammed', 'eight-by-ten', \"jeweler's\", 'precise', 'spots', 'literary', 'artistic', 'journals', 'Yorker', 'Esquire', 'paperback', 'reprints', 'huddling', 'corners', 'sleeves', 'revolver', 'Brothers', 'Karamazov', 'illustration', 'Magpie', 'Press', 'historical', 'novel', 'Edward', '3', 'fifteenth-century', 'ferreted', 'materials', 'shields', 'linden', 'reflections', 'sketch', 'Rufus', 'static', 'reproduce', 'readers', 'researches', 'television', 'movies', 'knights', 'flowing', 'haircuts', 'Fauntleroy', 'villains', 'beards', 'Byron', 'sword', 'stills', 'movie', 'clam', 'authenticity', 'painters', 'scholarships', 'Joyce', 'style', 'adults', 'freshness', 'perception', 'self-consciousness', 'twist', 'forms', 'leap', 'smack', 'nearsighted', 'prevent', 'uncanny', 'absent-minded', 'sleepwalker', \"Christ's\", 'Prussian', 'discovery', 'gravy', 'expenses', 'lodgings', 'Attending', \"Askington's\", 'ambition', 'reputation', 'illustrator', 'Peter', 'goal', 'cashmere', 'buttons', 'Viyella', 'necktie', 'bolo', 'jade', 'texture', 'clothing', 'decorations', \"Brush-off's\", 'sparkle', 'erudite', 'illustrators', 'cover', 'magazines', 'Modern', 'Artists', 'photographed', 'Sixties', 'Velasquez', 'royalty', 'Spain', 'bookshelves', 'Modigliani', 'portrait', 'Pollock', 'Miro', 'background', 'inscription', 'Martian', 'fireplace', 'tiles', 'Picasso', 'restaurants', 'illusion', 'wealthy', 'craft', 'absorb', 'masters', 'Durer', 'Bellini', 'Mantegna', 'Painting', 'interdependent', 'varied', 'multitudinous', 'fragment', 'mosaic', 'performer', 'philharmonic', 'organize', 'gigantic', 'abstract', 'expressionism', 'photorealism', 'outward', 'Eye', 'anatomy', 'teaches', 'five-hundred-dollar', 'prize', 'bums', 'Hudson', 'enameling', 'Hajime', 'Iijima', 'Osric', 'million', 'retrospective', 'contemporary', \"Cezanne's\", 'Still', 'canvases', \"Thoreau's\", 'hangouts', 'magical', 'Contact', 'stimulating', 'succeed', 'isolating', 'occupation', 'Middle', 'Ages', 'Renaissance', 'nineteenth', 'century', 'artisan', 'craftsmanship', 'goldsmith', 'carver', 'society', 'portraying', 'wars', 'impulses', 'propagandist', 'satirist', 'lover', 'philosopher', 'scientist', 'illustrating', 'tooth-paste', 'ads', 'salacious', 'incidents', 'trivial', 'novels', 'fluff', 'navel', 'contemplated', 'purity', 'amateur', 'dervishes', 'apprenticeship', 'upshot', \"Pendleton's\", 'Thursday', 'awkward', 'punch', 'drawing', 'finish', 'Brush-off', 'benefit', 'mailman', 'Stimulating', 'rewarding', 'Partly', 'anticipated', 'prepared', 'classic', 'cigars', 'quitting', 'compartment', 'readily', 'frowning', 'mutually', 'stir', 'courtesy', 'fraud', \"Walter's\", 'assail', 'temporary', 'narrows', 'half-murmured', 'statements', 'stress', 'indulge', 'sequence', 'noble', 'sponge', 'dessert', 'roasted', 'parboiled', 'vegetables', 'icebox', 'appointments', 'Thaxters', 'recovering', 'movie-to-be', 'London', 'tranquil', 'tolerance', 'patch', 'grooved', 'accustomed', 'dense', 'mists', 'sun-warmed', 'palisades', 'twinkle', 'woods', 'shopping', 'parish', 'bazaar', 'Prisoners', 'accused', 'accident', 'unsee', \"car's\", 'turnaround', 'Engisch', 'exclaiming', 'rushes', 'gasps', 'portfolio', 'lingerie', 'mink', \"Salter's\", 'servant', 'sandwiches', 'Sitting', 'hour-long', 'Constance', 'plea', 'Hanging', 'patience', 'stab', 'conclusion', 'winds', 'gleaming', 'moontrack', 'poetic', 'misstep', 'fisherman', 'triumphant', 'hysterical', 'Sonny', 'shuddering', 'pitied', 'underneath', 'unquenched', 'enduring', 'ghastly', 'appalled', 'exclamation', 'estranged', 'hangs', 'separation', 'prevented', 'justify', 'scapegoat', 'borne', 'avoidance', 'rankles', 'coolly', 'hurts', 'Mathias', 'persisted', 'explaining', 'advised', \"nobody's\", 'overplayed', 'appraisal', \"Mathias'\", 'towns', 'dispossessed', 'makeshifts', 'arid', 'discarded', 'risen', 'fullest', 'height', 'transcending', 'murky', 'self', 'ignorant', 'self-examination', 'conclusions', 'realizing', 'praise', 'deliverance', 'pettiness', 'greed', 'Methodist', 'eyeglasses', 'drumming', 'accompaniment', 'relentless', 'postponed', 'cremate', 'leaped', 'trestles', 'sheaf', 'whispering', 'parking', 'hearse', 'Jersey', 'chapel-like', 'auditorium', 'discreet', 'symbols', 'faiths', 'impelled', 'kneel', 'Bach', 'healing', 'Lancaster', 'Arms', 'eyelid', 'convivial', 'Pausing', 'Me', 'Umm', 'uhhu', 'Kleenex', 'wiped', 'flatter', 'vanity', 'Caneli', 'stories', 'cleansing', 'orderly', 'hide', 'annoyance', 'sloppy', 'scrub', 'vent', 'dwelt', 'indignities', 'view', 'wears', 'thin-soled', 'dark-gray', 'slacks', 'fawn-colored', 'encountered', 'tweedy', 'Englishman', 'delighted', 'Changing', 'dark-blue', 'remind', 'Parioli', 'loafed', 'Roman', 'Veneto', 'Farnese', 'Gardens', 'Farneses', 'Trastevere', 'piazza', 'Santa', 'Maria', 'eloquent', 'scolding', 'obelisk', 'clung', \"Aren't\", 'drowsily', 'certainty', 'temperament', 'achieved', 'tranquility', 'composure', 'accepting', 'Testament', 'Sistine', 'folklore', 'prophets', 'Catholic', 'Protestant', 'dogmatic', 'atheists', 'Communist', 'wearied', 'ideologies', 'enlargement', 'wearying', 'pondering', 'hurrying', 'joking', 'stair', 'behave', 'softly', 'Ciao', 'gleamed', 'gestures', 'courteous', \"'ello\", 'biscuits', 'Acting', 'interpreter', 'impersonal', 'Watching', 'bounced', 'encouragingly', 'mill', 'seasonal', 'unemployment', 'migrated', 'railway', 'countrymen', 'Regretfully', 'adopted', 'sparkling', 'intellectual', 'mystical', 'Portugal', 'confirmation', 'serenity', 'unaware', 'homely', 'enchantment', 'translate', 'rulers', 'Nodding', 'approvingly', 'confidentially', 'discontent', 'splendor', 'intellect', 'regime', 'Jobs', 'Italians', 'Devout', 'Brooklyn', 'Malta', 'Ireland', 'glowing', 'softening', 'purify', 'caressing', 'flageolet', 'whisper', 'emerge', 'unhesitant', 'encounter', 'disdain', 'outraged', 'proceeded', 'destination', 'bounce', 'press', 'feared', 'presences', 'metabolism', 'weird', 'invested', 'eyeballs', 'thumbs', 'fled', 'minded', 'Elsewhere', 'clusters', 'half-drunk', 'mild', 'commotion', 'hushed', 'imperious', 'denying', 'dread', 'suffer', 'insult', \"girl's\", 'scorn', 'overloud', 'shriek', 'Telling', 'unsheathing', 'curing', 'hides', 'wickedness', 'cunning', 'Speaking', 'loathing', \"helsq'iyokom\", 'murmuring', 'knot', 'threats', 'mingling', 'hurled', 'embodiment', 'defiance', 'howling', 'bullhide', 'tripping', 'seized', 'flog', 'Drive', 'jerking', 'hovel', 'double-married', 'parades', 'confront', 'leads', 'fury', 'stirred', 'mumbling', 'stupor', 'wolves', 'nakedness', 'stale', 'odor', 'Lie', 'curses', 'gnarled', 'talons', 'rudely', 'shoving', 'spruce', 'shortening', 'arc', 'narrowed', 'snarled', 'cracked', 'unchanged', 'delineaments', 'begotten', 'forbidden', 'tracings', 'tree', 'refracted', 'crazed', 'monosyllables', 'unnnt', 'Sssshoo', 'quavering', \"whip's\", 'unbent', 'resigned', 'blows', 'fleeing', 'dodging', 'shadows', 'rocky', 'hid', 'madness', 'glorying', 'mastiff', 'bristling', 'ranted', 'doom', 'enemies', 'gasping', 'draughts', 'incoherent', 'oblivion', 'amused', 'appeasement', 'violence', 'retribution', 'antics', 'disdaining', 'gloom', 'games', 'races', 'paraded', 'grandly', 'attis', 'skeletons', 'paxam', 'dipped', 'arrowheads', 'venom', 'rattlesnakes', 'swift', 'maneuvers', 'firing', 'guns', 'unison', 'indeterminate', 'blankets', 'flew', 'Swan', 'Necklace', 'emulate', 'timidly', 'Yellow', 'Wolf', 'disordered', 'chemistries', 'bolt', 'inaction', 'boiled', 'fermenting', 'juices', 'Alokut', 'challenged', 'matched', 'raced', 'maneuvered', 'abreast', 'cavalry', 'frenziedly', 'regalia', 'preparations', 'combed', 'streaked', 'greased', 'foreheads', 'tails', 'sage', 'hens', 'whitened', 'leggings', 'reflection', 'speeding', 'arrow', 'bow', 'meadow', 'rue', 'admiring', 'envenomed', 'hilltops', 'descend', 'eagle', 'caper', 'cliffs', 'challenge', 'arrogance', 'streaming', 'amulets', 'ripening', 'bellicosity', 'throes', 'shifting', 'Appaloosas', 'Dogs', 'fleet', 'multicolored', 'legion', 'banners', 'drums', 'cacophony', 'accompanying', 'thousand-legged', 'saddles', 'ejaculated', 'hemlocks', 'birch', 'maples', 'quarry', 'unfamiliar', 'stubs', 'suicide', 'expertly', 'root', 'sneakers', 'sliding', 'expanse', 'seating', 'short-cut', 'reedy', 'frogs', 'faded', 'presently', 'vibrant', 'accomplished', 'Paul', 'Easter', 'holidays', 'Evening', 'Dancing', 'attend', 'casually', 'breasts', 'lower-cut', 'inability', 'embarrassment', 'sometime', 'threshold', 'china', 'lamp', 'switch', 'prison', 'indebted', 'Below', 'tunnel', 'fumbled', 'uncovered', 'gripped', 'fearful', 'coursing', 'vessels', 'fabric', 'stains', 'apparent', 'blood-soaked', 'tidiness', 'verge', 'Poldowski', 'owed', 'effete', 'peered', 'wristwatch', 'quarter', 'kneeling', 'tie', 'shoelaces', 'absurdly', 'clumsy', 'strands', 'gauze', 'moist', 'imperative', 'detained', 'freakish', 'cavern', \"Who's\", 'thief', 'Gosh', 'blazer', 'overly', 'knitted', \"Pietro's\", 'finishing', 'float', 'implausibly', 'shaft', 'sunlight', 'oddly', 'partly', \"moment's\", 'paneling', 'inserted', 'eyelids', 'good-bye', 'pounding', 'manage', 'Lincoln', 'parked', 'insolent', 'disdainful', 'Ardmore', 'merest', 'servants', 'born', 'inflection', 'rehearsed', 'analyze', 'Carrie', 'responded', 'restaurant', 'Forget', 'slackened', 'badly', 'disappointing', 'purposeless', 'relinquished', 'Abruptly', 'greatest', 'automatic', 'phrase', 'courtyard', 'tender', 'bumped', 'steadily', 'entry', 'brass', 'screwed', 'kitchenette', 'raw', 'gasp', 'swallows', 'lessen', 'hauled', 'upsets', 'insist', 'sweetheart', 'Happened', 'intoxicated', 'Unfortunately', 'Tony', 'Elliott', 'pinch-hit', \"He'll\", 'nudge', 'postponement', 'bribe', 'double', 'martini', 'relationship', 'Fulbright', 'renovated', 'brick', 'settler', 'clever', 'efficient', 'conformists', 'obnoxious', 'ruffled', 'feathers', 'unreliable', 'irresponsible', 'flyaway', 'jollying', 'Obviously', 'Upstairs', 'showering', 'raindrops', 'pattered', 'grandmother', 'prudent', 'lifetime', 'expressive', \"sister's\", 'Darling', 'mingled', 'blend', 'Leaving', 'supposedly', 'Gregg', 'otherwise', 'jingled', 'scare', \"she'll\", 'defensive', 'Greg', 'depends', 'rescue', 'crises', 'Remembering', 'succession', 'disasters', 'child-cloud', 'youngster', 'fuss', 'accuse', 'looming', 'specter', 'forever-Cathy', 'fiercely', 'piping', 'moon-washed', 'steeped', 'rung', 'stairway', 'brows', 'smoothed', 'muddling', 'worthless', 'clattering', 'overwhelmed', 'junk', 'gushed', 'approaching', 'handkerchief', 'mopping', 'skillfully', 'shape-up', 'groaned', 'gentler', 'grappling', 'outsized', 'armload', 'Scrooge-like', 'lecture', \"cousins'\", 'sympathize', 'limit', \"Cathy's\", 'Lilliputian', 'competing', 'rear', 'flower-scented', 'chilly', 'chillier', 'feeding', 'raffish', 'bobbed', 'gobbled', 'cardinal', 'feathered', 'vestments', 'mate', 'guarding', 'nest', 'sly', 'advantage', 'sentinels', 'abandoning', 'prepare', 'helplessness', 'unscrupulous', 'dump', 'curtains', 'hopping', 'rabbits', 'Goose', 'Mobile', 'hangers', 'neatly', 'fragrant', 'panicky', 'Many', 'Junction', 'phoned', 'sooner', 'napping', 'multitude', 'Methuselah', 'maternal', 'comforting', 'magic', 'Subdued', 'merriest', 'airport', 'fixing', 'hummed', 'voiceless', 'Puzzled', 'flowered', 'forthright', 'silences', 'moth', 'seekingly', 'Shocked', 'elastic', 'cord', 'uncoiling', 'bogey', 'thickened', 'fetching', 'tightened', 'to-and-fro', 'abrupt', \"hall's\", 'purple-black', \"cowbirds'\", 'Cowbird', 'trudged', 'package', \"aunt's\", \"uncle's\", 'Unlike', 'traveling', 'shortest', 'stays', 'Unimpressed', 'plopped', 'disbelieving', 'Esperanza', 'show-offy', 'lifeguards', 'swirling', 'pronounced', 'sherbet-colored', 'mint', 'deserted', 'Eats', 'jiggling', 'jaggedly', \"water's\", 'lumpy', 'trailing', 'dragon', 'Swiss', 'belt', 'corkscrew', 'Canute', 'knight', 'Round', 'Table', 'Sir', 'Brave', 'slaying', 'banshees', 'vampires', 'witches', 'warty', 'astronaut', 'intrepid', 'persecuted', 'fearless', 'scary', 'Tomorrow', 'thump', 'signals', 'radar', 'Son', 'pig', 'cords', 'limping', 'dumb', 'nut', 'polio', 'bedside', 'spilled', 'probly', 'lick', 'Fatso', 'bedtime', 'Strength', 'zip', 'unlaced', 'wadded', 'stripped', 'trunks', 'Goolick', 'goooolick', 'creaked', 'gull', 'dignified', 'bored', 'wedged', 'crawled', 'Watch', 'straps', 'shouders', 'sore', 'boil', \"squatter's\", 'rights', 'Kansas-Nebraska', 'Britches', 'Mark', 'Peters', 'Fifth', 'kinds', 'aleck', 'ripping', 'unconsciously', 'imitating', \"Victoria's\", 'holier-than-thou', 'horrified', 'momentum', 'mama', 'wop', 'wops', 'unimpressed', 'obliged', 'jeans', 'Dingy-looking', 'barber', 'pole', 'spouted', \"cane's\", 'handy', \"Someone's\", 'zoooop', 'snag', 'crook', 'bowl', 'fly', 'bannnnnng', 'stooooomp', 'licking', 'stubby', 'plume', 'collie', 'wire-haired', 'terrier', 'unique', 'DiMaggio', 'tranquilizers', 'braver', 'lately', 'Last', 'jag', 'gotta', 'Squint', 'mornings', 'thereafter', 'blister', 'emerald', 'necklace', 'undying', 'picks', 'nuts', 'Naturally', 'curly', 'runs', 'Personally', 'prefer', 'Continent', 'continent', 'Name', 'pugh', 'beaches', 'scraped', 'Pugh', 'Camels', 'Tripoli', 'harelips', 'Near', 'Galway', 'tinkers', 'caravans', 'gypsies', 'Jerez', 'Really', 'yawn', 'Artfully', 'Cross', 'Korean', 'War', 'spit', 'personally', 'IQ', '141', 'currently', 'Mushr', 'Ozon', 'encyclopedia', 'schnooks', 'Chinaman', 'Tooth-hurty', 'Encouraged', 'imitated', 'described', 'decorated', 'pineapple', 'cherries', 'snuck', 'gumming', 'stumpy', 'panting', 'thirst', 'lagoon', 'staggering', 'Say', 'comic', 'admiringly', 'Either', 'cod', 'salmon', 'sharks', 'bones', 'shrimp', 'encylopedia', 'string', 'Boy', 'Willie', 'Mays', 'outfield', 'je', 'ne', 'quok', 'daydreamed', 'splashed', 'buckets', 'beseech', 'thee', 'sprayed', 'raisin', 'bib', \"Daddy's\", 'Francisco', \"one-o'clock\", 'keen', 'Children', 'organized', 'nap', 'noticing', 'sheets', 'Kool-Aid', 'gagging', 'Sweating', 'drowning', \"afternoon's\", 'Oakmont', 'miswritten', 'heartless', 'prematurely', \"Hadn't\", 'forty-seven', 'twenty-five', 'appalling', 'deserve', 'adult', 'realistic', 'rebound', 'reproach', '1936', 'odds', 'prettiest', 'brightest', 'Allegheny', 'handsomer', 'brighter', 'Pittsburgh', 'seasons', 'Golf', 'goggle-eyed', 'admiration', 'debs', 'claimed', 'skiing', 'crinkles', 'entity', 'Conneaut', \"lovers'\", 'spats', 'heal', 'Dillinger', 'First', \"Cooper's\", 'aggressive', 'thinkers', 'specifically', 'someplace', 'Baltimore', \"Horne's\", 'underestimate', 'giggled', 'bridesmaids', 'choking', 'myth', 'skirts', 'drifted', 'command', 'less-dramatic', 'Sewickley', 'Fox', 'reduced', 'method', 'grindstone', 'Ben', '1938', 'cutest', 'fontanel', 'nary', 'continuing', 'Sally', '1940', '1944', 'dizziness', 'uselessness', 'shuffled', 'missionary', 'Webber', 'eldest', 'elder', 'donated', 'avoid', 'bosoms', 'afterward', 'oversubscribed', 'missionaries', 'bargain', 'whimpering', 'bends', 'overgenerous', 'assignment', 'heir', 'apparency', 'Coopers', 'socially', 'hairpin', 'mourning', 'bites', 'luncheon', 'Le', 'Mont', \"could've\", 'bone-deep', 'sorrow', 'ambitious', 'undo', 'trembled', 'brink', 'settlement', 'soothe', 'capable', 'capturing', 'Years', 'struggled', 'wonders', 'Alloy', 'departments', 'MacIsaacs', 'alloy', 'division', 'Carnegie-Illinois', 'Stuart-family', 'Reuben', 'Pittsburghers', 'sticking', 'dragooned', 'director', 'S.', 'M.', 'jolly', 'factors', 'insiders', 'weighed', 'terms', 'incentive', \"John'll\", 'independent', 'directors', 'agreement', 'Furnaces', 'subordinate', 'advancement', 'meteoric', 'vice', 'biography', 'baby-sitter', 'worth', 'obligated', 'shakily', 'answers', 'frantic', \"Thom's\", \"everything's\", 'Wondering', 'seconds', 'faltered', 'thirteen', 'sitters', 'reliable', 'beautifully', 'ragged', 'spreads', 'law', 'scooted', 'strike', 'hissed', 'thickly', 'lurched', 'raked', 'grabbed', 'insulted', 'takes', 'simmer', 'Tea', 'forgive', 'dime', \"Francie's\", 'televison-record', 'owe', 'grubby', 'filth', 'traipsing', 'curtly', 'collapsed', 'snoring', 'Whether', 'Thankful', 'attending', 'grease', 'confessed', 'pits', 'repaired', 'Seems', 'hired', 'repair', 'promises', 'blew', 'stack', 'Things', 'deserved', 'breaks', 'necks', 'owing', 'received', 'evicted', 'Worst', 'stranded', 'fretted', 'brightened', 'candle', 'embarrassed', 'pleading', 'Straightened', 'flatly', 'uncomfortable', 'darn', 'snobs', 'finance', 'crawl', 'dishes', 'groceries', 'Virus', 'infection', 'lazy', 'no-good', 'alibis', 'excuses', 'Wendell', 'black-balled', 'grocery', 'sewed', 'punks', \"they'll\", 'encouraging', 'July', 'stifling', 'intuition', 'based', 'nine-to-five', 'five-days-a-week', 'medicine', 'shaving', 'sneezed', 'orphanage', 'boarding-home', 'fonder', 'Especially', 'Growing', 'intern', 'lightened', 'heaviness', 'Should', 'interns', 'Ishii', 'resident', 'Medicine', 'sicker', 'post-operative', 'Got', 'lulu', 'alcoholics', 'segregated', 'charity', 'focus', 'forming', 'alcoholism', 'puffy', 'marred', 'tell-tale', 'veins', 'drinkers', 'two-colored', 'bleached', 'unfortunate', 'resemblance', 'forcing', 'disquiet', 'knocked', 'AA', 'describe', 'ounce', 'relaxes', 'chat', 'gladly', 'drawn-back', 'bloodspots', 'spotting', 'emotion', 'negative', 'Papanicolaou', 'vaginal', 'gross', 'lab', 'Late', 'results', 'D.', 'C.', 'hysterectomy', 'intraepithelial', 'situ', 'wryly', 'tells', 'badgering', 'Wants', 'booze', 'grimaced', \"Bancroft's\", 'knob', 'dozing', 'disregarded', 'dilatation', 'curettage', 'proves', 'definite', 'difficulty', 'liquor', 'surgery', 'Mostly', 'coverlet', 'secrets', \"patient's\", 'disregarding', 'protests', 'guinea', 'pigs', 'learn', 'cultivated', 'strongly', 'assure', 'gaped', 'intently', 'half-smile', 'Surprised', 'Guess', 'heart-stopping', 'stammered', 'goodness', 'echoed', 'mockingly', 'Think', 'introduce', 'alcoholic', 'blackout', 'favors', 'alike', \"what's\", 'pathetic', 'disappear', 'caresses', 'tricks', 'ultimate', 'pressures', 'squeezed', 'breathed', 'anesthetic', 'yielding-Mediterranian-woman-', 'soothed', '230', 'drunker', 'metal-tasting', 'suck', 'aqua-lung', 'swim', 'chuckled', 'hazy', 'concentrated', 'sorted', 'duds', 'bellyfull', 'Wild', 'kicks', 'dig', 'twenty-one', 'Orly', 'Rhine-Main', 'April', 'mist-like', 'pocketful', 'grounded', 'Champs', 'Elysees', 'dazzled', 'machines', 'Bugatti', 'Farina', 'coachwork', 'chassis', 'Swallow', 'A40-AjK', 'Mercedes', 'Arc', 'de', 'Triomphe', 'Tour', \"d'Eiffel\", 'yokel', \"Maxim's\", 'Claire', 'feed', 'Handsome', 'soldier', 'assuaged', 'urgency', 'Madame', 'noblesse', 'oblige', 'yeah', 'nymphomaniac', 'Toward', 'waxed', 'philosophical', 'analogies', 'mistake', 'sweep', 'Panther', 'Pils', 'switched', 'Tuborg', 'crocked', 'orbit', 'Capricorn', 'Cancer', 'Elemental', 'debauchery', 'reconciled', 'flop', 'binge', 'therapeutic', 'Sometime', 'snowing', 'snowy', 'dim', 'flakes', 'Pretty', 'poise', 'posture', 'drab', 'propriety', 'unuttered', 'bleak', 'unhappiness', 'profoundly', 'derriere', 'crumble', 'snorted', 'chuckle', 'tiredness', 'infinite', 'reinforce', 'gambit', 'Remy', 'cork', 'Non', 'non', 'lug', 'tart', 'drowsing', 'Allons', 'rested', 'rime', 'melted', 'absolute', \"night's\", 'melancholy', 'vital', 'brightly', 'tousled', 'make-up', \"J'ai\", 'faim', 'wheeled', 'presenting', 'dangerously', 'scald', 'filter', 'tremendous', 'amount', 'hard-boiled', 'garlic', 'Suzanne', 'comfortably', 'remarked', 'Um', 'grunted', 'sipping', 'les', 'putains', 'immense', 'sentence', 'Shall', 'enthusiasm', 'immediately', 'bullet', 'Metro', 'cabaret', 'steamed', 'mussels', 'sommelier', 'magnum', 'steely', 'Jeroboam', 'humorous', \"mind's\", 'responding', 'exhaustingly', \"shores'\", 'powers', 'telescoped', 'plains', 'aqueducts', 'tombs', 'cypress', 'Appian', 'Way', 'Arch', 'Constantine', 'Moreover', 'nursing', 'aunt', 'lengthy', 'illnesses', 'England', 'France', 'Germany', 'Switzerland', 'rendered', 'tonics', 'conceived', 'lamplight', 'Cars', 'taxis', 'buses', 'motorscooters', 'swerving', 'perilously', 'groups', 'German', 'cameras', 'attached', 'Glad', 'outdoor', 'unloading', 'potted', 'placing', 'banked', 'rows', 'shrubs', 'myriad', 'bud', 'ranged', 'fuchsia', 'palest', 'Marvelous', 'portly', 'well-bred', 'Halfway', 'Other', 'banisters', 'parapets', 'Mediterranean', 'underside', 'insides', 'churches', 'Content', 'excited', 'contracted', 'dismay', 'script', \"t's\", \"l's\", 'inclined', 'wobble', 'slope', 'post', 'Alabama', 'mailed', 'next-door', 'neighbor', 'Piazza', 'di', 'Spagna', 'President', 'Republic', 'Inside', 'concerning', 'illness', 'devoted', 'aging', 'immemorial', 'Montgomery', 'sweetpeas', 'mother-of-pearl', 'coveted', 'sigh', 'refolded', 'alas', 'reminiscence', 'weakening', 'values', 'diapers', 'gentility', 'principle', 'lest', 'fraternity', 'pursued', 'mammas', 'aroused', 'uttermost', 'cramp', 'hobbled', 'bathrobe', 'footstool', 'clenched', 'heroically', 'monkey', 'kittens', 'kettle', 'steaming', 'enamelled', 'pan', 'treat', 'hiccups', 'Drop', 'unamused', 'round-eyed', 'woe', 'parting', 'Robbie', 'Beryl', 'imposition', 'Rosie', 'populated', 'Edna', 'Whittaker', 'meets', 'crumbling', 'makeshift', 'sneezing', 'uncertainly', 'unnaturally', 'tray', 'cameos', 'pens', 'papal', 'portraits', 'borders', 'Carrozza', 'escape', 'landing', 'vacant', 'banister', 'vendor', 'well-dressed', 'ascending', 'cast', 'policeman', 'viewed', 'interlude', 'hinting', 'teachers', 'September', 'crisis', 'Balzac', 'Dickens', 'Stendhal', \"Mother's\", 'operation', 'twenty-six', 'volumes', 'Dinsmore', 'arranging', 'Mi', 'diapiace', 'ma', 'insomma', 'indicated', 'stealer', '4000-plus', \"nobody'd\", 'bothered', 'good-living', 'truthfully', 'Sue', 'Much', \"Lucille's\", 'Honest', 'compete', 'trucker', 'carpentry', 'February', 'delivery', 'masculine', 'providing', 'Neither', 'submit', 'penniless', 'helpless', 'Against', \"folks'\", 'crackling', 'vices', 'jockey', 'drinks', 'smokes', 'ticking', 'items', 'swears', 'whoever', 'examined', 'noncommittally', 'scheming', 'Devil', 'Route', '10', 'clutches', 'blushed', 'sexual', 'triggered', 'fierceness', 'Astonishingly', 'summertime', 'fields', 'crickets', 'serenaded', 'huh', 'blissful', 'fulfilling', 'witnesses', 'certificate', 'Infinite', 'contentment', \"Idiot's\", 'upbringing', 'resisted', 'passes', \"Wouldn't\", 'insure', \"Johnnie's\", 'denied', 'contribute', 'doubts', 'petted', 'longest', 'behaved', 'emotionally', 'mere', 'rescued', 'husband-stealer', 'wondrous', 'fitting', 'union', 'Uh-huh', 'Convention', 'eastern', 'represent', 'mixed', 'emotions', 'Faneuil', 'cousin', 'convention', 'introduced', 'Rhode', \"someone's\", 'anxiously', 'twin', 'pumpkin', 'Schmalma', 'shrilly', 'alcohol', 'disguised', 'ginger', 'ale', \"'most\", 'snugly', \"C'mon\", 'lubricated', 'all-American-boy', 'Astronaut', 'vitamin', 'Breakfast', 'Eden', 'two-burner', 'pelting', 'lunatic', 'arrangement', 'Share', 'units', 'bachelor-type', 'eatables', 'curving', 'bloomed', 'haze', 'names', 'magenta', 'downed', 'Lots', 'vitamin-and-iron', 'compound', 'capsule', 'comment', 'beriberi', 'cure', 'Wayne', 'kissing', 'piling', 'ailment', 'confirmed', 'distrust', 'scalded', 'encouragement', 'shy', 'Woods', 'blinds', 'dawning', 'poker', 'secretaries', 'Meanwhile', 'sandy', 'puddles', 'prescription', 'mockery', 'odds-on', 'develop', 'awesome', 'black-and-yellow', 'polka-dotted', 'slicker', 'Three-day', 'natives', 'filthy', 'Pyhrric', 'Fine', 'Thought', 'bake', 'briefly', 'forbore', 'universal', 'weatherproof', 'conviction', 'honeymooning', 'lounging', 'sands', 'unalloyed', 'bliss', 'leafed', 'adventures', 'Seashore', 'Farm', 'Danger', 'agricultural', 'treatment', 'hoof-and-mouth', 'disease', 'cattle', 'hideously', 'illustrated', 'bruising', 'shin', 'choke', 'fuzzy', 'mania', 'dyed', 'climb', 'long-hair', 'according', 'label', 'explanation', 'Bermuda', 'Said', 'permeates', 'squirt', \"Kissin'\", 'Kare', 'handwriting', 'dainty', 'evidence', 'Sorry', 'swiping', 'onions', 'Ugh', 'Must', 'idly', 'fast-frozen', 'guided', 'obscure', 'horse-blanket', 'plaid', 'splashy', 'Pink', 'grown-up', 'clinging', 'infancy', 'toilsome', 'laden', 'clattery', 'mops', 'brushes', 'pails', \"rain's\", 'scorcher', 'scrutinized', 'warn', \"kind's\", 'inviting', 'description', 'Run-down', 'iron-poor', 'Frail', 'feeble', 'blazing', 'varicolored', 'properties', \"four-o'clock\", 'gaudy', 'mist', 'floodlit', 'punctuated', 'refrigerators', 'retreat', 'Cape', \"flower's\", 'transferred', 'M-m-m', 'propped', 'postscript', 'scrap', 'Gee', 'redheads', 'rental', 'browny', 'blondes', 'bikinis', 'preponderance', 'tank', 'Up', 'dune', 'stool', 'bulky', 'turtle-neck', 'sweaters', 'not-so-pale', 'moonlit', 'filched', 'Use', \"goodness'\", 'Sympathy', \"Vivian's\", 'slopping', 'offering', 'peel', 'pajama', 'subside', 'quantities', 'Correspondence', 'glycerin', 'whitens', 'Broiled', 'Puny', 'Surviving', 'Wilderness', 'Speedy', 'Canoe', 'Also', 'canoe', 'overhand', 'innings', 'fifth', \"Anniston's\", 'smacked', \"Riverside's\", 'redheaded', 'relay', 'punches', 'uproar', 'fighters', 'resumed', 'cursed', 'eyeing', 'sonny', 'infuriated', 'Mind', 'goddamn', 'sixth', \"fielder's\", 'rounding', 'heading', 'tagged', 'straddling', 'rammed', \"catcher's\", 'physician', 'injured', 'nearer', 'quivering', 'ram', 'calculated', 'pitching', 'powerfully', 'hander', 'eighth', 'hefty', \"batter's\", 'Hit', 'shortstop', 'bastards', 'Haydon', 'pounded', 'rubber', 'flat-footed', 'unhurried', 'infield', 'Fights', 'squelched', 'helluva', 'team-mate', 'undressing', 'inches', \"Eddie's\", 'unconditional', 'belong', 'confirm', 'Sit', 'steering', 'pro-ball', 'Dazed', 'fielding', 'bases', \"Baseball's\", 'cinch', 'ramming', 'puts', 'sport', 'goddamit', 'ribbons', 'league', 'Someday', \"Springfield's\", 'Talk', 'Ask', 'Springfield', 'Atta', 'outfielders', 'insularity', 'by-ways', 'fascinate', 'Augustine', 'Aquinas', 'Lao-tse', 'Confucius', 'Mencius', 'Suzuki', 'Hindu', 'tomes', 'Krishnaists', 'socio-archaeological', 'papers', 'Zend-Avesta', 'Indian', 'entitled', 'Grinned', 'gloomily', 'laughter', 'formal', 'sixteen', 'substituted', 'fond', 'graduated', 'foundation', 'remoteness', 'thorough', 'bookish', 'lore', 'literature', 'politics', 'awarded', 'practicing', 'sailing', 'distrusted', 'buying', 'shorten', 'relax', 'incarcerated', 'dusted', 'sped', 'plucked', 'wrought', 'resemble', 'instrument', 'stole', 'gusty', 'similar', 'Adams', 'succeeded', 'unusually', 'frankness', 'zest', 'envied', 'occasional', 'participate', 'tea-drinking', 'complied', 'random', 'gatherings', 'flippant', 'superficial', 'pointless', 'persons', 'students', 'judging', 'popular', 'foods', 'equation', 'Zen', 'philosophy', 'modifier', 'Soba', 'udon', 'noisily', 'Sushi', 'Sashimi', 'Asked', 'Witter', 'irritation', 'Lovers', 'Mound', 'Gompachi', 'Komurasaki', 'parkish', 'concluded', 'Anyhow', 'potatoes', 'Kanto', 'bothersome', 'pull', 'Twenty-two', 'twenty-three', 'colder', 'tiniest', 'reflect', 'version', 'ascetic', 'returning', 'waters', 'gate', 'spigots', 'caged', 'incarnation', 'creature', 'overwhelmingly', 'likened', 'limpid', 'fountain-falls', 'familiarity', 'Into', 'hundred-yen', 'urge', 'bronze', 'priest', 'Instead', 'stare', 'rigidly', 'fascination', 'metallic', 'scraping', 'stillness', 'severe', 'emptied', 'pockets', 'coins', 'hurried', 'stairways', 'virus', 'process', 'reminder', 'consciousness', 'lingered', 'inexplicable', 'elements', 'reality', 'resolved', 'farm', 'snow-fence', 'wheels', 'loaded', 'multi-colored', 'graceful', 'throttle', 'fright', 'idle', 'movement', 'tubular', 'interlaced', 'jet-black', 'astonishing', 'unthinkable', 'fire-colored', 'lumps', 'forepart', 'fieldmice', 'satin', 'newly-plowed', 'target', 'shone', 'sun-burned', 'crumbled', 'paleness', 'dusty', 'uncolored', 'triangular', 'plowshares', 'scouring', 'stamping', 'ferocity', 'frenzied', 'impatience', 'fierce', 'hunched-up', 'determination', 'whipped', 'lumbering', 'halt', 'frenetic', 'savagery', 'glimmer', 'remnant', 'ache', 'fumes', 'crushed', 'vanish', 'mount', 'bubbly', 'finely-spun', 'effeminate', 'eyelashes', 'brushlike', 'newly-scrubbed', 'resembled', 'splintered', 'bitten', 'poisonous', 'foolishly', 'gray-looking', 'beautifully-tapered', 'flattened', 'vise', 'splayed', 'wrenched', 'thickest', 'colored', 'crystals', 'ice-feeling', 'spasm', 'muscles', 'wrenches', 'tool-kit', 'gleeful', 'puzzled', 'brute', 'nasty', 'beast', 'slap', 'wipe', 'insolence', 'glee', 'unglued', 'tidings', 'Tax', 'alma', 'mater', 'snapper', 'enrolled', 'credentials', 'transcript', 'references', 'boards', 'applicants', 'co-operate', 'qualifications', 'enroll', 'merchant', 'Gone', 'reunions', '1935', 'old-grad-type', 'Alcorn', \"Pete's\", 'rejected', 'football', 'B', \"A's\", 'math', 'temples', 'chemistry', 'Getting', 'politicking', 'conscience', 'recommend', 'candidate', 'rugged', 'Height', \"6'\", 'Weight', '160', 'Health', 'excellent', 'astronomy', 'geology', 'enrolling', 'keyboard', 'ruining', 'Venetian', 'bestowed', 'bucket', 'commending', 'eyebrow', 'amusement', 'soul-searching', 'participated', 'high-school', 'activities', 'civic', 'flurry', 'shoved', 'unsealed', 'omitted', 'whistled', 'locked', 'gasser', 'rounded', 'pre-packed', 'State', 'rocket', 'messing', 'Real', 'Yale', 'thinning', 'unease', 'Weakness', 'Limited', 'darned', 'Martini', 'tasted', 'meal', 'salad', 'surcease', 'ease', 'sprinkle', 'navy-blue', 'shag', 'woolly', 'Board', 'Work', 'midweek', 'eight', 'commencement', 'Carefully', 'loudly', 'Chairman', \"Partlow's\", 'proverbial', 'pie', 'methodically', 'Burke', 'straight-A', 'antisocial', 'lone', 'completely', 'one-sided', 'Girls', 'parent', 'frayed', 'coffeepot', 'tattered', 'book-lined', 'dilapidated', 'Wrong', \"Dave's\", 'motives', \"Here's\", 'belligerence', 'Already', \"Anne's\", 'acting', 'Daley', 'basketball', 'play-off', 'remains', 'Astronomy', 'stars', 'Geology', 'fishpond', 'suggest', 'chess', 'Music', 'season', 'rock-and-roll', 'combo', 'solos', 'guitar', 'Rich', 'sax', 'solo', 'shine', 'Beethoven', 'mollified', 'brutally', 'loosen', 'padding', 'document', 'ham-radio', 'sale', 'curb', 'applying', 'towel', 'hemming', 'guild', 'banker', 'conform', 'leader', 'well-worn', 'chip', 'qualities', 'Give', 'Army', 'ninety-nine', 'generals', 'mailbox', 'arguing', 'monstrous', 'proportions', 'cotton', 'ruled', 'Lying', 'messy', 'negligent', 'strewn', 'fun', 'kidding', 'trail', 'littered', 'rug', 'assertive', 'grinning', 'refusal', 'stoop', 'petals', 'seed', 'catalogues', 'referred', 'flower', 'contain', 'patina', 'veining', 'reveal', 'column', 'vertebrae', 'magnificently', 'unyielding', 'segments', 'bone', 'cracking', 'noises', 'stem', 'tulip', 'bluntly', 'seriously', 'leisure', 'beforehand', 'delights', 'emotional', 'clash', 'invigorating', 'tenth', 'teasing', 'pat', 'superhuman', 'neatness', 'incongruity', 'mole', 'Quietly', 'foil', 'doorbell', 'Ten', 'cross', 'bend', \"Bill's\", 'deposited', 'strained', 'glissade', 'instructed', 'ellipsis', 'topic', 'Brainards', 'aback', 'contrary', 'weaken', 'fumed', 'adamant', 'raving', 'titters', 'whisperings', 'scabrous', 'unclean', 'gossiping', 'lowering', 'hoarseness', 'crouched', 'Afterwards', 'apologized', 'annoyed', 'pallid', 'resolution', 'About', 'newly', 'Angrily', 'delayed', 'preferably', 'lasted', 'deciding', 'postpone', 'mercy', 'These', 'stray', 'inspection', 'ever-present', 'visitors', 'blot', \"bedroom's\", 'countenance', 'Quite', 'lighthearted', 'witty', 'gaily', 'anecdote', 'frothy', 'deceptive', 'merriment', 'snort', 'mock', 'ritual', 'relations', 'fatal', 'scattering', 'possessions', 'drawers', 'impinge', 'Bizarre', 'bizarre', 'Six', 'Pursuing', 'trapped', 'Day', 'dilemma', 'frantically', 'seeking', 'exit', 'Alternately', 'periods', 'hostile', 'defeatism', 'sullenly', 'morosely', 'simplicity', 'occurred', 'frenzy', 'Beside', 'Instantaneously', 'immeasurable', 'panties', 'alongside', 'particular', 'humiliation', 'Furthermore', 'maneuver', 'endlessly', 'slip', 'brassiere', 'girdle', 'traversed', 'installment', 'rumpled', 'childishness', 'looseness', 'untouched', 'disciplined', 'unruly', 'underclothes', 'linked', 'visibly', 'magnificence', 'virility', 'analyzing', 'scheme', 'exalted', 'fanaticism', 'nylon', 'tenderly', 'focused', 'Slowly', 'reasoning', 'grasped', 'implications', 'occurring', 'Extending', 'inch', 'swiftly', 'unsteady', 'breakfasted', 'refer', \"That'll\", 'resurgence', 'industrialist', \"nibs'\", 'racket', 'glory', 'inadvertent', 'agency', 'brother-in-law', \"Hershey's\", 'draft', 'Eddyman', 'responsible', 'eminence', 'detached', 'estate', 'skyscraper', 'provide', 'Plastic', 'skeleton', 'sorts', 'undergone', 'vicissitudes', 'engaged', 'Shortly', 'proliferation', 'bold', 'exposed', 'suggestions', 'prospering', 'influenced', 'involvement', 'familial', 'loyalty', 'aid', 'prodded', \"Joan's\", 'competency', 'factor', 'eventual', 'disposal', 'unquestionably', 'entwined', 'likely', 'richly', 'Whatever', 'factory', 'ceramics', 'experimentally', 'high-speed', 'calculators', 'political', 'additional', 'worlds', 'conquer', 'Heavy', 'industry', 'slanted', 'inexhaustible', 'coffers', 'attracted', 'Auto', 'Company', 'medium-sized', 'manufactured', 'four-wheel-drive', 'vehicles', 'off-road', 'over-large', 'misguided', 'optimism', 'Cursed', 'dissatisfied', 'stockholders', 'amalgamation', 'mergers', 'consequences', \"Herberet's\", \"Allstates'\", 'folly', 'airplane', 'sub-assembly', 'tanks', 'missiles', 'ordnance', 'desiring', 'Freed', 'complaisant', 'broader', 'overall', 'administration', 'corporate', 'structure', 'wider', 'enchanted', 'proposition', 'Chrysler', 'acquainted', 'hardware', 'Air', 'Force', 'technical', 'Missiles', 'grabs', 'manned', 'affects', 'procurement', 'transports', 'revolutionized', 'airline', 'compound-engine', 'planes', 'companies', 'competition', 'assembled', 'settles', 'bound', 'pages', 'assures', 'raising', 'legal', 'exhibit', 'deadly', 'dampening', 'elderly', 'caution', 'appreciate', 'lukewarm', 'entering', 'fray', 'cudgels', 'prospect', 'mix', 'commercial', 'patriarchy', 'aide', 'Hamilton', 'prevailed', 'devoting', 'swaying', 'dismissed', 'project', 'unwilling', 'interfere', 'transfers', 'hyphenated', 'Clay', 'backing', 'deficit', 'beset', 'deliver', 'elephant', 'Stock', 'abundance', 'Confronted', 'plumped', 'drastic', 'liquidation', 'summoned', 'bind', 'addition', 'defects', 'closeted', 'motor', 'existence', \"A-Z's\", 'set-up', 'exposure', 'conceptions', 'coincidental', 'failure', 'out-dated', 'small-car', 'manufacturer', 'dimensions', 'broach', 'Initially', 'passenger', 'discouraging', 'models', 'founding', 'nationwide', 'dealerships', 'cheaply', 'presses', 'dies', 'precisely', 'hurdle', 'insurmountable', 'investigate', 'marshal', 'statistics', 'arguments', 'Taking', 'dubious', 'alternative', 'breathtaking', 'frank', 'resolve', 'guarantee', 'audience', 'advocating', 'publicly', 'Forgive', 'attended', 'staff', 'presiding', 'incredulity', 'listeners', 'animosity', 'quarrels', 'transact', 'larger', 'low-priced', 'bucking', 'old-fashioned', 'sell', 'economy', 'merit', 'romance', 'snobbery', 'European', 'woo', 'consumer', 'bludgeon', 'chromed', 'excess', 'seduction', 'dealers', 'ready-made', 'steam', 'yachts', 'Georgian', 'bloated', 'too-expensive', 'Allstates-Zenith', 'debate', 'raged', 'Financing', 'emerged', 'obstacle', 'contributed', 'maximum', 'underwrite', 'department', 'risk', 'risky', 'basis', 'capital', 'Heads', 'instinctively', 'onus', 'recriminations', 'broadcast', 'availing', 'in-laws', 'Sweat', 'forehead', 'disquietude', 'Across', 'saluted', 'jubilantly', 'encircled', 'forefinger', 'Spike-haired', 'burly', 'red-faced', 'decked', 'horn-rimmed', \"officers'\", 'expect', 'episode']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  0    1    2    3    4    5    6    7    8    9 \n",
            "       English    0  185  525  883  997 1166 1283 1440 1558 1638 \n",
            "German_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Package udhr is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# journal code brown\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "# Create CFD mapping genres to their words\n",
        "fd = nltk.ConditionalFreqDist(\n",
        "    (genre, word)\n",
        "    for genre in brown.categories()\n",
        "    for word in brown.words(categories=genre)\n",
        ")\n",
        "# Filter for 'news' and 'romance' genres\n",
        "genre_word = [\n",
        "    (genre, word)\n",
        "    for genre in ['news', 'romance']\n",
        "    for word in brown.words(categories=genre)\n",
        "]\n",
        "print(len(genre_word)) # Total (genre, word) pairs\n",
        "print(genre_word[:4]) # First 4 pairs: [('news', 'The'), ('news', 'Fulton'), ...]\n",
        "print(genre_word[-4:]) # Last 4 pairs: [('romance', 'like'), ('romance', 'she')]\n",
        "# Create CFD for selected genres\n",
        "cfd = nltk.ConditionalFreqDist(genre_word)\n",
        "print(cfd.conditions()) # Output: ['news', 'romance']\n",
        "print(cfd['news']) # FreqDist of all words in 'news'\n",
        "print(cfd['romance']) # FreqDist of all words in 'romance'\n",
        "print(list(cfd['romance'])) # List of words in 'romance'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbRIWR1jPxsu",
        "outputId": "d9dc3e9e-8313-4abd-fe4d-cb02d5166b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170576\n",
            "[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')]\n",
            "[('romance', 'afraid'), ('romance', 'not'), ('romance', \"''\"), ('romance', '.')]\n",
            "['news', 'romance']\n",
            "<FreqDist with 14394 samples and 100554 outcomes>\n",
            "<FreqDist with 8452 samples and 70022 outcomes>\n",
            "[',', '.', 'the', 'and', 'to', 'a', 'of', '``', \"''\", 'was', 'I', 'in', 'he', 'had', '?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him', 'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', 'out', 'were', 'up', 'all', 'from', 'could', 'me', 'like', 'been', 'so', 'there', 'they', 'one', 'about', 'my', 'an', 'or', 'is', 'this', 'It', 'them', 'if', 'into', 'But', 'And', 'down', 'when', 'back', 'no', 'what', 'did', 'their', 'do', 'by', 'only', 'your', 'thought', 'which', 'You', \"didn't\", 'then', 'just', 'little', 'time', 'too', 'get', 'who', 'got', 'before', 'know', 'over', 'man', 'because', 'more', 'never', 'way', 'now', 'went', 'we', \"I'm\", 'eyes', 'go', 'came', 'see', 'can', 'old', 'come', 'even', 'are', 'looked', 'other', 'They', 'its', 'knew', 'some', 'much', 'around', 'any', 'There', 'here', 'long', 'than', 'good', 'away', 'felt', 'day', 'own', 'still', 'made', 'take', \"don't\", 'say', 'going', 'how', 'something', 'after', 'through', ':', 'off', 'think', 'In', 'right', 'night', 'where', 'look', 'those', 'again', 'himself', \"I'll\", 'thing', 'first', 'might', 'seemed', 'life', 'very', 'What', \"wasn't\", 'always', 'left', 'make', 'young', 'put', 'being', 'people', 'while', 'took', 'two', 'turned', 'A', 'nothing', 'saw', 'told', 'head', \"couldn't\", 'home', 'asked', 'place', 'room', 'must', 'His', 'mother', 'face', 'wanted', 'last', 'Phil', 'door', 'next', 'will', 'against', 'anything', 'us', 'Then', 'No', 'herself', 'enough', 'morning', 'let', 'Mrs.', 'John', 'once', 'This', 'boy', 'really', 'well', 'tell', 'When', 'few', 'stood', 'want', 'looking', 'course', 'house', 'big', 'feel', 'hand', 'ever', 'woman', 'why', 'Well', 'find', 'until', 'cold', 'kind', 'water', 'years', 'voice', \"wouldn't\", 'son', 'All', 'Mr.', 'along', \"I'd\", 'black', 'gave', 'sat', 'work', 'better', 'should', 'days', 'love', 'called', 'new', 'For', 'heard', 'small', 'We', 'hands', 'these', 'without', 'same', 'white', 'hair', 'sure', 'great', 'things', 'Lucy', 'church', 'men', 'That', 'else', 'though', 'At', 'Her', 'done', 'found', \"hadn't\", 'Now', 'both', 'Just', \"It's\", 'give', 'Why', 'If', 'Miss', 'Mike', 'everything', 'many', \"I've\", 'moment', 'walked', 'myself', 'job', 'Cady', 'kept', 'girl', 'clothes', 'keep', 'has', 'world', 'another', 'most', 'baby', 'stopped', 'beautiful', 'together', 'our', 'gone', 'Yes', 'pale', 'talk', 'Linda', 'Johnnie', 'each', 'light', 'having', 'money', \"can't\", 'leave', 'thinking', 'Oh', \"Don't\", 'end', 'call', 'field', 'help', 'alone', 'mind', 'Myra', 'Theresa', 'Not', 'smiled', 'began', 'sun', 'sound', 'Anne', 'father', 'every', 'trying', 'bed', 'whole', 'family', 'idea', 'God', 'wrong', 'Wally', 'women', 'toward', 'Maggie', 'started', 'stay', 'quite', 'tried', 'wish', 'between', \"she'd\", 'suddenly', 'slowly', 'Spencer', 'Bobbie', 'Martin', 'Cousin', 'Eddie', 'Deegan', 'early', \"that's\", 'wife', '(', ')', 'yet', 'such', 'feet', 'used', 'feeling', 'My', 'business', 'weeks', 'George', 'snake', 'watching', 'seen', 'children', 'high', 'doing', \"isn't\", 'am', 'already', 'Henrietta', 'Owen', 'dark', 'Alexander', 'As', 'car', 'Susan', 'Freddy', 'since', 'happened', 'pretty', 'taking', 'care', 'coffee', 'lived', 'table', 'blue', 'body', 'sitting', 'sorry', 'turn', 'full', 'almost', 'brought', 'across', 'air', 'later', 'letter', 'Cathy', 'Dolores', 'given', 'part', 'matter', 'nice', 'fine', 'saying', 'So', \"won't\", 'open', 'week', \"That's\", 'Sam', 'ball', 'tractor', 'shorts', 'William', 'Old', 'above', 'front', 'making', 'side', 'pink', 'rest', 'lot', 'child', 'set', 'three', 'half', 'talking', 'How', 'rather', 'word', 'couple', 'true', 'clear', 'heart', 'bright', 'real', 'Nadine', 'Richard', 'stop', 'upon', 'hear', 'One', \"You're\", 'close', 'picked', 'hard', 'run', 'getting', \"he'd\", 'loved', 'hours', 'Of', 'beside', 'breakfast', 'held', 'chance', 'standing', \"you're\", 'Rachel', 'waited', 'Hanford', 'Willis', 'To', 'Or', 'past', 'dead', 'behind', 'town', 'waiting', 'also', 'perhaps', 'Eugenia', 'New', 'second', 'need', 'talked', 'red', 'caught', 'moved', 'far', 'hot', 'school', 'age', 'girls', 'bad', 'maybe', 'Poor', 'Anniston', 'evening', 'street', 'remember', 'stared', 'times', 'words', 'dropped', 'instead', 'American', 'under', 'fell', 'ought', 'use', \"'\", 'Doc', 'year', 'yourself', \"He's\", 'Japanese', 'Tommy', 'believe', 'understand', 'soon', 'listening', 'rock', 'apartment', 'Edythe', 'refrigerator', 'Man', 'name', 'With', 'fingers', 'yellow', 'color', 'Even', 'heavy', 'arms', 'lost', 'opened', 'company', 'office', 'York', 'everybody', 'laughed', 'floor', \"it's\", 'doctor', 'ran', 'live', \"haven't\", 'read', 'coming', 'note', 'On', 'certain', 'remembered', 'seeing', 'friends', 'wondered', 'swimming', 'probably', 'realized', 'Cromwell', 'large', 'boys', 'straight', 'quietly', 'Carla', 'Jim', 'liked', 'sounds', 'tiny', 'Did', 'hold', 'less', \"doesn't\", 'death', 'afternoon', 'Do', 'neck', 'sick', 'kitchen', 'bedroom', 'bit', 'brown', 'shoes', 'meeting', 'fire', 'spoke', 'best', 'hour', 'party', 'met', 'walking', 'rain', 'gray', 'short', 'move', 'green', 'Navy', 'Maybe', 'least', 'college', 'added', 'guy', 'interest', 'kill', 'late', 'Charlie', 'catcher', 'Frankie', 'six', 'tongue', 'nobody', 'thin', 'anyone', 'silent', 'steps', 'inside', 'running', 'ten', 'months', 'answer', 'within', 'eye', 'knowing', 'mouth', 'watch', 'tired', 'Chris', 'living', 'worry', 'during', 'outside', 'war', 'wore', 'meet', 'strange', 'stayed', 'dressed', 'lips', 'simply', 'lay', 'meant', 'handsome', 'sent', 'surprised', 'smile', 'able', 'someone', 'After', 'does', 'stick', 'different', 'today', 'hall', 'reason', 'may', 'finally', 'group', 'among', 'speak', 'shouted', 'ready', 'taken', 'afraid', 'colors', 'Quint', 'Pete', 'Lucille', 'Vivian', 'country', 'except', \"shouldn't\", 'known', 'city', 'low', 'walls', 'alive', 'sleep', 'case', 'leaned', 'wet', \"She's\", 'warm', 'wide', 'ones', 'seem', 'Grandma', \"Let's\", 'government', 'sort', 'forever', 'teeth', 'anyway', 'married', 'yes', 'followed', 'drove', 'line', 'hope', 'promise', 'drank', 'please', 'try', 'husband', 'wished', 'wait', 'till', 'sea', 'passed', 'clean', 'drink', 'ahead', 'cry', 'sometimes', 'mean', 'visit', 'tomorrow', 'book', 'walk', 'themselves', 'drew', 'wonder', 'needed', 'innocent', 'ask', 'Your', 'hell', 'Via', 'Paris', 'beach', 'manager', 'pitcher', 'Dave', 'tone', 'Yet', 'thick', 'present', 'Once', 'carry', 'paid', 'taste', \"man's\", 'Perhaps', 'spent', 'rose', 'window', 'beyond', 'threw', 'either', 'bring', 'whose', 'beginning', 'near', 'corner', 'hearing', 'food', 'moments', 'strong', 'changed', 'stairs', 'dress', 'settled', \"weren't\", 'supposed', 'Something', 'start', 'trip', 'person', \"she's\", 'hardly', 'class', 'others', 'imagine', 'show', 'nodded', 'beard', 'possible', 'answered', 'regular', 'jacket', 'turning', 'trouble', 'Nobody', 'minutes', 'religion', 'luck', 'game', 'longer', 'Captain', 'paper', 'knife', 'stand', 'returned', 'anger', 'Nothing', 'bench', 'ago', 'cool', 'crazy', 'hate', 'shut', \"you've\", 'decided', \"What's\", 'ugly', 'scared', 'president', 'Elec', 'Warren', 'club', 'biwa', 'Partlow', 'broken', 'sweet', 'rising', 'dog', 'smelled', 'wall', 'shop', 'Only', 'top', 'pay', 'fifteen', 'order', \"There's\", 'dream', 'quickly', 'ground', 'happy', 'shook', 'strength', 'plenty', 'Evans', 'laughing', 'working', 'truth', 'bottle', 'evil', 'burst', 'wearing', 'poor', \"you'll\", 'nearly', 'forth', 'Robards', 'Let', 'dear', 'difficult', 'change', 'marriage', 'forward', 'stomach', 'special', 'spirits', 'summer', \"We'll\", 'notice', 'Doaty', 'flowers', 'play', 'pulled', 'sense', 'mine', 'earth', 'necessary', 'sky', 'impossible', 'hospital', 'English', 'faces', 'image', 'considered', 'Buzz', 'worked', 'happen', 'fashion', 'bag', 'shirt', 'Is', 'quiet', 'guess', 'fortune', 'doubt', 'Here', 'four', 'building', 'shrugged', 'Gratt', 'somebody', 'asleep', 'picture', 'painting', 'Salter', 'Rome', 'Look', 'lodge', 'mad', 'sharp', 'phone', 'Dad', 'Cooper', \"John's\", 'Alice', 'Charlotte', 'Ryusenji', 'temple', 'Hamrick', 'neither', 'pushed', 'distance', 'touch', 'shoulders', 'climbed', 'sister', 'Italy', 'foot', 'center', 'trees', 'glass', 'porch', 'holding', 'named', 'smoke', 'older', 'leg', 'pointed', 'showed', 'lines', 'somewhat', 'shining', 'space', 'figure', 'pleasure', 'flesh', 'became', 'comfortable', 'carried', 'round', 'figures', 'minute', 'twenty', 'stock', 'fast', 'somehow', 'begged', 'hoped', 'die', 'anybody', 'Jenny', 'nose', 'weather', 'marry', 'died', 'peace', 'often', 'speaking', 'indeed', 'concern', 'worried', 'enjoyed', 'glad', 'folks', 'send', 'funny', 'safe', 'daughter', 'garden', 'duty', 'whatever', 'narrow', 'An', 'understood', 'Come', 'eating', 'arm', 'grow', 'leaving', 'letters', 'watched', 'main', 'mirror', 'says', 'wonderful', 'ship', 'Riverside', 'dinner', 'explained', 'suppose', 'desk', 'hotel', 'conversation', 'smiling', 'dollars', 'slightly', \"he's\", 'studied', 'carrying', 'entered', 'Heiser', 'further', 'seven', 'From', 'grinned', 'fight', 'hurt', 'naked', 'suit', 'whether', 'wants', 'Donald', 'By', 'Burns', 'reading', 'fog', 'length', 'busy', 'Julia', 'Julie', 'Sabella', 'Mousie', 'Ken', 'fall', 'Emma', 'Kirby', 'sweater', 'lotion', 'dugout', 'Fudo', 'College', 'leadership', 'David', 'rang', 'month', 'parents', 'orange', 'beneath', 'become', 'onto', 'direction', 'soul', 'guessed', 'memory', 'houses', 'cemetery', 'rich', 'sit', 'service', 'pocket', 'pipe', 'chest', 'sign', 'reached', 'moving', 'Suddenly', 'sight', 'skin', 'sake', 'rolled', 'slid', 'glance', 'expected', 'closer', 'Stuart', 'social', 'insisted', 'keeping', 'envelope', 'Every', 'bread', 'sighed', 'jobs', 'wondering', 'grew', 'desperate', 'remained', 'hated', 'worse', 'Lord', 'suspected', 'Tolley', \"He'd\", 'fact', 'laugh', 'none', 'write', \"You've\", 'spring', 'Dr.', 'damn', 'horse', 'horses', 'Gunny', 'Jen', 'quick', 'suggested', 'pleased', 'tears', 'unhappy', 'giving', \"you'd\", 'return', 'driving', 'shame', 'Have', 'Club', 'ideas', \"Doaty's\", 'knowledge', 'curious', 'music', 'stone', 'sad', 'growing', 'Outside', 'Go', 'sounded', 'interested', 'cottage', 'terrible', \"Can't\", 'places', 'reasons', 'Probably', 'limp', 'pick', 'defense', 'Who', 'voices', 'easy', 'grown', 'soft', 'ward', 'approach', 'stag', 'admit', 'finished', 'road', 'played', 'bottom', 'glasses', 'telling', 'raised', 'books', 'usual', 'five', 'appeared', 'blonde', 'dance', 'Somers', 'Gansevoort', 'sir', \"Spencer's\", 'missing', 'Are', 'cannot', 'coat', 'teacher', 'fighting', 'Sometimes', 'deep', 'Out', 'hit', 'kid', 'Shafer', 'wedding', 'drive', 'ham', 'truck', 'miserable', 'drunk', 'looks', 'closet', 'advertising', 'rooms', 'kids', 'studio', 'younger', 'bitter', 'attention', 'guests', 'habit', 'Was', 'cried', 'decision', 'funeral', 'bathroom', 'Pope', 'breath', 'whip', 'yours', 'future', 'wind', 'flat', 'Will', 'dolls', 'guest', 'Stop', 'Victoria', 'honest', 'Gladdy', 'cut', 'beauty', 'stake', 'Alma', 'pill', 'peaked', 'team', 'Lee', 'Rossoff', 'student', 'Tokyo', 'interests', 'science', 'Adam', 'nor', 'streets', 'missed', 'unusual', 'youth', 'Sunday', 'passing', 'Street', 'wine', 'passion', 'cared', 'park', 'playing', 'cards', 'neighborhood', 'wood', 'owned', 'whom', 'price', 'step', 'Pompeii', 'ride', 'calling', 'stuff', 'Laura', 'toes', 'mood', 'milk', 'smooth', 'brother', 'reach', 'filled', 'excitement', 'proud', 'Little', 'fat', 'developed', 'laid', \"father's\", 'bought', 'plain', 'stove', 'snap', \"let's\", 'sleeping', 'fair', 'relief', 'furnace', 'giant', 'heater', 'camp', 'worries', 'proved', 'burned', 'mentioned', 'Kizzie', 'pictures', 'simple', 'Frank', \"They're\", 'fault', 'exactly', 'lie', 'parties', 'touching', 'fool', 'animal', 'sympathy', 'humor', 'winter', 'rise', 'Those', 'box', 'friendly', 'land', 'hat', 'Adelia', 'Their', 'dancing', 'Papa', 'circle', 'question', 'birds', 'chair', 'nature', 'slipped', 'human', 'thirty', 'impulse', 'fellow', 'captain', 'size', \"there's\", 'harm', 'beat', 'beer', \"You'll\", 'led', 'fish', 'edge', 'gathering', 'principal', 'blood', 'average', 'sunburn', 'Japan', 'putting', 'San', 'patients', 'tight', 'anywhere', 'check', 'angry', 'promised', 'Actually', 'mast', 'chief', 'Base', 'telephone', 'Whitey', \"we're\", 'Parker', 'nervous', 'wildly', 'aware', 'forget', 'helped', 'lawyer', 'explain', 'easily', \"o'clock\", 'eat', 'announced', 'redhead', 'checks', 'ordered', 'Philip', 'letting', 'prisoners', 'listen', 'Three', 'rushed', 'mass', 'study', 'market', 'stepped', 'Europe', 'brilliant', 'follow', 'Reuveni', 'weight', 'difference', 'dying', 'While', 'lean', 'store', 'written', 'handle', 'eleven', 'various', 'actually', 'usually', 'paint', 'Askington', 'profession', 'national', 'twelve', 'point', 'extreme', 'Ah', 'religious', 'path', 'itself', 'friend', 'drop', 'broad', 'pair', 'aside', 'wake', 'stuck', 'nephew', 'happiness', 'stumbled', 'Walitzee', 'pain', 'shouting', 'darkness', 'hollow', 'ears', 'discovered', 'Mother', 'mention', 'Please', 'goes', 'character', 'sand', 'chosen', 'bathing', 'snapped', 'Bill', 'tonight', 'John-and-Linda', 'pool', 'Jack', 'Janice', 'Thom', 'gotten', 'tough', 'lots', 'Joe', 'carefully', 'Tuxapoka', 'Can', 'League', 'salami', 'Tom', 'plate', 'players', 'god', 'Acala', 'furrow', 'snakes', 'motivation', 'electronics', 'situation', 'A-Z', 'Zenith', 'Richert', 'disliked', 'bell', 'noon', 'breaking', 'buzzing', 'screen', 'June', 'below', 'female', 'heat', 'although', 'quarters', 'Saturday', 'hundred', 'suitcase', 'rode', 'swayed', 'Rose', 'spun', 'hatred', 'square', 'hill', 'Love', 'fear', 'coolness', 'brief', 'cane', 'remove', 'Hey', 'swung', 'interrupted', \"ain't\", \"boy's\", 'due', 'shaved', 'bear', 'flying', 'dry', 'deal', 'juice', 'washing', 'odd', \"baby's\", \"She'll\", 'nine', 'interesting', 'ashamed', 'wishes', 'appearance', 'grim', 'buildings', 'poverty', 'regarded', 'East', 'circumstances', 'bills', 'soles', 'dropping', 'cracks', 'range', 'important', 'ice', 'talent', 'Christian', 'especially', 'Hope', 'crisp', 'Laban', 'Roy', 'Zion', \"it'll\", 'favorite', 'unable', 'opening', 'pot', 'Surely', 'Quinzaine', 'poured', 'bother', 'Dunne', 'ways', 'manner', 'danger', 'curled', 'lack', 'expensive', 'showing', \"'em\", 'mighty', 'legs', 'More', 'welcome', 'nurse', 'fit', 'crowd', 'gets', 'See', 'Mama', 'Charles', 'choice', 'awake', 'forgotten', 'Everything', 'closed', 'swelling', 'bare', 'downstairs', 'Rosa', 'tied', 'danced', 'branches', 'offered', 'plan', 'straightened', 'tissues', 'island', 'blame', 'defend', 'ancient', 'bent', \"day's\", 'wild', 'covered', 'hoping', 'tourist', 'trade', 'Most', 'lovely', 'Diego', 'serious', 'States', 'involved', 'drinking', 'obviously', 'lieutenant', 'rage', 'Take', 'training', 'accepted', 'language', 'Two', 'experienced', 'America', 'male', 'terribly', 'key', 'natural', 'certainly', 'member', 'opinion', 'entirely', 'delight', 'hills', 'taxi', 'smart', \"one's\", 'Listen', 'Gertrude', 'frowned', 'lay-sisters', 'effect', 'admitted', 'miles', 'Pacific', 'plays', 'excuse', 'card', 'ate', 'Although', 'crowded', 'combination', 'won', 'champagne', 'dirty', 'arranged', 'authentic', 'Five', 'everyone', 'fourteen', \"We're\", 'broke', 'lump', 'murder', 'cabin', 'news', 'Where', 'deck', 'brace', 'sending', 'signal', 'begin', 'placed', 'joined', 'aboard', 'force', 'number', 'Besides', 'lead', 'positive', 'lose', 'dignity', 'plot', 'agreed', 'hesitate', 'guilty', 'Some', 'alley', 'trousers', 'tightly', 'swinging', 'Like', 'stupid', 'Israel', 'chose', 'nerve', 'relaxed', 'refugee', 'continued', 'skinny', 'imagined', 'gang', 'board', 'maid', 'thinks', 'tire', 'crying', 'damned', 'diamond', 'makes', 'means', 'share', 'gift', 'St.', 'courage', 'chill', 'opposite', 'sales', 'sold', 'doorway', 'painted', 'paintings', 'easel', 'admired', 'joy', 'pure', 'eighteen', 'lemon', 'Dolly', 'assumed', \"women's\", 'lunch', 'equal', 'final', 'release', 'Everyone', 'value', 'advice', 'darling', 'Father', 'arrived', 'knees', 'fountain', 'midnight', 'woke', \"mother's\", 'worrying', 'lady', 'Agnese', 'folded', 'experience', 'Early', 'Spring', 'Pile', 'Clouds', 'spat', 'dreamed', 'damp', 'staring', 'circles', 'Rawlings', 'Millie', 'slept', 'plane', 'vivid', \"aren't\", 'receiver', 'packed', \"Myra's\", 'widow', 'Sure', 'rocks', 'perfect', 'waves', 'yelled', 'cough', 'Better', \"we'll\", 'earlier', 'ruined', 'Chandler', 'steel', 'bride', 'suits', 'blessing', 'managed', 'Francie', 'possibly', 'throw', 'cars', 'position', \"we'd\", 'bureau', 'story', 'anyhow', 'Dick', 'Seven', 'Bancroft', 'Groggins', 'guys', 'sex', 'snow', 'Stubblefield', 'arched', 'Stubblefields', 'pills', 'Somebody', 'spread', 'Always', 'inning', 'bat', 'dressing', 'seated', 'Ricco', 'gonna', 'towards', 'baseball', 'Samuel', 'plow', 'killed', 'soil', 'Good', 'engineer', \"hasn't\", 'Gerry', \"Freddy's\", 'Allstates', 'Wisconsin', 'Ticonderoga', 'boredom', 'smell', 'ripe', 'wrapped', 'dogs', 'kiss', 'dollar', 'bill', 'noticed', 'Drexel', 'higher', 'thinner', 'stockings', 'miniature', 'wrinkled', 'station', 'intervals', 'companion', 'divided', 'ash', 'concrete', 'wooden', 'doors', 'thrown', 'iron', 'Above', 'mysterious', 'form', 'forked', 'gossip', 'silently', 'fed', 'devil', 'determined', 'belly', 'dreaming', 'Nor', 'fist', 'bowing', 'smoked', 'fence', 'torn', 'fold', 'sullen', 'quality', 'fully', 'chin', 'aged', 'goat', 'boiling', 'taut', 'warmth', 'roof', 'player', 'shadow', 'pass', 'raise', 'sandals', 'examining', 'belonged', 'sink', 'shrill', \"Maggie's\", 'complexion', 'sunny', 'activity', 'catch', 'lugged', 'besides', 'machine', 'dresses', 'Evadna', 'Mae', 'hang', 'careful', 'block', 'tea', 'tables', 'serve', 'type', 'happier', 'typewriter', 'whenever', 'waste', 'City', 'icy', 'grip', 'physical', 'climate', 'eternal', 'agencies', 'facilities', 'depression', 'dragging', 'West', 'meanwhile', 'theme', 'personal', 'dentist', 'minor', 'twice', 'coal', 'loosened', \"Grandma's\", 'wretched', 'mainly', 'shivering', 'upper', 'beds', 'mouthful', 'lives', 'brains', 'superior', 'dragged', \"Didn't\", \"Tolley's\", 'empty', 'kissed', 'decide', 'oh', 'Swift', 'Under', \"God's\", 'tore', 'shall', 'gather', 'envy', 'Another', 'effort', 'Mare', 'bless', 'Never', 'mark', 'racing', 'triumph', 'understanding', 'silly', 'Night', 'attentive', 'triplets', 'disappointment', 'pike', 'fetch', 'deeply', 'wear', 'thanks', 'silver', 'Kiz', 'chickens', 'babies', 'lift', 'French', 'pupils', 'gay', 'enormously', 'Blackwell', 'heavily', 'Thank', 'easier', 'moral', 'dawn', 'lying', 'ability', 'Before', 'grave', 'impatient', 'comfort', 'permitted', 'Feeling', 'yesterday', 'bouquet', 'Folly', 'impression', 'eager', 'village', 'ear', 'estimate', 'choose', 'coffin', 'system', 'sharply', 'Next', 'spoken', 'actions', 'Island', 'comes', 'visiting', 'nod', 'lifted', 'hungry', 'lively', 'fallen', 'savage', 'moonlight', 'clouds', 'stolen', 'Momoyama', 'tall', 'bridge', 'tilted', 'considering', 'request', \"doctor's\", 'suffered', 'Such', 'tours', 'Fleet', 'Yokosuka', 'Beach', 'sports', 'Hong', 'Kong', 'marked', 'applied', 'foreign', 'charm', 'date', 'officer', 'decency', 'thumb', 'muttered', 'Everybody', 'charming', 'addressed', 'Spanish', 'United', 'steak', 'sandwich', 'cup', 'remain', 'becoming', 'slight', 'cheerful', 'naturally', 'report', 'Perry', 'convinced', 'executive', 'members', 'draw', 'enjoy', 'spare', 'Pagan', 'Room', 'Friday', 'Thanks', 'rid', 'Our', 'bringing', 'freezing', 'resent', 'pitch', 'retreated', 'knocking', 'proper', 'active', 'whiskey', 'bar', 'showered', 'Golden', 'Calf', 'dice', 'test', 'management', 'dimly', 'lit', 'hopeful', 'cowboy', 'steady', 'build', 'repeating', 'Wrangler', 'spend', 'Sparky', 'loss', 'Hurrays', 'spirit', 'exhibition', 'wheel', 'boss', 'success', \"How's\", 'honey', 'hurry', 'throat', \"name's\", 'offer', 'planned', 'address', 'arrest', 'Greek', 'Somehow', 'handspikes', 'McKinley', 'reported', 'crash', 'heads', 'Small', 'violently', 'swear', 'evident', 'turmoil', 'ideal', 'battle', 'irons', 'followers', 'Wilson', 'Wales', 'useless', 'informed', 'truly', 'paused', 'stands', 'suspicious', 'lowered', 'act', 'failed', 'foolish', 'reward', 'someday', 'shake', 'guilt', 'painful', 'recognized', 'red-haired', 'shocked', 'favored', 'constant', 'favor', 'whispered', 'continue', 'frightful', 'approached', 'glancing', 'fixed', 'paot', 'sides', 'swallowed', 'windows', 'flight', 'peering', 'Yiddish', 'ghettos', 'noses', 'Since', 'worn', 'hung', 'rigid', 'rubbed', 'glanced', 'skirt', 'horrible', 'orthodox', 'amazed', 'section', 'visited', 'freedom', 'sought', 'intimate', 'pace', 'plans', 'begun', 'swing', 'Wednesday', 'nights', 'headlights', 'ceiling', 'complete', 'casual', 'waving', 'listened', 'Memphis', 'mayor', 'balloon', 'mud', 'changing', 'gives', 'Because', 'particularly', 'reflecting', 'thoughts', 'toilet', 'lock', 'encourage', 'bangs', 'break', 'drag', 'clutching', 'cooling', 'overhead', 'magazine', 'delicate', 'details', 'sheet', 'instance', 'covers', 'research', 'armor', 'McKenzie', 'art', 'third', 'cost', 'middle', 'Monmouth', 'page', 'artist', 'drawn', 'golden', 'prominent', 'slick', 'Life', 'photograph', 'library', 'gorgeous', 'studying', 'classes', 'painter', 'embrace', 'common', 'Pendleton', 'modern', 'leaders', 'ugliness', 'Tuesday', 'glove', 'Had', 'signed', 'thank', 'remark', 'weekend', 'river', 'upstairs', 'Harvie', 'disturbed', 'shudder', 'outcry', 'knows', 'absurd', 'connection', 'thousands', 'hanging', 'sacred', 'traffic', 'Again', 'Uhhu', 'wanting', 'history', 'Would', 'lower', 'snuggled', 'somewhere', 'Ferraros', 'tennis', 'Signora', 'North', 'lumber', 'boxcar', 'scornful', 'trembling', 'hideous', 'images', 'pressed', 'Tell', 'wolf', 'upright', 'nagging', 'sticks', 'noise', 'screaming', 'demons', 'Get', 'sank', 'happens', 'enter', 'brain', 'bore', 'thousand', 'whine', 'meaning', 'Move', 'hoarse', 'cave', 'single', 'grace', 'vanished', 'Hell', 'ridge', 'grotesque', 'totally', 'shape', 'committed', 'dive', 'jumped', 'Class', 'ghost', 'remote', 'bandages', 'mess', 'gentleman', 'Strange', 'cheekbones', 'leading', 'Uncle', 'ridiculous', 'general', 'crossed', 'Joan', 'win', 'Okay', \"Jim's\", \"She'd\", 'registered', 'awfully', 'grateful', 'appreciated', 'nervously', 'troubled', 'hers', 'yell', 'sharing', 'silence', 'good-by', 'cowbirds', 'solitary', \"child's\", 'row', 'surrounded', 'tension', 'bet', 'realize', 'hunting', 'buy', \"they'd\", 'tourists', 'curve', 'Quintus', 'intended', 'nightmare', 'Big', 'Mister', 'metal', 'ocean', 'shot', 'duck', \"Isn't\", 'Latin', 'waiter', 'Part', 'glared', 'skipped', 'backward', 'unbearable', 'smashed', 'birthday', 'attractive', 'rarely', 'surely', 'Longue', 'Vue', \"Linda's\", 'ceremony', \"who'd\", 'brave', 'keeps', 'generous', 'Washington', 'successful', 'save', 'stream', 'anxious', 'steels', 'Lovejoy', 'Funk', 'Shirley', \"Nadine's\", 'flushed', 'Roberts', \"Wally's\", 'dozed', 'figured', 'thankful', 'fired', 'cent', 'incredible', 'practically', 'fresh', 'Fairview', 'secret', 'Chief', 'elevator', 'shock', \"woman's\", 'scar', \"We'd\", 'dull', 'slide', 'private', 'Suppose', 'refused', 'tremble', 'acceptance', 'solution', 'deeper', 'recent', 'cognac', 'sober', 'wisdom', 'wise', 'toothbrush', 'bath', 'subject', 'plants', 'Carraway', 'scrawled', 'tense', 'agony', 'cat', 'ladies', 'rides', 'batting', 'affairs', 'driven', 'touched', 'Ronald', 'burning', 'reasonable', 'veranda', \"Kirby's\", 'flu', 'Peony', \"what's-his-name\", 'several', 'Be', 'worst', 'closely', 'wrote', 'absorbed', 'puny', 'base', 'tossed', 'ballplayers', \"Mike's\", 'dirt', \"Richard's\", 'Book', 'Dead', 'Asian', 'basement', 'flames', 'rush', 'searching', 'Fudomae', \"Charlotte's\", 'recall', 'tale', \"brother's\", 'plowed', 'jar', 'brutality', 'Snakes', 'likes', 'push', 'production', 'treasurer', 'firm', 'equipment', 'Anthea', 'expenditure', 'Cap', 'General', 'financing', 'Arthur', \"William's\", 'Herberet', 'plant', 'Ham', \"We've\", 'whirling', 'bats', 'lights', 'obeyed', 'commanded', 'confession', 'ignored', 'blind', 'Without', 'directly', 'Torino', 'Eh', 'stage', 'slapped', 'thighs', 'caressed', 'smoothness', 'rosaries', 'sweeter', 'condemned', 'fig', 'consumed', 'questioned', 'wave', 'exhausted', 'farewell', 'altered', \"Man's\", 'climbing', 'glued', 'stretched', 'roared', 'stucco', 'Sameness', 'sloping', 'upward', 'porches', 'screeched', 'housed', 'scent', 'lawn', 'hedge', 'supporting', 'facade', 'everywhere', 'blocks', 'fortresses', 'property', 'privacy', 'shaded', 'grape', 'licked', 'sinister', 'Aunt', 'provided', 'tribute', 'restless', 'Americans', 'brushing', 'drying', 'tanned', 'arch', 'swept', 'features', 'dreams', 'lapping', \"goat's\", 'sphere', 'Christ', 'jowls', 'swollen', 'teats', 'straining', 'storm', 'finger', 'reassured', 'pretended', \"Concetta's\", 'disfigured', 'People', 'possession', 'rapidly', 'huge', 'basket', 'household', \"wife's\", 'contributing', 'upkeep', 'soap', 'desperation', 'seems', 'baked', 'nursery', 'flannel', 'extra', 'list', 'assets', 'yard', 'elaborate', 'typing', 'bitterly', 'numbers', 'stark', 'cities', 'greedy', 'Western', 'sunshine', 'mountains', 'endless', 'resources', 'huddled', 'offices', 'newspaper', 'protected', 'rates', 'west', 'doubtless', 'Shivering', 'argued', 'song', 'problems', 'dozen', 'fuel', 'gas', 'fifty', 'catastrophe', 'Abernathy', 'ravenous', 'depths', 'widened', 'Presently', 'depend', 'removed', 'installed', 'freak', 'undressed', 'plunged', 'grimly', 'Plenty', 'ruin', 'prove', 'pneumonia', 'span', 'bedspread', 'trusted', 'deliberately', 'joke', \"they're\", 'Mamma', 'pin', 'Pictures', 'camera', 'Indeed', 'marrying', 'Mt.', 'Pleasant', 'Wait', 'hooked', 'Hurry', 'Fairbrothers', 'Make', 'congregation', 'cruel', 'unfair', 'rules', 'thunder', 'preserved', 'thrusting', 'burden', 'remaining', 'sons', 'necessity', 'clay', 'pieces', 'flame', 'haunting', \"Fair's\", 'graves', 'toast', 'absent', 'adding', 'Simply', 'Whipsnade', 'Mist', 'scream', 'assured', 'importance', 'furnishings', 'fail', 'spiritual', 'irritating', 'token', 'barn', 'bite', 'scarf', 'nerves', 'foal', 'laments', 'afterwards', 'robbed', 'mare', 'greatness', 'conflict', 'intensely', 'indifference', 'Tillie', 'aversion', 'chances', 'veterinarian', 'rubbing', 'cook', 'carpet', 'allowed', 'hon', 'cute', 'firmly', \"an'\", 'elegant', 'Sis', 'changes', 'rings', 'trips', 'frame', 'cats', 'Though', 'collar', 'holy', 'reserved', 'regretted', 'swell', 'dare', 'teaching', 'hop', 'hesitation', 'momentary', 'lonesome', 'relish', 'uncomfortably', 'murmured', 'loving', 'extraordinary', 'Hetty', 'tempted', 'childhood', 'School', 'taught', 'profound', 'distaste', 'tireless', 'one-two-three', 'supper', 'balls', 'sighing', 'ignore', 'regard', 'temptation', 'sin', 'creep', 'mission', 'sentimental', 'enclosed', 'grass', 'dew', 'bird', 'sang', 'engine', 'barefoot', 'neat', 'subdued', 'soberly', 'reassurance', 'tame', 'daisies', 'scented', \"lady's\", 'stirring', 'coldly', 'recalling', 'slow', \"dryin'\", \"You'd\", \"Y're\", 'sprawled', 'boots', 'emphasizing', 'aspects', 'whatsoever', 'approval', 'Titus', 'sisters', 'senses', 'gently', 'impressed', 'boat', 'observed', 'laws', 'Great', 'stones', 'grand-daughter', 'heaven', 'graveyard', 'patiently', 'gentle', 'weak', 'wicked', 'crossing', 'rolling', 'roots', 'humming', 'Both', 'smaller', 'gold', 'shade', 'angel', 'tomb', 'dared', 'Miyagi', 'strain', 'ancestors', 'Asia', 'sing', 'tribal', 'hairy', 'warmed', 'South', 'occasions', 'taller', 'lucky', 'melody', 'alien', 'commission', 'granted', 'preserve', 'health', 'rule', \"Officers'\", 'hamburger', \"Navy's\", 'Coast', 'Seventh', 'liberty', 'Sooner', 'Long', 'desperately', 'Bar', 'tinkling', 'trifle', 'practice', 'tweed', 'unmarried', 'slender', 'license', 'therefore', 'quaint', 'heels', 'furiously', 'jerked', 'Uh', 'hint', 'Orient', 'beauties', 'states', 'similarly', 'Four', 'murdering', 'groomed', 'exceptionally', 'Seeing', 'pride', 'learning', 'perfectly', 'singing', 'incident', 'Back', 'uncle', 'literally', 'translated', 'strangers', \"captain's\", 'discipline', 'riot', 'Subic', 'Walt', 'expert', 'Doolittle', 'awful', 'practical', 'jokes', 'sobering', 'cockroaches', 'security', 'uniform', 'confines', 'naval', 'establishment', 'suited', 'dug', 'Naval', 'sonofabitch', 'exclaimed', 'affection', 'dock', 'dusk', 'exercise', 'golf', 'clubs', 'Sierras', 'Toodle', 'Williams', 'Imagine', 'pleasant', 'territory', 'dinners', 'timid', 'lobby', 'disappointed', 'Alaska', 'Eskimos', 'highball', 'apparently', 'satisfied', 'reaction', 'winking', 'outsiders', 'remarks', 'curiously', 'travel', 'salesmen', 'prayer', 'personnel', 'cosmetics', 'appointment', 'calm', 'dining', 'mail', 'bulletins', 'issued', 'asking', 'glisten', 'River', 'adventurous', 'flowerpot', 'spinning', 'seventeen', 'surprisingly', 'gamblers', 'caring', 'gazed', 'mused', 'cleaned', 'entrance', 'support', 'period', 'blackjack', 'parts', 'Nice', 'pushing', 'neglected', 'glorified', 'den', 'waned', 'pistol', 'cash', 'withdrew', 'customers', 'points', 'glamorous', 'frightened', 'oil', 'Tahoe', 'brings', 'properly', 'During', 'opportunity', 'communicate', 'glances', 'dangers', 'chilling', 'heavers', 'Green', 'information', 'sprang', 'revealed', 'topgallant', 'crew', 'struck', 'bellowed', 'response', 'shackled', 'Adrien', 'Deslonde', \"Alexander's\", 'intention', 'confusion', 'terror', 'potential', 'ominous', 'tones', 'Midshipman', 'Tillotson', 'brandishing', 'weapon', 'flush', 'boast', 'investigation', 'questioning', 'court', 'loose', 'flag', 'self-confident', 'appointed', 'murdered', 'harbor', 'instinct', 'Anything', 'midshipmen', 'responsibility', 'uncertain', 'trust', 'wept', 'dedicated', 'seek', 'sobbed', 'fame', 'career', 'safely', 'boatswain', 'squinting', 'shifted', 'inevitable', 'prisoner', 'shackles', 'action', 'fantastic', 'apprentices', 'awaited', 'fur', 'erect', 'cloth', 'dingy', 'heel', 'plodding', 'schools', 'occupied', 'ajar', 'steep', 'baggy', 'skullcap', 'greeted', 'flicked', 'preceded', 'corridor', 'classroom', 'benches', 'shrilled', 'interpretation', 'Books', 'chanted', 'Hebrew', 'tune', 'Each', 'rocked', 'prayed', 'portion', 'rapping', 'incessantly', 'freckles', 'barely', 'accompanied', 'pitched', 'shuddered', 'coin', 'handed', 'thanked', 'Does', 'appear', 'resented', 'faced', 'clicked', 'dangerous', 'slyly', 'wherever', 'helping', 'countries', 'farms', 'clasped', 'clamped', 'relatives', 'retrieve', 'imprisoned', 'hanged', 'energy', 'bright-eyed', 'Anyway', 'upset', 'waking', 'poling', 'fork', 'fellowship', 'pine', 'Patrol', 'gripping', 'blow', 'underwear', 'pregnant', 'dizzy', 'normal', 'daddy', 'raining', 'spectator', 'eggs', 'sheds', 'unlocked', 'discover', 'believed', 'resting', 'works', 'sees', 'plastered', 'afford', 'fill', 'Christmas', 'corn', 'factories', 'uniforms', 'bands', 'symphony', 'sophisticated', 'concert', 'virtuoso', 'nodding', 'cheeks', 'drought', 'August', 'clerk', 'thirty-four', 'conditions', 'Herman', 'connections', 'spot', 'drawings', 'jackets', 'furnished', 'daring', 'cops', 'leather', 'accurate', 'editor', 'including', 'ages', 'transparent', 'furniture', 'costume', 'inner', 'vision', 'reminded', 'bottoms', 'bottles', 'exciting', 'pants', 'shared', 'cheap', 'pressure', 'awoke', 'income', \"Monmouth's\", 'ring', 'richness', 'rent', 'forty-four', 'speech', 'included', 'series', 'Jackson', 'atmosphere', 'vast', 'artists', 'inward', 'title', 'fascinated', 'intelligent', 'replied', 'Manhattan', 'Art', 'scholarship', 'sixty', 'impressionist', 'stimulation', 'farther', 'picking', 'greater', 'bunch', 'cafe', 'clock', 'lonely', 'mostly', 'worship', 'registration', 'Ida', 'accusing', \"Via's\", 'Walter', 'tear', 'killing', 'enjoying', 'chicken', 'frozen', 'wiser', 'foggy', 'crept', 'discussed', 'annual', 'accounts', 'testimony', 'Thaxter', 'driveway', 'nowhere', 'screamed', 'search', 'closets', 'telephoning', 'recalled', 'alarm', 'conversations', 'cliff', 'romantic', 'enormous', 'account', 'vigilance', 'pity', 'void', 'self-pity', 'triumphantly', 'haunted', 'coarse', 'insulting', 'harsh', 'altogether', 'free', 'Ellen', 'minister', 'glistening', 'doll', 'stranger', 'burial', 'parlor', 'surface', 'roses', 'knelt', 'Heaven', 'grief', 'Together', 'bachelor', 'downtown', 'needs', 'purse', 'lipstick', 'solemnly', 'Alberto', 'lies', 'using', 'falling', 'exasperation', 'respect', 'shoe', 'Italian', 'outfit', 'ruins', 'amusing', 'scandal', 'bells', 'ringing', 'Steps', \"Peter's\", 'shaking', 'Bible', 'Chapel', 'Could', 'honestly', 'imagination', 'Being', 'strict', 'Ferraro', 'lap', 'utterly', 'bobbing', 'Canada', 'bush', \"Sam's\", 'boxcars', 'Holy', 'assistance', 'doctors', 'saint', 'cardinals', 'Signor', 'Raymond', \"Carla's\", 'dreamy', 'Dookiyoon', 'Shades', 'stiff', 'facing', 'contempt', 'bleeding', 'Going', 'queer', 'lodges', 'Standing', 'indignant', 'begging', 'hunger', 'bearing', 'indignation', 'shower', 'sting', 'pack', 'teach', 'Keep', 'worrisome', 'smelling', 'whisky', 'TuHulHulZote', 'concerns', 'twisted', 'moon', 'crack', 'settling', 'motionless', 'bark', 'rough', 'edges', 'lash', 'despair', 'settle', 'Turning', 'prophesied', 'Sarpsis', 'wings', 'file', 'hurtling', 'maleness', 'parade', 'nation', \"men's\", 'hearts', 'mounted', 'disheveled', 'hoofs', 'roll', 'monster', 'plunging', 'terrifying', 'pulse', 'clutch', 'downward', 'leaves', 'volume', 'miracle', 'confused', 'Hello', 'obvious', 'wire', 'pretense', 'acute', 'Bentley', 'robe', 'Monday', 'vacuum', 'cleaner', 'expanding', 'shivered', 'wound', 'Alex', 'debt', 'Too', 'adhesive', 'tape', 'however', 'Hall', 'hallway', 'wreck', 'reply', 'gazing', 'brow', 'considerate', 'clearly', \"It'll\", 'disappeared', 'footsteps', 'Pietro', 'forced', 'affectionate', 'respected', 'theater', 'Someone', 'jury', 'struggle', 'tactful', 'facts', 'routine', 'strip', 'constructed', 'secretary', 'train', 'peculiar', 'Websterville', 'anxiety', 'finds', 'Remember', 'keys', 'horizon', 'unhappily', 'startled', 'selfish', 'bundle', 'instant', 'warning', \"Susan's\", 'entire', 'dust', 'breeze', 'darkened', 'radiant', 'nests', 'baffled', \"people's\", 'airy', \"Greg's\", 'faintly', 'lavender', 'buried', 'pause', 'wallpaper', 'hesitated', 'Christmastime', 'speechless', 'teddy', 'delivered', 'jammed', \"Quint's\", 'cottages', 'V-shaped', 'inlet', 'seaweed', 'whiskers', 'blades', 'controlling', 'Fearless', 'piece', 'Fortman', 'Stuck-up', 'loud', 'Gord', 'sweat', 'Aw', 'hole', 'shoulder', 'choppy', 'peppermints', 'Ever', 'Gordon', 'tail', 'soup', 'shaken', 'mackerel', 'yodeling', 'Same', 'King', 'Which', 'mold', 'fought', 'smothered', 'occasion', 'tonsil', 'giggles', 'swam', 'plastic', \"he'll\", 'Fifteen', 'Yeah', 'rope', 'lung', 'caused', 'Jaguar', 'thoroughly', 'chapter', 'community', \"Wasn't\", 'Mountains', 'Dartmouth', 'engagement', 'events', 'jolt', 'Cleveland', 'wishful', 'absolutely', 'shallow', 'frivolous', 'scatterbrained', 'magnificent', 'example', 'correct', 'Others', 'trace', \"Edythe's\", 'glaring', 'graham', 'crackers', 'problem', 'match', \"Bobbie's\", 'decent', 'Smith', '&', 'salesman', 'branch', 'contacts', 'Murkland', 'harder', 'feels', 'opportunities', 'surprise', 'degree', 'sweetly', 'falls', 'staying', 'refuse', 'Pack', 'davenport', 'instantly', 'strode', 'nails', 'state', 'bubble', 'miserably', 'coy', 'unbelievable', 'tide', \"They'd\", 'spoiled', 'bum', 'application', 'logic', 'thousandth', 'complain', 'Hodges', \"mustn't\", \"Gladdy's\", 'credit', 'Michelson', 'promising', 'chatter', 'rounds', 'sights', 'chart', 'diagnosis', 'familiar', 'network', 'violent', 'patient', 'control', 'examination', 'penetrating', 'smear', 'Doctor', 'permission', 'shiver', 'amazement', 'cancer', 'echo', 'outta', 'nitrogen', 'contented', 'mouthpiece', 'aloud', 'vacation', 'Valery', 'army', 'Hun', 'Jour', 'et', 'Nuit', 'Montmartre', 'cheese', 'drained', 'Tropic', 'breathing', 'topcoat', 'umbrella', 'hustler', 'Hardly', 'lied', 'wager', 'Monsieur', 'sadly', 'cynicism', 'rotten', 'thirty-five', 'forty', 'sensed', 'Bon', 'jour', 'hips', 'brushed', 'innocently', 'brush', 'Stephen', 'cups', 'Louvre', 'Express', 'leaning', 'Today', 'Colosseum', 'prescribed', 'noisy', 'frightening', 'Shoals', 'smartly', 'splendid', 'staircase', 'display', 'azalea', 'workmen', 'azaleas', 'blooms', 'purple', 'learned', 'warming', 'cigarette', 'confidence', 'drawer', 'replaced', \"Elec's\", 'grandchildren', \"Emma's\", 'contorted', 'solemn', \"George's\", 'persuade', 'possibility', 'arrangements', 'separate', 'specialized', 'alligator', 'Finally', 'scurried', 'supremely', 'exist', 'Elsie', 'motion', 'Westfield', 'Dolan', 'Young', \"Christians'\", 'faith', \"guy's\", 'willing', 'kitten', 'honorable', 'hopelessly', 'surrendered', 'motel', 'desire', 'awareness', 'Mmm', 'elbow', 'maturity', 'Boston', 'blond', 'separated', 'Need', 'invisible', 'appeal', 'Unit', 'Number', 'lightly', 'victory', 'Nassau', 'Bobbsey', 'Twins', 'variety', 'presence', 'onion', 'wrapping', 'feminine', 'judgment', 'following', 'chic', 'subconsciously', 'collection', 'Larkspur', 'Clearly', 'ninety-six', 'browny-haired', 'sprinkling', 'sympathetic', 'tan', 'Third', \"Deegan's\", 'bastard', 'spectators', 'fielder', 'ballplayer', 'built', 'mound', 'arose', \"Phil's\", 'sweatshirt', 'jaw', 'locker', 'banged', 'professional', 'leagues', 'abruptly', 'shaky', 'product', 'numerous', 'Through', 'seldom', 'studies', 'acquired', 'freighter', 'eaten', 'pretentious', 'throughout', 'Ceecee', 'statue', 'paced', 'sensation', 'hillside', 'demon', \"Fudo's\", 'refusing', 'plowing', 'stubble', 'area', 'lath', 'panic', 'tapered', 'frighten', 'trailed', 'delicately', 'diamonds', 'gracefully', 'recently', 'solid', 'clods', 'pirouette', 'brutal', 'eagerly', 'scholastic', 'record', \"son's\", 'organization', 'Ivy', 'grades', 'elected', 'slammed', 'forum', \"David's\", 'Products', 'Half', 'colleges', 'Hear', 'surprising', 'bass', 'Crazy', 'Horse', 'Colts', 'generally', 'garments', 'resistance', 'stubborn', 'contest', 'lace', 'chain', 'forgot', 'Bryan', 'corporation', 'contracts', \"Willis'\", 'partner', 'power', 'conversion', 'allied', 'aircraft', 'jet', 'automobile', 'promotion', \"Hamrick's\", 'Motors', 'gantlet', 'tower', 'Mass', 'repetitive', 'monotonous', 'unimportant', 'bees', 'tomato', 'paste', 'sour', 'aluminum', 'trays', 'fly-dotted', 'cheesecloth', 'surging', 'bodies', 'dived', 'blackness', 'amber', 'bay', 'Filippo', 'Rossi', 'Signore', 'elders', 'Youth', 'fur-piece', 'wiggled', 'satin-covered', 'buttocks', 'clutched', 'straw', 'strutted', 'streetcars', 'bigger', 'fancy', 'airs', 'ours', 'speeches', 'Dante', 'actresses', 'Henh', 'Calloused', 'polished', 'excitedly', 'puckered', 'chins', 'hairs', 'sprouted', 'tweezed', 'Mauve-colored', 'mouths', 'Puttana', 'fleshy', 'suppleness', 'seams', 'shooing', 'fleas', 'hopped', 'sundials', 'variegated', 'expression', 'flown', 'withered', 'streetcar', 'contemptuous', 'purpose', 'opposition', 'outskirts', 'Philadelphia', 'Bari', 'Chieti', 'Ash', 'Road', 'elevated', 'trains', \"city's\", 'half-hour', 'creek', 'Schuylkill', 'aloneness', 'framed', 'ginkgo', 'lined', 'two-story', 'slashed', 'manure-scented', 'lawns', 'wicker', 'swings', 'rusted', 'hinges', 'stable-garage', 'rot', 'evenings', 'Sundays', 'reeked', 'dregs', 'squatted', 'sidewalk', 'grating', \"Bartoli's\", 'second-story', 'showroom', 'angels', 'surveyed', 'sameness', 'perched', 'slant', 'paved', 'alleyways', 'tunneled', 'core', 'intimacy', 'backyards', 'fences', 'blended', 'courtyards', 'vines', 'Waiting', 'pact', 'heritage', 'ended', 'white-columned', 'eight-thirty', 'local', 'pound', 'gloved', 'vest', 'gloves', 'unconcerned', 'gilded', 'lithe', 'straddled', 'railing', 'loosely', 'creaking', 'pales', 'artfully', 'tapering', 'tips', 'glowed', 'sinewy', 'swirled', 'childlike', 'softness', 'fragile', 'harshness', 'belied', 'lyric', 'contours', 'downcast', 'possessed', 'clouded', 'screeching', 'rail', 'daydreaming', 'wetness', 'claret', 'feasting', 'salt', 'sallow', 'time-cast', 'encrusted', 'marbleized', 'unfalteringly', 'git', 'Soothing', 'whiskered', 'expectantly', 'undulated', 'gradually', 'covering', 'squirted', 'savored', 'earthy', 'operated', 'skilled', 'unity', 'bagpipe', 'pressing', 'pulling', 'delighting', 'evasive', 'cloud', 'Its', 'fluttering', 'soutane', 'sensing', 'portentous', \"dog's\", 'Time', 'meantime', 'stained', 'ocher', \"Pompeii's\", 'edged', 'clapping', 'ecstatic', 'released', 'haunches', 'ticks', 'biggest', 'Niobe', 'neatest', 'Concetta', 'laced', 'Romeo', 'idiot', \"Romeo's\", 'grasp', 'impetuous', 'rotundity', 'throbbed', 'fatigue', 'earnest', 'quench', 'outdistanced', 'recovery', 'highly', 'fuzz', 'flaxen', 'rages', 'digesting', 'peaches', 'cream', 'disposition', 'amounts', 'stated', 'lifting', 'coaxed', 'rackety', 'washer', 'daily', 'chores', 'participating', 'Worry', 'produce', 'salary', 'Clifton', 'preferred', 'bankruptcy', \"family's\", 'vitamins', 'pored', 'squeeze', 'pennies', 'consulted', \"Woman's\", 'Exchange', 'goods', 'starched', 'nineteen', 'nuzzled', 'ironed', 'wrapper', \"Best's\", 'Liliputian', 'Bazaar', 'sensible', 'pencil', 'jotting', 'mothers', 'cribs', 'playroom', 'entail', 'salads', 'beans', 'gypsy', 'fortunes', 'clearing', 'chattering', 'contests', 'November', 'miseries', 'countless', 'temperature', 'zero', 'tangible', 'pall', 'Mile', 'High', 'locking', 'harshened', 'outlines', 'realism', 'resembling', 'habitual', 'sprawling', 'bumptious', 'open-handed', 'basking', 'stored', 'riches', 'jobless', 'employment', 'parks', 'exclusive', 'created', 'depressions', 'markets', 'congested', 'populations', 'centralization', 'industries', 'discriminatory', 'freight', 'popularly', 'creating', 'penetrated', 'spending', 'remedies', 'millions', 'urgent', 'Abernathys', 'stretch', 'mortgage', 'taxes', 'overshoes', 'leaks', 'emergencies', 'termed', 'major', 'disaster', 'maw', 'appeased', 'hurling', 'tons', 'Cold', 'innumerable', 'sprung', 'frames', 'attic', 'shingles', 'capacity', 'fireplaces', 'electric', 'gaiety', 'infected', 'stormbound', 'shipwrecked', 'circumstance', 'anxieties', 'cling', 'survive', 'pipes', 'ankles', 'swabbed', 'bathrooms', 'groaning', 'polar', 'regions', 'mining', 'joys', 'emergency', 'wallow', 'willed', \"Hope's\", 'tipped', 'overturning', 'Thrifty', 'Unusual', 'tenant', 'simplify', 'California', 'jilted', \"sun's\", \"Brace's\", 'sneaky', 'consorting', 'tenants', 'understands', 'Labans', 'valiant', 'pitiable', 'Returning', 'log-house', 'Jonathan', 'stormy', 'Him', 'Put', 'asunder', 'Absolution', 'telegraph', 'message', 'sealed', 'pigeonhole', 'resist', 'deception', 'peculiarly', 'confess', 'lacerate', 'Reaching', 'relic', 'pioneer', 'pyre', 'curl', 'sealing', 'wax', 'melting', 'bubbling', 'feathery', 'beloved', 'gathers', 'fruit', 'subtle', 'genius', 'Swinburne', 'gifted', 'satisfactions', 'Beyond', 'greening', 'Grand', \"parents'\", 'grandsons', 'thimble', 'Brace', 'easygoing', 'blacksmith', 'preacher', 'Howdy', 'careless', 'Oscar', 'P.GA', \"C'un\", 'Major', 'plain-out', 'carte', 'blanche', 'weakness', 'devotion', \"Robards'\", 'Queen', 'mares', 'geldings', 'what-nots', 'reverent', \"Racin'\", 'revolting', 'pearl', 'stamp', 'imperiously', 'stall', 'invariably', 'Nerves', 'nip', 'Stand', 'oneasy', 'Quit', \"sor'l\", 'lip', 'January', \"musn't\", 'annoy', 'Listening', 'predictions', 'procession', 'foals', 'symbolized', 'unpleasant', 'treated', 'acknowledge', 'condition', 'disgusting', 'Human', 'birth', 'novelty', 'midwife', \"Jenny's\", 'former', 'admirer', 'riding', 'quarts', 'liniment', 'fussing', 'bran', 'mash', 'charlotte', 'russe', 'tracking', 'manure', 'jiffy', \"trippin'\", \"ever'\", 'Rhyme', 'Arcilla', 'Flotilla', 'Edmonia', 'Jennifer', 'Kezziah', \"this'll\", 'boy-name', \"Mare's\", 'Handsomest', 'colt', 'Kentucky', 'Strong', 'Royal', 'Jesus', 'distressed', 'peach', 'Home', 'trimmed', 'callers', 'Shawnee', 'Rakestraw', 'criticisms', 'Heavenly', 'Rest', 'boarding', 'inclement', 'Spa', 'snippy', 'namesake', 'teething', 'cordial', \"Tillie's\", \"Nick's\", 'posts', 'pups', 'to-do', 'dollies', 'finest', 'spa', 'entertain', 'nigs', 'invite', \"ever'body\", 'Galt', 'House', 'Jockey', 'affair', 'dang', 'oystchers', \"bar'l\", \"oystchers'll\", 'perk', 'downright', \"Roy's\", 'buggy', 'Eph', 'Showers', 'preach', 'mountain', 'politely', 'gateway', \"Them's\", 'purtiest', 'babes', \"comin'\", 'forgiven', 'absolution', 'testament', 'proof', 'godless', 'wishing', 'garnet', 'counsel', 'inevitably', 'unfavorable', 'frequently', 'circumspect', \"Hetty's\", 'flair', 'drama', 'summers', 'Maneret', 'excellently', 'austere', 'lessons', 'Polish', 'nobleman', 'Craddock', 'supervising', 'wand', 'everlasting', 'teas', 'fetes', 'jocular', 'possessive', 'planets', 'revolved', 'corruption', 'interruption', 'assume', 'spared', \"Charles'\", 'vulnerable', 'argue', 'morality', 'consisted', 'finding', 'oppressive', 'Impossible', 'virtue', 'colloquy', 'sill', 'artificial', 'blossom', 'leaf', 'pilgrim', 'avoided', 'graveyards', 'remembering', 'lacy', 'enchanting', 'wildness', 'Leaning', 'tangle', 'rosebush', 'honeysuckle', 'bloom', 'spray', 'thorns', 'dewdrops', 'buds', 'Valentine', 'balanced', 'nondescriptly', 'artless', 'decidedly', 'une', 'femme', \"d'un\", 'unbelievably', 'protective', 'bun', 'jug', 'equivalent', 'wilderness', 'yielded', 'patchwork', 'william', 'bedstraw', 'grasses', 'haranguing', 'poodle', 'gleefully', 'supplicating', 'prayerful', 'forepaws', 'Rummaging', 'comply', 'slumbered', 'shuttered', 'shops', 'inhabitants', 'eying', 'frankest', 'curiosity', 'bowed', 'princess-in-a-carriage', 'acknowledgments', 'cautious', 'reflective', 'sap', 'plainly', 'cupped', 'repetition', 'announcing', 'nettled', 'unenthusiastic', \"Summer's\", 'scowled', 'vague', 'hospitality', \"They'll\", \"takin'\", 'pleasantly', \"soon's\", 'doubtfully', 'soldierly', 'ceased', 'pointer', 'finer', 'text', \"f'r\", 'happily', 'rests', 'womanly', 'Rests', 'pacifies', 'dad', 'link', 'nineties', 'Blackwells', 'silenced', 'scanty', 'deep-set', 'mariner', 'awed', 'fishing-boat', 'powerful', 'shriveled', \"well's\", 'Ran', 'lawful', 'wedded', \"leavin'\", 'cheerfully', 'Left', 'Any', 'dryly', 'knowingly', 'liking', 'connivance', 'Selma', \"Cotter's\", 'censure', 'praised', 'big-boned', 'drab-haired', 'apron', 'print', 'vaguely', \"gran'dad\", \"Y'r\", \"dam'\", 'porridge', 'Milk', 'sops', 'claws', 'clattered', 'lowly', 'wearily', 'piously', 'Beer', 'affect', 'ingenious', 'hypocrisy', 'forgave', 'stint', 'malevolence', \"Blackwell's\", 'somewheres', \"Ma'am\", 'Delia', 'cruelty', 'tyranny', 'mumbled', 'shuffling', 'mightily', 'buffeted', 'gabble', 'indoors', 'swearing', \"journey's\", 'dotted', 'bushes', 'sheep', 'blossoms', 'scant', 'clawing', 'stony', 'darted', 'rustle', 'placid', 'terrestrial', 'mindless', 'spire', 'lessened', 'headstones', 'Dorothy', 'Tredding', 'nearest', 'sea-damp', 'tracing', 'indecipherable', 'carved', 'padded', 'moss', 'parasol', 'rock-carved', 'nearby', 'scudding', 'fearing', 'ghosts', 'Prefecture', 'northeast', 'Honshu', 'traces', 'Ainu', 'Ainus', 'primitive', 'Southern', 'Apparently', 'Caucasian', 'skins', 'bearded', 'pitiful', 'subsist', 'chants', 'sadness', 'Indians', 'assimilated', 'Akita', 'prefectures', 'occasionally', 'strikingly', 'coloring', 'improved', 'tawny', 'honey-in-the-sun', 'tint', 'invaders', 'fortunate', 'exquisitely', 'willowy', 'susceptible', 'vegetable', 'diet', 'forebears', 'intriguingly', 'rebellious', 'requests', 'Bremerton', 'Lakes', 'Pensacola', 'threat', 'resign', 'Anywhere', 'assigned', 'psychopathic', 'depressingly', 'cases', 'teamed', 'chaplain', 'counselor', 'mental', 'salvage', 'firms', 'operate', 'hulks', 'warships', 'sunk', 'inshore', 'setting', 'nerve-shattering', 'blasts', 'psychiatry', 'off-duty', 'sustain', 'dinnertime', 'cocktail', 'crossroads', 'Ships', 'rotated', 'six-month', \"Fleet's\", 'port', 'maintenance', 'shore', 'ships', 'officers', 'good-looking', 'cubes', 'tipsy', 'consequently', 'tailored', 'carriers', 'aviator', 'fifty-fifty', 'overseas', 'implied', 'excused', 'intimated', 'Harro', 'girl-san', 'catchee', 'boy-furiendo', 'likee', 'beg', 'pardon', 'brand', 'nice-looking', 'open-mouthed', 'blushing', 'blush', 'gesture', 'slang', 'fly-boy', 'lasting', 'satisfaction', \"nurses'\", 'brunettes', 'Moorish', 'Balkan', 'endowed', 'blessed', 'halfway', 'humiliated', 'self-consciously', 'waitresses', 'counter', 'Yuki', 'Kobayashi', 'Bifutek-san', 'Kohi', 'Futotsu', 'whitehaired', 'doting', 'commander', 'appealing', \"Tommy's\", 'rebelliously', 'civilian', 'transient', 'increasingly', 'Nurse', 'Corps', 'congenial', 'after-duty', 'raucous', 'poorly', 'companionship', 'put-upon', 'repeated', 'Eating', 'indigestion', 'bicarbonate', 'soda', 'sulked', 'crude', 'Oyajima', 'kimono', 'faculty', 'hostess', 'gown', 'kotowaza', 'proverb', 'Tanin', 'yori', 'miuchi', 'Relatives', 'thicker', \"Doolittle's\", 'scheduled', 'dispensed', 'ordinarily', 'immature', 'tactics', 'rabble', 'rousing', 'belief', 'fun-loving', 'Guns', 'Appleby', 'perils', 'horseplay', 'stabilizing', 'influence', 'overdeveloped', 'wisecracked', 'indoctrination', 'slaughtering', 'ethyl', 'chloride', 'Armed', 'Forces', 'heights', 'eventually', 'retired', 'restriction', 'Bustard', 'deprived', 'Boats', 'McCafferty', 'restricted', 'thoughtful', 'medical', 'supplies', 'Supply', 'inspect', 'caves', 'shipmate', 'connect', 'exchange', 'Gresham', 'commissary', 'Grab', 'errand', 'salute', 'gangway', 'Petty', 'excursion', 'boating', 'hiking', 'virile', 'strenuous', 'fashionable', 'Hotel', 'frog', 'packing', \"Buzz's\", 'Wow', 'Strippers', 'scrumptious', 'all-lesbian', 'band', 'Hi', 'Willows', 'notes', 'Tough', 'Reno', 'instigator', 'victims', 'rap', 'Ahah', 'lush', 'divorcee', 'scrawny', 'pajamas', 'moons', 'semi-professionally', 'sponsoring', 'courageous', 'sparkled', 'dully', 'hardships', 'undergo', 'Smug', 'smug', 'sappy', 'twitch', 'region', 'lewd', 'Eskimo', 'sickly-tolerant', 'canvassing', 'Forebearing', 'Several', 'wandering', 'corridors', \"strangers'\", 'difficulties', 'pairs', 'hundred-and-fifty', 'Northwest', 'overexcited', 'remonstrated', 'secular', 'bibles', 'publishing', 'appliances', 'waspishly', 'appliance', 'heatedly', 'blinking', 'seller', 'Leave', 'bucking-up', 'crestfallen', 'race', 'indecisively', 'receive', 'enthusiastic', 'statement', 'overdue', 'alimony', 'Collector', 'Internal', 'Revenue', \"year's\", 'exemptions', 'Virginia', 'braced', 'crest', 'white-topped', 'Truckee', 'spangle', 'elated', 'rattling', 'roulette', 'neon', 'automatically', 'mural', 'depicted', 'settlers', 'wagons', 'animated', 'expectancy', 'splendidly', 'stern', 'Donner', 'starving', 'heavily-upholstered', 'one-arm', 'bandits', 'cunningly', 'slots', 'bested', 'mechanical', 'devices', 'depending', 'dried-up', \"blonde's\", 'stables', 'martingale', 'hit-and-miss', 'five-seventeen', 'logging', 'twelve-hour', \"roulette's\", 'Incidentally', 'famous', 'ranch', 'Bar-H', 'collect', 'rack', 'single-foot', 'fastest', 'Washoe', 'County', 'publicity', 'campaign', 'TV', 'hero', 'trained', 'Hoot', 'Gibson', 'pinto', 'photographs', 'wonderfully', 'conclusively', 'jinx', 'gambling', 'marking', 'keno', 'featured', 'attraction', 'ogled', 'shill', 'prejudice', 'shills', 'flipping', 'discreetly', 'chips', 'premium', 'announcement', 'whereby', 'feature', 'floorshow', 'A.M.', 'starring', 'Adele', 'Body', 'Brenner', 'schoolgirls', 'brides', 'orchids', 'chuck-a-luck', 'Hawaiian', 'sun-tan', 'shorter', 'fatter', 'Lake', 'Cal-Neva', 'instigating', 'guarantees', 'stickman', 'pit', 'crossroading', 'faro', 'allergic', 'pollen', 'okay', 'crap', 'pyramid', 'consecutive', 'platinum', 'bursting', 'sun-suit', 'well-fed', 'prosperous', 'propositioned', 'Stake', 'tango', 'discuss', 'merger', 'Sounds', 'Gisele', 'Scotch', 'Named', 'ballet', 'Sylphide', 'affected', 'Answer', 'Very', 'procedure', 'cold-bloodedly', 'chide', 'lapse', 'manufacturing', 'steal', 'aft', 'pretence', 'Hostile', 'flashed', \"Captain's\", 'arrests', 'E.', 'Andrews', 'oldest', 'glories', 'identified', 'mutineer', 'tap', 'Oliver', 'breathless', 'wild-eyed', 'holystones', 'mysteriously', 'customary', 'articles', 'souvenirs', 'African', 'battle-ax', 'sharpened', 'overheard', 'disappearance', 'ladder', 'aimless', 'milling', 'well-trained', 'well-organized', 'horror', 'orders', 'alert', 'questioningly', 'instruction', 'hastened', 'weather-royal', 'consultation', 'mistaken', 'aiding', 'obey', 'bloodshed', 'strategy', 'reasoned', 'defeated', 'unwillingness', 'sanction', 'originated', 'stalked', 'openly', 'morose', 'muster', 'expressing', 'displeasure', 'communicating', 'hiding', 'insolently', 'disobeyed', 'Seaman', 'attacked', 'snarling', 'McKee', 'brig', 'loyal', 'villainous', 'unaffected', 'crime', 'mutiny', 'fears', 'requires', 'omniscient', 'select', 'rely', 'staunch', 'modest', 'ventured', 'guard', 'increased', 'inquiry', 'punished', 'perform', 'workable', 'extravagant', 'agree', 'thoughtfully', 'conspiracy', 'ferocious', 'anarchy', 'Implements', 'available', 'hasty', 'combat', 'ourselves', 'sentinel', 'brothers', 'labor', 'accordance', 'implacable', 'protection', 'honor', 'yearned', 'passionately', 'strive', 'gates', 'eighteen-year-old', 'sailed', 'shed', 'desired', \"egotist's\", 'precious', 'satisfy', 'egotist', 'served', 'Dear', 'dried', 'assurance', 'Stern-faced', 'inspected', 'satisfying', 'averted', 'glimpsed', 'mild-mannered', 'respectful', 'seeming', 'lethargy', 'gaze', 'calmness', 'detachment', 'unawareness', 'implicit', 'naive', 'thick-skulled', 'sudden', 'clarity', 'kinship', 'stupidity', 'quarreling', 'handcuffs', 'ankle', 'follows', 'misfortune', 'towering', 'symbol', 'authority', 'tragic', 'lad', 'forged', \"other's\", 'undoing', 'deny', 'hazel', 'accept', 'gain', 'defending', 'contemplation', 'logical', 'untruth', 'interpreted', 'prior', 'significance', \"Cromwell's\", 'explanations', 'Rogers', 'geometry', 'wardroom', 'element', 'absurdity', 'percentage', 'steered', 'Torah', 'Bits', 'trash', 'roadway', 'warmish', 'foul', 'strides', 'double-breasted', 'material', 'buckles', 'brim', 'crown', 'trim', 'pinkish-white', 'conscious', 'long-sleeved', 'stride', 'pivoting', 'twisting', 'discolored', 'boarded', 'synagogues', 'shabby', 'clasping', 'unclasping', 'paunch', 'bare-armed', 'dripped', 'poised', 'surly', 'half-closed', 'Rapping', 'translation', 'Moses', 'previously', 'grandfather', 'great-grandfather', 'upturned', 'cropped', 'ringlets', 'framing', 'yellowed', 'prayerbooks', 'penalty', 'distraction', 'guttural', 'ghetto', 'curls', 'traveled', 'partially', 'texts', 'memorized', 'singsonged', 'off-key', 'baritone', 'spurred', 'tapping', 'defined', 'rhythm', 'backed', 'clucked', 'high-pitched', 'devote', 'Except', 'Shabbat', 'praying', 'numb', 'prickly', 'asks', 'fingered', 'pruta', 'Sabras', 'roads', 'Messiah', 'convictions', 'immortal', 'heroic', 'twinkling', 'respectfully', 'kibbutzim', 'Aliah', 'immigrants', 'Ready', \"Me'a\", \"She'arim\", 'Jewish', 'anyplace', 'gaining', 'rebuffed', 'slowed', 'cobblestones', 'pursed', 'Trouble', 'middle-aged', 'strut', 'numerals', 'branded', 'concentration', 'Often', 'despondent', 'wandered', 'official', 'Mandate', 'arrested', 'direct', 'complained', 'intend', 'proposal', 'straighten', 'beaten', 'chariot', 'south', 'Forked', 'Deer', 'braving', 'wastes', 'dumped', 'barge', 'Ethiopians', 'tooling', 'sweeping', 'expansiveness', 'courts', 'cabins', 'bulb', 'unfrosted', 'streetlight', 'swooping', 'bugs', 'bouts', 'hopscotch', 'moths', 'pinging', 'cruising', 'insect', 'Highway', 'feelers', 'sagged', 'overhearing', 'curtain', 'rehearsal', 'Mattie', 'microphone', 'Toonker', 'Burkette', 'yanking', 'leak', 'burn', 'parachute', 'Starkey', 'Poe', 'draining', 'punctured', 'muddy', 'pumps', 'squat', 'son-of-a-bitch', 'hating', 'raped', 'tool', 'rape', 'nestled', 'breast', 'tramp', 'skull', 'drives', 'delivers', 'berry', 'crates', 'owns', 'wells', 'mines', 'mild-voiced', 'little-town', 'big-town', 'level', 'Houdini', 'mile', 'chump', 'too-shiny', 'radio', 'dawns', 'seat', \"Shafer's\", 'substitute', 'Louis', 'Damn', 'acres', 'whitewashed', 'bordering', 'grounds', 'envisioned', 'Homes', 'federal', 'highway', 'peaceful', 'bandstand', 'flash', 'instruments', 'spellbound', 'piano', 'rehearsing', 'governor', 'swelled', 'goose', 'bumps', 'rippled', 'Tennessee', 'farmer', 'rained', \"rat's\", 'ass', 'furrowed', 'powdery', 'droughts', 'predicting', 'festival', 'Factory-to-You', 'maids', 'youngest', 'unventilated', 'allow', 'malingering', 'primping', 'sincere', 'scrubbing', 'boxed-in', 'public', 'count', 'rapped', 'jerk', 'whichever-the-hell', 'six-thirty', 'Saturdays', 'chairs', 'fans', 'porter', 'janitor', 'Among', 'handled', 'commissions', 'uptown', 'lettering', 'specialty', 'complicated', 'pen-and-ink', 'medieval', 'hundreds', 'crammed', 'eight-by-ten', \"jeweler's\", 'precise', 'spots', 'literary', 'artistic', 'journals', 'Yorker', 'Esquire', 'paperback', 'reprints', 'huddling', 'corners', 'sleeves', 'revolver', 'Brothers', 'Karamazov', 'illustration', 'Magpie', 'Press', 'historical', 'novel', 'Edward', '3', 'fifteenth-century', 'ferreted', 'materials', 'shields', 'linden', 'reflections', 'sketch', 'Rufus', 'static', 'reproduce', 'readers', 'researches', 'television', 'movies', 'knights', 'flowing', 'haircuts', 'Fauntleroy', 'villains', 'beards', 'Byron', 'sword', 'stills', 'movie', 'clam', 'authenticity', 'painters', 'scholarships', 'Joyce', 'style', 'adults', 'freshness', 'perception', 'self-consciousness', 'twist', 'forms', 'leap', 'smack', 'nearsighted', 'prevent', 'uncanny', 'absent-minded', 'sleepwalker', \"Christ's\", 'Prussian', 'discovery', 'gravy', 'expenses', 'lodgings', 'Attending', \"Askington's\", 'ambition', 'reputation', 'illustrator', 'Peter', 'goal', 'cashmere', 'buttons', 'Viyella', 'necktie', 'bolo', 'jade', 'texture', 'clothing', 'decorations', \"Brush-off's\", 'sparkle', 'erudite', 'illustrators', 'cover', 'magazines', 'Modern', 'Artists', 'photographed', 'Sixties', 'Velasquez', 'royalty', 'Spain', 'bookshelves', 'Modigliani', 'portrait', 'Pollock', 'Miro', 'background', 'inscription', 'Martian', 'fireplace', 'tiles', 'Picasso', 'restaurants', 'illusion', 'wealthy', 'craft', 'absorb', 'masters', 'Durer', 'Bellini', 'Mantegna', 'Painting', 'interdependent', 'varied', 'multitudinous', 'fragment', 'mosaic', 'performer', 'philharmonic', 'organize', 'gigantic', 'abstract', 'expressionism', 'photorealism', 'outward', 'Eye', 'anatomy', 'teaches', 'five-hundred-dollar', 'prize', 'bums', 'Hudson', 'enameling', 'Hajime', 'Iijima', 'Osric', 'million', 'retrospective', 'contemporary', \"Cezanne's\", 'Still', 'canvases', \"Thoreau's\", 'hangouts', 'magical', 'Contact', 'stimulating', 'succeed', 'isolating', 'occupation', 'Middle', 'Ages', 'Renaissance', 'nineteenth', 'century', 'artisan', 'craftsmanship', 'goldsmith', 'carver', 'society', 'portraying', 'wars', 'impulses', 'propagandist', 'satirist', 'lover', 'philosopher', 'scientist', 'illustrating', 'tooth-paste', 'ads', 'salacious', 'incidents', 'trivial', 'novels', 'fluff', 'navel', 'contemplated', 'purity', 'amateur', 'dervishes', 'apprenticeship', 'upshot', \"Pendleton's\", 'Thursday', 'awkward', 'punch', 'drawing', 'finish', 'Brush-off', 'benefit', 'mailman', 'Stimulating', 'rewarding', 'Partly', 'anticipated', 'prepared', 'classic', 'cigars', 'quitting', 'compartment', 'readily', 'frowning', 'mutually', 'stir', 'courtesy', 'fraud', \"Walter's\", 'assail', 'temporary', 'narrows', 'half-murmured', 'statements', 'stress', 'indulge', 'sequence', 'noble', 'sponge', 'dessert', 'roasted', 'parboiled', 'vegetables', 'icebox', 'appointments', 'Thaxters', 'recovering', 'movie-to-be', 'London', 'tranquil', 'tolerance', 'patch', 'grooved', 'accustomed', 'dense', 'mists', 'sun-warmed', 'palisades', 'twinkle', 'woods', 'shopping', 'parish', 'bazaar', 'Prisoners', 'accused', 'accident', 'unsee', \"car's\", 'turnaround', 'Engisch', 'exclaiming', 'rushes', 'gasps', 'portfolio', 'lingerie', 'mink', \"Salter's\", 'servant', 'sandwiches', 'Sitting', 'hour-long', 'Constance', 'plea', 'Hanging', 'patience', 'stab', 'conclusion', 'winds', 'gleaming', 'moontrack', 'poetic', 'misstep', 'fisherman', 'triumphant', 'hysterical', 'Sonny', 'shuddering', 'pitied', 'underneath', 'unquenched', 'enduring', 'ghastly', 'appalled', 'exclamation', 'estranged', 'hangs', 'separation', 'prevented', 'justify', 'scapegoat', 'borne', 'avoidance', 'rankles', 'coolly', 'hurts', 'Mathias', 'persisted', 'explaining', 'advised', \"nobody's\", 'overplayed', 'appraisal', \"Mathias'\", 'towns', 'dispossessed', 'makeshifts', 'arid', 'discarded', 'risen', 'fullest', 'height', 'transcending', 'murky', 'self', 'ignorant', 'self-examination', 'conclusions', 'realizing', 'praise', 'deliverance', 'pettiness', 'greed', 'Methodist', 'eyeglasses', 'drumming', 'accompaniment', 'relentless', 'postponed', 'cremate', 'leaped', 'trestles', 'sheaf', 'whispering', 'parking', 'hearse', 'Jersey', 'chapel-like', 'auditorium', 'discreet', 'symbols', 'faiths', 'impelled', 'kneel', 'Bach', 'healing', 'Lancaster', 'Arms', 'eyelid', 'convivial', 'Pausing', 'Me', 'Umm', 'uhhu', 'Kleenex', 'wiped', 'flatter', 'vanity', 'Caneli', 'stories', 'cleansing', 'orderly', 'hide', 'annoyance', 'sloppy', 'scrub', 'vent', 'dwelt', 'indignities', 'view', 'wears', 'thin-soled', 'dark-gray', 'slacks', 'fawn-colored', 'encountered', 'tweedy', 'Englishman', 'delighted', 'Changing', 'dark-blue', 'remind', 'Parioli', 'loafed', 'Roman', 'Veneto', 'Farnese', 'Gardens', 'Farneses', 'Trastevere', 'piazza', 'Santa', 'Maria', 'eloquent', 'scolding', 'obelisk', 'clung', \"Aren't\", 'drowsily', 'certainty', 'temperament', 'achieved', 'tranquility', 'composure', 'accepting', 'Testament', 'Sistine', 'folklore', 'prophets', 'Catholic', 'Protestant', 'dogmatic', 'atheists', 'Communist', 'wearied', 'ideologies', 'enlargement', 'wearying', 'pondering', 'hurrying', 'joking', 'stair', 'behave', 'softly', 'Ciao', 'gleamed', 'gestures', 'courteous', \"'ello\", 'biscuits', 'Acting', 'interpreter', 'impersonal', 'Watching', 'bounced', 'encouragingly', 'mill', 'seasonal', 'unemployment', 'migrated', 'railway', 'countrymen', 'Regretfully', 'adopted', 'sparkling', 'intellectual', 'mystical', 'Portugal', 'confirmation', 'serenity', 'unaware', 'homely', 'enchantment', 'translate', 'rulers', 'Nodding', 'approvingly', 'confidentially', 'discontent', 'splendor', 'intellect', 'regime', 'Jobs', 'Italians', 'Devout', 'Brooklyn', 'Malta', 'Ireland', 'glowing', 'softening', 'purify', 'caressing', 'flageolet', 'whisper', 'emerge', 'unhesitant', 'encounter', 'disdain', 'outraged', 'proceeded', 'destination', 'bounce', 'press', 'feared', 'presences', 'metabolism', 'weird', 'invested', 'eyeballs', 'thumbs', 'fled', 'minded', 'Elsewhere', 'clusters', 'half-drunk', 'mild', 'commotion', 'hushed', 'imperious', 'denying', 'dread', 'suffer', 'insult', \"girl's\", 'scorn', 'overloud', 'shriek', 'Telling', 'unsheathing', 'curing', 'hides', 'wickedness', 'cunning', 'Speaking', 'loathing', \"helsq'iyokom\", 'murmuring', 'knot', 'threats', 'mingling', 'hurled', 'embodiment', 'defiance', 'howling', 'bullhide', 'tripping', 'seized', 'flog', 'Drive', 'jerking', 'hovel', 'double-married', 'parades', 'confront', 'leads', 'fury', 'stirred', 'mumbling', 'stupor', 'wolves', 'nakedness', 'stale', 'odor', 'Lie', 'curses', 'gnarled', 'talons', 'rudely', 'shoving', 'spruce', 'shortening', 'arc', 'narrowed', 'snarled', 'cracked', 'unchanged', 'delineaments', 'begotten', 'forbidden', 'tracings', 'tree', 'refracted', 'crazed', 'monosyllables', 'unnnt', 'Sssshoo', 'quavering', \"whip's\", 'unbent', 'resigned', 'blows', 'fleeing', 'dodging', 'shadows', 'rocky', 'hid', 'madness', 'glorying', 'mastiff', 'bristling', 'ranted', 'doom', 'enemies', 'gasping', 'draughts', 'incoherent', 'oblivion', 'amused', 'appeasement', 'violence', 'retribution', 'antics', 'disdaining', 'gloom', 'games', 'races', 'paraded', 'grandly', 'attis', 'skeletons', 'paxam', 'dipped', 'arrowheads', 'venom', 'rattlesnakes', 'swift', 'maneuvers', 'firing', 'guns', 'unison', 'indeterminate', 'blankets', 'flew', 'Swan', 'Necklace', 'emulate', 'timidly', 'Yellow', 'Wolf', 'disordered', 'chemistries', 'bolt', 'inaction', 'boiled', 'fermenting', 'juices', 'Alokut', 'challenged', 'matched', 'raced', 'maneuvered', 'abreast', 'cavalry', 'frenziedly', 'regalia', 'preparations', 'combed', 'streaked', 'greased', 'foreheads', 'tails', 'sage', 'hens', 'whitened', 'leggings', 'reflection', 'speeding', 'arrow', 'bow', 'meadow', 'rue', 'admiring', 'envenomed', 'hilltops', 'descend', 'eagle', 'caper', 'cliffs', 'challenge', 'arrogance', 'streaming', 'amulets', 'ripening', 'bellicosity', 'throes', 'shifting', 'Appaloosas', 'Dogs', 'fleet', 'multicolored', 'legion', 'banners', 'drums', 'cacophony', 'accompanying', 'thousand-legged', 'saddles', 'ejaculated', 'hemlocks', 'birch', 'maples', 'quarry', 'unfamiliar', 'stubs', 'suicide', 'expertly', 'root', 'sneakers', 'sliding', 'expanse', 'seating', 'short-cut', 'reedy', 'frogs', 'faded', 'presently', 'vibrant', 'accomplished', 'Paul', 'Easter', 'holidays', 'Evening', 'Dancing', 'attend', 'casually', 'breasts', 'lower-cut', 'inability', 'embarrassment', 'sometime', 'threshold', 'china', 'lamp', 'switch', 'prison', 'indebted', 'Below', 'tunnel', 'fumbled', 'uncovered', 'gripped', 'fearful', 'coursing', 'vessels', 'fabric', 'stains', 'apparent', 'blood-soaked', 'tidiness', 'verge', 'Poldowski', 'owed', 'effete', 'peered', 'wristwatch', 'quarter', 'kneeling', 'tie', 'shoelaces', 'absurdly', 'clumsy', 'strands', 'gauze', 'moist', 'imperative', 'detained', 'freakish', 'cavern', \"Who's\", 'thief', 'Gosh', 'blazer', 'overly', 'knitted', \"Pietro's\", 'finishing', 'float', 'implausibly', 'shaft', 'sunlight', 'oddly', 'partly', \"moment's\", 'paneling', 'inserted', 'eyelids', 'good-bye', 'pounding', 'manage', 'Lincoln', 'parked', 'insolent', 'disdainful', 'Ardmore', 'merest', 'servants', 'born', 'inflection', 'rehearsed', 'analyze', 'Carrie', 'responded', 'restaurant', 'Forget', 'slackened', 'badly', 'disappointing', 'purposeless', 'relinquished', 'Abruptly', 'greatest', 'automatic', 'phrase', 'courtyard', 'tender', 'bumped', 'steadily', 'entry', 'brass', 'screwed', 'kitchenette', 'raw', 'gasp', 'swallows', 'lessen', 'hauled', 'upsets', 'insist', 'sweetheart', 'Happened', 'intoxicated', 'Unfortunately', 'Tony', 'Elliott', 'pinch-hit', \"He'll\", 'nudge', 'postponement', 'bribe', 'double', 'martini', 'relationship', 'Fulbright', 'renovated', 'brick', 'settler', 'clever', 'efficient', 'conformists', 'obnoxious', 'ruffled', 'feathers', 'unreliable', 'irresponsible', 'flyaway', 'jollying', 'Obviously', 'Upstairs', 'showering', 'raindrops', 'pattered', 'grandmother', 'prudent', 'lifetime', 'expressive', \"sister's\", 'Darling', 'mingled', 'blend', 'Leaving', 'supposedly', 'Gregg', 'otherwise', 'jingled', 'scare', \"she'll\", 'defensive', 'Greg', 'depends', 'rescue', 'crises', 'Remembering', 'succession', 'disasters', 'child-cloud', 'youngster', 'fuss', 'accuse', 'looming', 'specter', 'forever-Cathy', 'fiercely', 'piping', 'moon-washed', 'steeped', 'rung', 'stairway', 'brows', 'smoothed', 'muddling', 'worthless', 'clattering', 'overwhelmed', 'junk', 'gushed', 'approaching', 'handkerchief', 'mopping', 'skillfully', 'shape-up', 'groaned', 'gentler', 'grappling', 'outsized', 'armload', 'Scrooge-like', 'lecture', \"cousins'\", 'sympathize', 'limit', \"Cathy's\", 'Lilliputian', 'competing', 'rear', 'flower-scented', 'chilly', 'chillier', 'feeding', 'raffish', 'bobbed', 'gobbled', 'cardinal', 'feathered', 'vestments', 'mate', 'guarding', 'nest', 'sly', 'advantage', 'sentinels', 'abandoning', 'prepare', 'helplessness', 'unscrupulous', 'dump', 'curtains', 'hopping', 'rabbits', 'Goose', 'Mobile', 'hangers', 'neatly', 'fragrant', 'panicky', 'Many', 'Junction', 'phoned', 'sooner', 'napping', 'multitude', 'Methuselah', 'maternal', 'comforting', 'magic', 'Subdued', 'merriest', 'airport', 'fixing', 'hummed', 'voiceless', 'Puzzled', 'flowered', 'forthright', 'silences', 'moth', 'seekingly', 'Shocked', 'elastic', 'cord', 'uncoiling', 'bogey', 'thickened', 'fetching', 'tightened', 'to-and-fro', 'abrupt', \"hall's\", 'purple-black', \"cowbirds'\", 'Cowbird', 'trudged', 'package', \"aunt's\", \"uncle's\", 'Unlike', 'traveling', 'shortest', 'stays', 'Unimpressed', 'plopped', 'disbelieving', 'Esperanza', 'show-offy', 'lifeguards', 'swirling', 'pronounced', 'sherbet-colored', 'mint', 'deserted', 'Eats', 'jiggling', 'jaggedly', \"water's\", 'lumpy', 'trailing', 'dragon', 'Swiss', 'belt', 'corkscrew', 'Canute', 'knight', 'Round', 'Table', 'Sir', 'Brave', 'slaying', 'banshees', 'vampires', 'witches', 'warty', 'astronaut', 'intrepid', 'persecuted', 'fearless', 'scary', 'Tomorrow', 'thump', 'signals', 'radar', 'Son', 'pig', 'cords', 'limping', 'dumb', 'nut', 'polio', 'bedside', 'spilled', 'probly', 'lick', 'Fatso', 'bedtime', 'Strength', 'zip', 'unlaced', 'wadded', 'stripped', 'trunks', 'Goolick', 'goooolick', 'creaked', 'gull', 'dignified', 'bored', 'wedged', 'crawled', 'Watch', 'straps', 'shouders', 'sore', 'boil', \"squatter's\", 'rights', 'Kansas-Nebraska', 'Britches', 'Mark', 'Peters', 'Fifth', 'kinds', 'aleck', 'ripping', 'unconsciously', 'imitating', \"Victoria's\", 'holier-than-thou', 'horrified', 'momentum', 'mama', 'wop', 'wops', 'unimpressed', 'obliged', 'jeans', 'Dingy-looking', 'barber', 'pole', 'spouted', \"cane's\", 'handy', \"Someone's\", 'zoooop', 'snag', 'crook', 'bowl', 'fly', 'bannnnnng', 'stooooomp', 'licking', 'stubby', 'plume', 'collie', 'wire-haired', 'terrier', 'unique', 'DiMaggio', 'tranquilizers', 'braver', 'lately', 'Last', 'jag', 'gotta', 'Squint', 'mornings', 'thereafter', 'blister', 'emerald', 'necklace', 'undying', 'picks', 'nuts', 'Naturally', 'curly', 'runs', 'Personally', 'prefer', 'Continent', 'continent', 'Name', 'pugh', 'beaches', 'scraped', 'Pugh', 'Camels', 'Tripoli', 'harelips', 'Near', 'Galway', 'tinkers', 'caravans', 'gypsies', 'Jerez', 'Really', 'yawn', 'Artfully', 'Cross', 'Korean', 'War', 'spit', 'personally', 'IQ', '141', 'currently', 'Mushr', 'Ozon', 'encyclopedia', 'schnooks', 'Chinaman', 'Tooth-hurty', 'Encouraged', 'imitated', 'described', 'decorated', 'pineapple', 'cherries', 'snuck', 'gumming', 'stumpy', 'panting', 'thirst', 'lagoon', 'staggering', 'Say', 'comic', 'admiringly', 'Either', 'cod', 'salmon', 'sharks', 'bones', 'shrimp', 'encylopedia', 'string', 'Boy', 'Willie', 'Mays', 'outfield', 'je', 'ne', 'quok', 'daydreamed', 'splashed', 'buckets', 'beseech', 'thee', 'sprayed', 'raisin', 'bib', \"Daddy's\", 'Francisco', \"one-o'clock\", 'keen', 'Children', 'organized', 'nap', 'noticing', 'sheets', 'Kool-Aid', 'gagging', 'Sweating', 'drowning', \"afternoon's\", 'Oakmont', 'miswritten', 'heartless', 'prematurely', \"Hadn't\", 'forty-seven', 'twenty-five', 'appalling', 'deserve', 'adult', 'realistic', 'rebound', 'reproach', '1936', 'odds', 'prettiest', 'brightest', 'Allegheny', 'handsomer', 'brighter', 'Pittsburgh', 'seasons', 'Golf', 'goggle-eyed', 'admiration', 'debs', 'claimed', 'skiing', 'crinkles', 'entity', 'Conneaut', \"lovers'\", 'spats', 'heal', 'Dillinger', 'First', \"Cooper's\", 'aggressive', 'thinkers', 'specifically', 'someplace', 'Baltimore', \"Horne's\", 'underestimate', 'giggled', 'bridesmaids', 'choking', 'myth', 'skirts', 'drifted', 'command', 'less-dramatic', 'Sewickley', 'Fox', 'reduced', 'method', 'grindstone', 'Ben', '1938', 'cutest', 'fontanel', 'nary', 'continuing', 'Sally', '1940', '1944', 'dizziness', 'uselessness', 'shuffled', 'missionary', 'Webber', 'eldest', 'elder', 'donated', 'avoid', 'bosoms', 'afterward', 'oversubscribed', 'missionaries', 'bargain', 'whimpering', 'bends', 'overgenerous', 'assignment', 'heir', 'apparency', 'Coopers', 'socially', 'hairpin', 'mourning', 'bites', 'luncheon', 'Le', 'Mont', \"could've\", 'bone-deep', 'sorrow', 'ambitious', 'undo', 'trembled', 'brink', 'settlement', 'soothe', 'capable', 'capturing', 'Years', 'struggled', 'wonders', 'Alloy', 'departments', 'MacIsaacs', 'alloy', 'division', 'Carnegie-Illinois', 'Stuart-family', 'Reuben', 'Pittsburghers', 'sticking', 'dragooned', 'director', 'S.', 'M.', 'jolly', 'factors', 'insiders', 'weighed', 'terms', 'incentive', \"John'll\", 'independent', 'directors', 'agreement', 'Furnaces', 'subordinate', 'advancement', 'meteoric', 'vice', 'biography', 'baby-sitter', 'worth', 'obligated', 'shakily', 'answers', 'frantic', \"Thom's\", \"everything's\", 'Wondering', 'seconds', 'faltered', 'thirteen', 'sitters', 'reliable', 'beautifully', 'ragged', 'spreads', 'law', 'scooted', 'strike', 'hissed', 'thickly', 'lurched', 'raked', 'grabbed', 'insulted', 'takes', 'simmer', 'Tea', 'forgive', 'dime', \"Francie's\", 'televison-record', 'owe', 'grubby', 'filth', 'traipsing', 'curtly', 'collapsed', 'snoring', 'Whether', 'Thankful', 'attending', 'grease', 'confessed', 'pits', 'repaired', 'Seems', 'hired', 'repair', 'promises', 'blew', 'stack', 'Things', 'deserved', 'breaks', 'necks', 'owing', 'received', 'evicted', 'Worst', 'stranded', 'fretted', 'brightened', 'candle', 'embarrassed', 'pleading', 'Straightened', 'flatly', 'uncomfortable', 'darn', 'snobs', 'finance', 'crawl', 'dishes', 'groceries', 'Virus', 'infection', 'lazy', 'no-good', 'alibis', 'excuses', 'Wendell', 'black-balled', 'grocery', 'sewed', 'punks', \"they'll\", 'encouraging', 'July', 'stifling', 'intuition', 'based', 'nine-to-five', 'five-days-a-week', 'medicine', 'shaving', 'sneezed', 'orphanage', 'boarding-home', 'fonder', 'Especially', 'Growing', 'intern', 'lightened', 'heaviness', 'Should', 'interns', 'Ishii', 'resident', 'Medicine', 'sicker', 'post-operative', 'Got', 'lulu', 'alcoholics', 'segregated', 'charity', 'focus', 'forming', 'alcoholism', 'puffy', 'marred', 'tell-tale', 'veins', 'drinkers', 'two-colored', 'bleached', 'unfortunate', 'resemblance', 'forcing', 'disquiet', 'knocked', 'AA', 'describe', 'ounce', 'relaxes', 'chat', 'gladly', 'drawn-back', 'bloodspots', 'spotting', 'emotion', 'negative', 'Papanicolaou', 'vaginal', 'gross', 'lab', 'Late', 'results', 'D.', 'C.', 'hysterectomy', 'intraepithelial', 'situ', 'wryly', 'tells', 'badgering', 'Wants', 'booze', 'grimaced', \"Bancroft's\", 'knob', 'dozing', 'disregarded', 'dilatation', 'curettage', 'proves', 'definite', 'difficulty', 'liquor', 'surgery', 'Mostly', 'coverlet', 'secrets', \"patient's\", 'disregarding', 'protests', 'guinea', 'pigs', 'learn', 'cultivated', 'strongly', 'assure', 'gaped', 'intently', 'half-smile', 'Surprised', 'Guess', 'heart-stopping', 'stammered', 'goodness', 'echoed', 'mockingly', 'Think', 'introduce', 'alcoholic', 'blackout', 'favors', 'alike', \"what's\", 'pathetic', 'disappear', 'caresses', 'tricks', 'ultimate', 'pressures', 'squeezed', 'breathed', 'anesthetic', 'yielding-Mediterranian-woman-', 'soothed', '230', 'drunker', 'metal-tasting', 'suck', 'aqua-lung', 'swim', 'chuckled', 'hazy', 'concentrated', 'sorted', 'duds', 'bellyfull', 'Wild', 'kicks', 'dig', 'twenty-one', 'Orly', 'Rhine-Main', 'April', 'mist-like', 'pocketful', 'grounded', 'Champs', 'Elysees', 'dazzled', 'machines', 'Bugatti', 'Farina', 'coachwork', 'chassis', 'Swallow', 'A40-AjK', 'Mercedes', 'Arc', 'de', 'Triomphe', 'Tour', \"d'Eiffel\", 'yokel', \"Maxim's\", 'Claire', 'feed', 'Handsome', 'soldier', 'assuaged', 'urgency', 'Madame', 'noblesse', 'oblige', 'yeah', 'nymphomaniac', 'Toward', 'waxed', 'philosophical', 'analogies', 'mistake', 'sweep', 'Panther', 'Pils', 'switched', 'Tuborg', 'crocked', 'orbit', 'Capricorn', 'Cancer', 'Elemental', 'debauchery', 'reconciled', 'flop', 'binge', 'therapeutic', 'Sometime', 'snowing', 'snowy', 'dim', 'flakes', 'Pretty', 'poise', 'posture', 'drab', 'propriety', 'unuttered', 'bleak', 'unhappiness', 'profoundly', 'derriere', 'crumble', 'snorted', 'chuckle', 'tiredness', 'infinite', 'reinforce', 'gambit', 'Remy', 'cork', 'Non', 'non', 'lug', 'tart', 'drowsing', 'Allons', 'rested', 'rime', 'melted', 'absolute', \"night's\", 'melancholy', 'vital', 'brightly', 'tousled', 'make-up', \"J'ai\", 'faim', 'wheeled', 'presenting', 'dangerously', 'scald', 'filter', 'tremendous', 'amount', 'hard-boiled', 'garlic', 'Suzanne', 'comfortably', 'remarked', 'Um', 'grunted', 'sipping', 'les', 'putains', 'immense', 'sentence', 'Shall', 'enthusiasm', 'immediately', 'bullet', 'Metro', 'cabaret', 'steamed', 'mussels', 'sommelier', 'magnum', 'steely', 'Jeroboam', 'humorous', \"mind's\", 'responding', 'exhaustingly', \"shores'\", 'powers', 'telescoped', 'plains', 'aqueducts', 'tombs', 'cypress', 'Appian', 'Way', 'Arch', 'Constantine', 'Moreover', 'nursing', 'aunt', 'lengthy', 'illnesses', 'England', 'France', 'Germany', 'Switzerland', 'rendered', 'tonics', 'conceived', 'lamplight', 'Cars', 'taxis', 'buses', 'motorscooters', 'swerving', 'perilously', 'groups', 'German', 'cameras', 'attached', 'Glad', 'outdoor', 'unloading', 'potted', 'placing', 'banked', 'rows', 'shrubs', 'myriad', 'bud', 'ranged', 'fuchsia', 'palest', 'Marvelous', 'portly', 'well-bred', 'Halfway', 'Other', 'banisters', 'parapets', 'Mediterranean', 'underside', 'insides', 'churches', 'Content', 'excited', 'contracted', 'dismay', 'script', \"t's\", \"l's\", 'inclined', 'wobble', 'slope', 'post', 'Alabama', 'mailed', 'next-door', 'neighbor', 'Piazza', 'di', 'Spagna', 'President', 'Republic', 'Inside', 'concerning', 'illness', 'devoted', 'aging', 'immemorial', 'Montgomery', 'sweetpeas', 'mother-of-pearl', 'coveted', 'sigh', 'refolded', 'alas', 'reminiscence', 'weakening', 'values', 'diapers', 'gentility', 'principle', 'lest', 'fraternity', 'pursued', 'mammas', 'aroused', 'uttermost', 'cramp', 'hobbled', 'bathrobe', 'footstool', 'clenched', 'heroically', 'monkey', 'kittens', 'kettle', 'steaming', 'enamelled', 'pan', 'treat', 'hiccups', 'Drop', 'unamused', 'round-eyed', 'woe', 'parting', 'Robbie', 'Beryl', 'imposition', 'Rosie', 'populated', 'Edna', 'Whittaker', 'meets', 'crumbling', 'makeshift', 'sneezing', 'uncertainly', 'unnaturally', 'tray', 'cameos', 'pens', 'papal', 'portraits', 'borders', 'Carrozza', 'escape', 'landing', 'vacant', 'banister', 'vendor', 'well-dressed', 'ascending', 'cast', 'policeman', 'viewed', 'interlude', 'hinting', 'teachers', 'September', 'crisis', 'Balzac', 'Dickens', 'Stendhal', \"Mother's\", 'operation', 'twenty-six', 'volumes', 'Dinsmore', 'arranging', 'Mi', 'diapiace', 'ma', 'insomma', 'indicated', 'stealer', '4000-plus', \"nobody'd\", 'bothered', 'good-living', 'truthfully', 'Sue', 'Much', \"Lucille's\", 'Honest', 'compete', 'trucker', 'carpentry', 'February', 'delivery', 'masculine', 'providing', 'Neither', 'submit', 'penniless', 'helpless', 'Against', \"folks'\", 'crackling', 'vices', 'jockey', 'drinks', 'smokes', 'ticking', 'items', 'swears', 'whoever', 'examined', 'noncommittally', 'scheming', 'Devil', 'Route', '10', 'clutches', 'blushed', 'sexual', 'triggered', 'fierceness', 'Astonishingly', 'summertime', 'fields', 'crickets', 'serenaded', 'huh', 'blissful', 'fulfilling', 'witnesses', 'certificate', 'Infinite', 'contentment', \"Idiot's\", 'upbringing', 'resisted', 'passes', \"Wouldn't\", 'insure', \"Johnnie's\", 'denied', 'contribute', 'doubts', 'petted', 'longest', 'behaved', 'emotionally', 'mere', 'rescued', 'husband-stealer', 'wondrous', 'fitting', 'union', 'Uh-huh', 'Convention', 'eastern', 'represent', 'mixed', 'emotions', 'Faneuil', 'cousin', 'convention', 'introduced', 'Rhode', \"someone's\", 'anxiously', 'twin', 'pumpkin', 'Schmalma', 'shrilly', 'alcohol', 'disguised', 'ginger', 'ale', \"'most\", 'snugly', \"C'mon\", 'lubricated', 'all-American-boy', 'Astronaut', 'vitamin', 'Breakfast', 'Eden', 'two-burner', 'pelting', 'lunatic', 'arrangement', 'Share', 'units', 'bachelor-type', 'eatables', 'curving', 'bloomed', 'haze', 'names', 'magenta', 'downed', 'Lots', 'vitamin-and-iron', 'compound', 'capsule', 'comment', 'beriberi', 'cure', 'Wayne', 'kissing', 'piling', 'ailment', 'confirmed', 'distrust', 'scalded', 'encouragement', 'shy', 'Woods', 'blinds', 'dawning', 'poker', 'secretaries', 'Meanwhile', 'sandy', 'puddles', 'prescription', 'mockery', 'odds-on', 'develop', 'awesome', 'black-and-yellow', 'polka-dotted', 'slicker', 'Three-day', 'natives', 'filthy', 'Pyhrric', 'Fine', 'Thought', 'bake', 'briefly', 'forbore', 'universal', 'weatherproof', 'conviction', 'honeymooning', 'lounging', 'sands', 'unalloyed', 'bliss', 'leafed', 'adventures', 'Seashore', 'Farm', 'Danger', 'agricultural', 'treatment', 'hoof-and-mouth', 'disease', 'cattle', 'hideously', 'illustrated', 'bruising', 'shin', 'choke', 'fuzzy', 'mania', 'dyed', 'climb', 'long-hair', 'according', 'label', 'explanation', 'Bermuda', 'Said', 'permeates', 'squirt', \"Kissin'\", 'Kare', 'handwriting', 'dainty', 'evidence', 'Sorry', 'swiping', 'onions', 'Ugh', 'Must', 'idly', 'fast-frozen', 'guided', 'obscure', 'horse-blanket', 'plaid', 'splashy', 'Pink', 'grown-up', 'clinging', 'infancy', 'toilsome', 'laden', 'clattery', 'mops', 'brushes', 'pails', \"rain's\", 'scorcher', 'scrutinized', 'warn', \"kind's\", 'inviting', 'description', 'Run-down', 'iron-poor', 'Frail', 'feeble', 'blazing', 'varicolored', 'properties', \"four-o'clock\", 'gaudy', 'mist', 'floodlit', 'punctuated', 'refrigerators', 'retreat', 'Cape', \"flower's\", 'transferred', 'M-m-m', 'propped', 'postscript', 'scrap', 'Gee', 'redheads', 'rental', 'browny', 'blondes', 'bikinis', 'preponderance', 'tank', 'Up', 'dune', 'stool', 'bulky', 'turtle-neck', 'sweaters', 'not-so-pale', 'moonlit', 'filched', 'Use', \"goodness'\", 'Sympathy', \"Vivian's\", 'slopping', 'offering', 'peel', 'pajama', 'subside', 'quantities', 'Correspondence', 'glycerin', 'whitens', 'Broiled', 'Puny', 'Surviving', 'Wilderness', 'Speedy', 'Canoe', 'Also', 'canoe', 'overhand', 'innings', 'fifth', \"Anniston's\", 'smacked', \"Riverside's\", 'redheaded', 'relay', 'punches', 'uproar', 'fighters', 'resumed', 'cursed', 'eyeing', 'sonny', 'infuriated', 'Mind', 'goddamn', 'sixth', \"fielder's\", 'rounding', 'heading', 'tagged', 'straddling', 'rammed', \"catcher's\", 'physician', 'injured', 'nearer', 'quivering', 'ram', 'calculated', 'pitching', 'powerfully', 'hander', 'eighth', 'hefty', \"batter's\", 'Hit', 'shortstop', 'bastards', 'Haydon', 'pounded', 'rubber', 'flat-footed', 'unhurried', 'infield', 'Fights', 'squelched', 'helluva', 'team-mate', 'undressing', 'inches', \"Eddie's\", 'unconditional', 'belong', 'confirm', 'Sit', 'steering', 'pro-ball', 'Dazed', 'fielding', 'bases', \"Baseball's\", 'cinch', 'ramming', 'puts', 'sport', 'goddamit', 'ribbons', 'league', 'Someday', \"Springfield's\", 'Talk', 'Ask', 'Springfield', 'Atta', 'outfielders', 'insularity', 'by-ways', 'fascinate', 'Augustine', 'Aquinas', 'Lao-tse', 'Confucius', 'Mencius', 'Suzuki', 'Hindu', 'tomes', 'Krishnaists', 'socio-archaeological', 'papers', 'Zend-Avesta', 'Indian', 'entitled', 'Grinned', 'gloomily', 'laughter', 'formal', 'sixteen', 'substituted', 'fond', 'graduated', 'foundation', 'remoteness', 'thorough', 'bookish', 'lore', 'literature', 'politics', 'awarded', 'practicing', 'sailing', 'distrusted', 'buying', 'shorten', 'relax', 'incarcerated', 'dusted', 'sped', 'plucked', 'wrought', 'resemble', 'instrument', 'stole', 'gusty', 'similar', 'Adams', 'succeeded', 'unusually', 'frankness', 'zest', 'envied', 'occasional', 'participate', 'tea-drinking', 'complied', 'random', 'gatherings', 'flippant', 'superficial', 'pointless', 'persons', 'students', 'judging', 'popular', 'foods', 'equation', 'Zen', 'philosophy', 'modifier', 'Soba', 'udon', 'noisily', 'Sushi', 'Sashimi', 'Asked', 'Witter', 'irritation', 'Lovers', 'Mound', 'Gompachi', 'Komurasaki', 'parkish', 'concluded', 'Anyhow', 'potatoes', 'Kanto', 'bothersome', 'pull', 'Twenty-two', 'twenty-three', 'colder', 'tiniest', 'reflect', 'version', 'ascetic', 'returning', 'waters', 'gate', 'spigots', 'caged', 'incarnation', 'creature', 'overwhelmingly', 'likened', 'limpid', 'fountain-falls', 'familiarity', 'Into', 'hundred-yen', 'urge', 'bronze', 'priest', 'Instead', 'stare', 'rigidly', 'fascination', 'metallic', 'scraping', 'stillness', 'severe', 'emptied', 'pockets', 'coins', 'hurried', 'stairways', 'virus', 'process', 'reminder', 'consciousness', 'lingered', 'inexplicable', 'elements', 'reality', 'resolved', 'farm', 'snow-fence', 'wheels', 'loaded', 'multi-colored', 'graceful', 'throttle', 'fright', 'idle', 'movement', 'tubular', 'interlaced', 'jet-black', 'astonishing', 'unthinkable', 'fire-colored', 'lumps', 'forepart', 'fieldmice', 'satin', 'newly-plowed', 'target', 'shone', 'sun-burned', 'crumbled', 'paleness', 'dusty', 'uncolored', 'triangular', 'plowshares', 'scouring', 'stamping', 'ferocity', 'frenzied', 'impatience', 'fierce', 'hunched-up', 'determination', 'whipped', 'lumbering', 'halt', 'frenetic', 'savagery', 'glimmer', 'remnant', 'ache', 'fumes', 'crushed', 'vanish', 'mount', 'bubbly', 'finely-spun', 'effeminate', 'eyelashes', 'brushlike', 'newly-scrubbed', 'resembled', 'splintered', 'bitten', 'poisonous', 'foolishly', 'gray-looking', 'beautifully-tapered', 'flattened', 'vise', 'splayed', 'wrenched', 'thickest', 'colored', 'crystals', 'ice-feeling', 'spasm', 'muscles', 'wrenches', 'tool-kit', 'gleeful', 'puzzled', 'brute', 'nasty', 'beast', 'slap', 'wipe', 'insolence', 'glee', 'unglued', 'tidings', 'Tax', 'alma', 'mater', 'snapper', 'enrolled', 'credentials', 'transcript', 'references', 'boards', 'applicants', 'co-operate', 'qualifications', 'enroll', 'merchant', 'Gone', 'reunions', '1935', 'old-grad-type', 'Alcorn', \"Pete's\", 'rejected', 'football', 'B', \"A's\", 'math', 'temples', 'chemistry', 'Getting', 'politicking', 'conscience', 'recommend', 'candidate', 'rugged', 'Height', \"6'\", 'Weight', '160', 'Health', 'excellent', 'astronomy', 'geology', 'enrolling', 'keyboard', 'ruining', 'Venetian', 'bestowed', 'bucket', 'commending', 'eyebrow', 'amusement', 'soul-searching', 'participated', 'high-school', 'activities', 'civic', 'flurry', 'shoved', 'unsealed', 'omitted', 'whistled', 'locked', 'gasser', 'rounded', 'pre-packed', 'State', 'rocket', 'messing', 'Real', 'Yale', 'thinning', 'unease', 'Weakness', 'Limited', 'darned', 'Martini', 'tasted', 'meal', 'salad', 'surcease', 'ease', 'sprinkle', 'navy-blue', 'shag', 'woolly', 'Board', 'Work', 'midweek', 'eight', 'commencement', 'Carefully', 'loudly', 'Chairman', \"Partlow's\", 'proverbial', 'pie', 'methodically', 'Burke', 'straight-A', 'antisocial', 'lone', 'completely', 'one-sided', 'Girls', 'parent', 'frayed', 'coffeepot', 'tattered', 'book-lined', 'dilapidated', 'Wrong', \"Dave's\", 'motives', \"Here's\", 'belligerence', 'Already', \"Anne's\", 'acting', 'Daley', 'basketball', 'play-off', 'remains', 'Astronomy', 'stars', 'Geology', 'fishpond', 'suggest', 'chess', 'Music', 'season', 'rock-and-roll', 'combo', 'solos', 'guitar', 'Rich', 'sax', 'solo', 'shine', 'Beethoven', 'mollified', 'brutally', 'loosen', 'padding', 'document', 'ham-radio', 'sale', 'curb', 'applying', 'towel', 'hemming', 'guild', 'banker', 'conform', 'leader', 'well-worn', 'chip', 'qualities', 'Give', 'Army', 'ninety-nine', 'generals', 'mailbox', 'arguing', 'monstrous', 'proportions', 'cotton', 'ruled', 'Lying', 'messy', 'negligent', 'strewn', 'fun', 'kidding', 'trail', 'littered', 'rug', 'assertive', 'grinning', 'refusal', 'stoop', 'petals', 'seed', 'catalogues', 'referred', 'flower', 'contain', 'patina', 'veining', 'reveal', 'column', 'vertebrae', 'magnificently', 'unyielding', 'segments', 'bone', 'cracking', 'noises', 'stem', 'tulip', 'bluntly', 'seriously', 'leisure', 'beforehand', 'delights', 'emotional', 'clash', 'invigorating', 'tenth', 'teasing', 'pat', 'superhuman', 'neatness', 'incongruity', 'mole', 'Quietly', 'foil', 'doorbell', 'Ten', 'cross', 'bend', \"Bill's\", 'deposited', 'strained', 'glissade', 'instructed', 'ellipsis', 'topic', 'Brainards', 'aback', 'contrary', 'weaken', 'fumed', 'adamant', 'raving', 'titters', 'whisperings', 'scabrous', 'unclean', 'gossiping', 'lowering', 'hoarseness', 'crouched', 'Afterwards', 'apologized', 'annoyed', 'pallid', 'resolution', 'About', 'newly', 'Angrily', 'delayed', 'preferably', 'lasted', 'deciding', 'postpone', 'mercy', 'These', 'stray', 'inspection', 'ever-present', 'visitors', 'blot', \"bedroom's\", 'countenance', 'Quite', 'lighthearted', 'witty', 'gaily', 'anecdote', 'frothy', 'deceptive', 'merriment', 'snort', 'mock', 'ritual', 'relations', 'fatal', 'scattering', 'possessions', 'drawers', 'impinge', 'Bizarre', 'bizarre', 'Six', 'Pursuing', 'trapped', 'Day', 'dilemma', 'frantically', 'seeking', 'exit', 'Alternately', 'periods', 'hostile', 'defeatism', 'sullenly', 'morosely', 'simplicity', 'occurred', 'frenzy', 'Beside', 'Instantaneously', 'immeasurable', 'panties', 'alongside', 'particular', 'humiliation', 'Furthermore', 'maneuver', 'endlessly', 'slip', 'brassiere', 'girdle', 'traversed', 'installment', 'rumpled', 'childishness', 'looseness', 'untouched', 'disciplined', 'unruly', 'underclothes', 'linked', 'visibly', 'magnificence', 'virility', 'analyzing', 'scheme', 'exalted', 'fanaticism', 'nylon', 'tenderly', 'focused', 'Slowly', 'reasoning', 'grasped', 'implications', 'occurring', 'Extending', 'inch', 'swiftly', 'unsteady', 'breakfasted', 'refer', \"That'll\", 'resurgence', 'industrialist', \"nibs'\", 'racket', 'glory', 'inadvertent', 'agency', 'brother-in-law', \"Hershey's\", 'draft', 'Eddyman', 'responsible', 'eminence', 'detached', 'estate', 'skyscraper', 'provide', 'Plastic', 'skeleton', 'sorts', 'undergone', 'vicissitudes', 'engaged', 'Shortly', 'proliferation', 'bold', 'exposed', 'suggestions', 'prospering', 'influenced', 'involvement', 'familial', 'loyalty', 'aid', 'prodded', \"Joan's\", 'competency', 'factor', 'eventual', 'disposal', 'unquestionably', 'entwined', 'likely', 'richly', 'Whatever', 'factory', 'ceramics', 'experimentally', 'high-speed', 'calculators', 'political', 'additional', 'worlds', 'conquer', 'Heavy', 'industry', 'slanted', 'inexhaustible', 'coffers', 'attracted', 'Auto', 'Company', 'medium-sized', 'manufactured', 'four-wheel-drive', 'vehicles', 'off-road', 'over-large', 'misguided', 'optimism', 'Cursed', 'dissatisfied', 'stockholders', 'amalgamation', 'mergers', 'consequences', \"Herberet's\", \"Allstates'\", 'folly', 'airplane', 'sub-assembly', 'tanks', 'missiles', 'ordnance', 'desiring', 'Freed', 'complaisant', 'broader', 'overall', 'administration', 'corporate', 'structure', 'wider', 'enchanted', 'proposition', 'Chrysler', 'acquainted', 'hardware', 'Air', 'Force', 'technical', 'Missiles', 'grabs', 'manned', 'affects', 'procurement', 'transports', 'revolutionized', 'airline', 'compound-engine', 'planes', 'companies', 'competition', 'assembled', 'settles', 'bound', 'pages', 'assures', 'raising', 'legal', 'exhibit', 'deadly', 'dampening', 'elderly', 'caution', 'appreciate', 'lukewarm', 'entering', 'fray', 'cudgels', 'prospect', 'mix', 'commercial', 'patriarchy', 'aide', 'Hamilton', 'prevailed', 'devoting', 'swaying', 'dismissed', 'project', 'unwilling', 'interfere', 'transfers', 'hyphenated', 'Clay', 'backing', 'deficit', 'beset', 'deliver', 'elephant', 'Stock', 'abundance', 'Confronted', 'plumped', 'drastic', 'liquidation', 'summoned', 'bind', 'addition', 'defects', 'closeted', 'motor', 'existence', \"A-Z's\", 'set-up', 'exposure', 'conceptions', 'coincidental', 'failure', 'out-dated', 'small-car', 'manufacturer', 'dimensions', 'broach', 'Initially', 'passenger', 'discouraging', 'models', 'founding', 'nationwide', 'dealerships', 'cheaply', 'presses', 'dies', 'precisely', 'hurdle', 'insurmountable', 'investigate', 'marshal', 'statistics', 'arguments', 'Taking', 'dubious', 'alternative', 'breathtaking', 'frank', 'resolve', 'guarantee', 'audience', 'advocating', 'publicly', 'Forgive', 'attended', 'staff', 'presiding', 'incredulity', 'listeners', 'animosity', 'quarrels', 'transact', 'larger', 'low-priced', 'bucking', 'old-fashioned', 'sell', 'economy', 'merit', 'romance', 'snobbery', 'European', 'woo', 'consumer', 'bludgeon', 'chromed', 'excess', 'seduction', 'dealers', 'ready-made', 'steam', 'yachts', 'Georgian', 'bloated', 'too-expensive', 'Allstates-Zenith', 'debate', 'raged', 'Financing', 'emerged', 'obstacle', 'contributed', 'maximum', 'underwrite', 'department', 'risk', 'risky', 'basis', 'capital', 'Heads', 'instinctively', 'onus', 'recriminations', 'broadcast', 'availing', 'in-laws', 'Sweat', 'forehead', 'disquietude', 'Across', 'saluted', 'jubilantly', 'encircled', 'forefinger', 'Spike-haired', 'burly', 'red-faced', 'decked', 'horn-rimmed', \"officers'\", 'expect', 'episode']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import inaugural\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "  (target, fileid[:4]) # (word, year)\n",
        "  for fileid in inaugural.fileids() # e.g., '1789-Washington.txt'\n",
        "  for w in inaugural.words(fileid)\n",
        "  for target in ['america', 'citizen']\n",
        "  if w.lower().startswith(target)\n",
        ")"
      ],
      "metadata": {
        "id": "ZCqASy9PQNBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import udhr\n",
        "languages = [\n",
        "  'Chickasaw', 'English', 'German_Deutsch',\n",
        "  'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik'\n",
        "]\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "  (lang, len(word)) # (language, word length)\n",
        "  for lang in languages\n",
        "  for word in udhr.words(lang + '-Latin1')\n",
        ")\n",
        "# Tabulate results for English and German\n",
        "cfd.tabulate(\n",
        "  conditions=['English', 'German_Deutsch'],\n",
        "  samples=range(10), # Word lengths 0-9\n",
        "  cumulative=True # Cumulative counts\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zTbMLCGQMw7",
        "outputId": "3202f7c1-bf49-4c84-cded-4bb10edb3e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  0    1    2    3    4    5    6    7    8    9 \n",
            "       English    0  185  525  883  997 1166 1283 1440 1558 1638 \n",
            "German_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** c.Study of tagged corpora with methods like tagged_sents, tagged_words."
      ],
      "metadata": {
        "id": "hdhb_dM4anE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # This line downloads the required data\n",
        "\n",
        "# Download the words resource to support word tokenization\n",
        "nltk.download('words')\n",
        "\n",
        "para = \"Hello! My name is Mudra Bagul. Today you'll be learning NLTK.\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\\n===================\\n\", sents)\n",
        "# word tokenization\n",
        "print(\"\\nword tokenization\\n===================\\n\")\n",
        "for index in range(len(sents)):\n",
        "  words = tokenize.word_tokenize(sents[index])\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78sN7bmAa2BZ",
        "outputId": "776e75d1-5434-4ac7-d602-09ef118710a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence tokenization\n",
            "===================\n",
            " ['Hello!', 'My name is Mudra Bagul.', \"Today you'll be learning NLTK.\"]\n",
            "\n",
            "word tokenization\n",
            "===================\n",
            "\n",
            "['Hello', '!']\n",
            "['My', 'name', 'is', 'Mudra', 'Bagul', '.']\n",
            "['Today', 'you', \"'ll\", 'be', 'learning', 'NLTK', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** d. Write a program to find the most frequent noun tags."
      ],
      "metadata": {
        "id": "ZbIGM15KcscC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required resource\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from collections import defaultdict\n",
        "text = nltk.word_tokenize(\"Nick likes to play football. Nick does not like to play cricket.\")\n",
        "tagged = nltk.pos_tag(text)\n",
        "print(tagged)\n",
        "# checking if it is a noun or not\n",
        "addNounWords = []\n",
        "count = 0\n",
        "for words in tagged:\n",
        "  val = tagged[count][1]\n",
        "  if (val == 'NN' or val == 'NNS' or val == 'NNPS' or val == 'NNP'):\n",
        "    addNounWords.append(tagged[count][0])\n",
        "  count += 1\n",
        "print(addNounWords)\n",
        "temp = defaultdict(int)\n",
        "# memoizing count\n",
        "for sub in addNounWords:\n",
        "  for wrd in sub.split():\n",
        "    temp[wrd] += 1\n",
        "# getting max frequency\n",
        "res = max(temp, key=temp.get)\n",
        "# printing result\n",
        "print(\"Word with maximum frequency : \" + res) # Changed str(res) to res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47zhfKSykFJS",
        "outputId": "94dde86a-84ae-48f8-87a9-f8808baea482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Nick', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('play', 'VB'), ('football', 'NN'), ('.', '.'), ('Nick', 'NNP'), ('does', 'VBZ'), ('not', 'RB'), ('like', 'VB'), ('to', 'TO'), ('play', 'VB'), ('cricket', 'NN'), ('.', '.')]\n",
            "['Nick', 'football', 'Nick', 'cricket']\n",
            "Word with maximum frequency : Nick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** e. Map Words to Properties Using Python Dictionaries"
      ],
      "metadata": {
        "id": "fqAPJX1KdYpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating and printing a dictionay by mapping word with its properties\n",
        "thisdict = {\n",
        "\"brand\": \"Ford\",\n",
        "\"model\": \"Mustang\",\n",
        "\"year\": 1964\n",
        "}\n",
        "# Printing the entire dictionary\n",
        "print(thisdict)\n",
        "# Accessing and printing the value associated with the key \"brand\"\n",
        "print(thisdict[\"brand\"])\n",
        "# Printing the number of key-value pairs in the dictionary\n",
        "print(len(thisdict))\n",
        "# Printing the type of the variable 'thisdict\n",
        "print(type(thisdict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGey14emdb5-",
        "outputId": "6ff25062-c9f1-4409-9981-f32651a5734f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'brand': 'Ford', 'model': 'Mustang', 'year': 1964}\n",
            "Ford\n",
            "3\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** f. Study i) Default Tagger, ii) Regular expression Tagger, iii) Unigram Tagger"
      ],
      "metadata": {
        "id": "PmhKzlYf61bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default Tagger journal:\n",
        "import nltk\n",
        "from nltk.tag import DefaultTagger\n",
        "# Create a DefaultTagger that tags every word as a noun (NN)\n",
        "default_tagger = DefaultTagger('NN')\n",
        "# Sample text\n",
        "text = \"John loves programming.\"\n",
        "# Tokenize and tag the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tagged = default_tagger.tag(tokens)\n",
        "print(\"Default Tagger Output:\", tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5ys5pqlSfW0",
        "outputId": "531c04f5-eb01-4cb6-8112-31b7fb1c182a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Tagger Output: [('John', 'NN'), ('loves', 'NN'), ('programming', 'NN'), ('.', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default Tagger:\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.tag import DefaultTagger\n",
        "exptagger = DefaultTagger('NN')\n",
        "from nltk.corpus import treebank\n",
        "testsentences = treebank.tagged_sents() [1000:]\n",
        "print(exptagger.evaluate (testsentences))\n",
        "#Tagging a list of sentences\n",
        "import nltk\n",
        "from nltk.tag import DefaultTagger\n",
        "exptagger = DefaultTagger('NN')\n",
        "print(exptagger.tag_sents([['Hi', ','], ['How', 'are', 'you', '?']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxUyn7aC65m5",
        "outputId": "a91c92df-80a3-48f6-827e-e28601dadc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "<ipython-input-27-94c528bc54cd>:8: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  print(exptagger.evaluate (testsentences))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13198749536374715\n",
            "[[('Hi', 'NN'), (',', 'NN')], [('How', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular expression tagger journal:\n",
        "from nltk.tag import RegexpTagger\n",
        "# Define patterns for tagging\n",
        "patterns = [\n",
        "  (r'.*ing$', 'VBG'), # Gerunds\n",
        "  (r'.*ed$', 'VBD'), # Past tense verbs\n",
        "  (r'.*es$', 'VBZ'), # Present tense verbs\n",
        "  (r'.*\\'s$', 'POS'), # Possessive nouns\n",
        "  (r'.*ly$', 'RB'), # Adverbs\n",
        "  (r'.*', 'NN') # Default to noun\n",
        "]\n",
        "# Create a RegexpTagger with the patterns\n",
        "regexp_tagger = RegexpTagger(patterns)\n",
        "# Sample text\n",
        "text = \"John's programming skills improved greatly.\"\n",
        "# Tokenize and tag the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tagged = regexp_tagger.tag(tokens)\n",
        "print(\"Regexp Tagger Output:\", tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXCO4wTwSvte",
        "outputId": "d99052c9-62c3-4934-cc1c-7fe6fc284558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regexp Tagger Output: [('John', 'NN'), (\"'s\", 'POS'), ('programming', 'VBG'), ('skills', 'NN'), ('improved', 'VBD'), ('greatly', 'RB'), ('.', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular expression tagger:\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import RegexpTagger\n",
        "test_sent = brown.sents(categories='news')[0]\n",
        "regexp_tagger = RegexpTagger(\n",
        " [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
        " (r'(The|the|A|a|An|an)$', 'AT'), # articles\n",
        " (r'.*able$', 'JJ'), # adjectives\n",
        " (r'.*ness$', 'NN'), # nouns formed from adjectives\n",
        " (r'.*ly$', 'RB'), # adverbs\n",
        " (r'.*s$', 'NNS'), # plural nouns\n",
        " (r'.*ing$', 'VBG'), # gerundsS\n",
        " (r'.*ed$', 'VBD'), # past tense verbs\n",
        " (r'.*', 'NN') # nouns (default)\n",
        "])\n",
        "print(regexp_tagger)\n",
        "print(regexp_tagger.tag(test_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frvhNNgi7aPY",
        "outputId": "2c0b62e2-f111-45f5-be8a-3d36f500058c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Regexp Tagger: size=9>\n",
            "[('The', 'AT'), ('Fulton', 'NN'), ('County', 'NN'), ('Grand', 'NN'), ('Jury', 'NN'), ('said', 'NN'), ('Friday', 'NN'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'NN'), (\"Atlanta's\", 'NNS'), ('recent', 'NN'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', 'NN'), ('no', 'NN'), ('evidence', 'NN'), (\"''\", 'NN'), ('that', 'NN'), ('any', 'NN'), ('irregularities', 'NNS'), ('took', 'NN'), ('place', 'NN'), ('.', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram Tagger journal:\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import UnigramTagger\n",
        "# Load tagged sentences from the Brown Corpus\n",
        "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        "# Train a UnigramTagger on the Brown Corpus\n",
        "unigram_tagger = UnigramTagger(brown_tagged_sents)\n",
        "# Sample text\n",
        "text = \"John loves programming.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# Tag the tokens using the UnigramTagger\n",
        "tagged = unigram_tagger.tag(tokens)\n",
        "print(\"Unigram Tagger Output:\", tagged)\n",
        "# Evaluate the UnigramTagger's accuracy on the Brown Corpus\n",
        "accuracy = unigram_tagger.evaluate(brown_tagged_sents)\n",
        "print(\"Unigram Tagger Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFHtcXr1S_bO",
        "outputId": "51806149-cd58-4c0e-e842-d2275d389f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Tagger Output: [('John', 'NP'), ('loves', 'VBZ'), ('programming', None), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-bcc7f243436c>:15: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy = unigram_tagger.evaluate(brown_tagged_sents)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Tagger Accuracy: 0.9349006503968017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram Tagger:\n",
        "\n",
        "# Loading Libraries\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.corpus import treebank\n",
        "# Training using first 10 tagged sentences of the treebank corpus as data.\n",
        "# Using data\n",
        "train_sents = treebank.tagged_sents()[:10]\n",
        "# Initializing\n",
        "tagger = UnigramTagger(train_sents)\n",
        "# Lets see the first sentence\n",
        "# (of the treebank corpus) as list\n",
        "print(treebank.sents()[0])\n",
        "print('\\n', tagger.tag(treebank.sents()[0]))\n",
        "# Finding the tagged results after training.\n",
        "tagger.tag(treebank.sents()[0])\n",
        "# Overriding the context model\n",
        "tagger = UnigramTagger(model={'Pierre': 'NN'})\n",
        "print('\\n', tagger.tag(treebank.sents()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcPq0ccA7yAA",
        "outputId": "597bd78a-e952-40f8-e671-0fffb317edc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "\n",
            " [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "\n",
            " [('Pierre', 'NN'), ('Vinken', None), (',', None), ('61', None), ('years', None), ('old', None), (',', None), ('will', None), ('join', None), ('the', None), ('board', None), ('as', None), ('a', None), ('nonexecutive', None), ('director', None), ('Nov.', None), ('29', None), ('.', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** g. Find different words from a given plain text without any space by comparing\n",
        "this text with a given corpus of words. Also find the score of words."
      ],
      "metadata": {
        "id": "wnseR-2H8C-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from __future__ import with_statement # with statement for reading file\n",
        "import re # Regular expression\n",
        "words = [] # corpus file words\n",
        "testword = [] # test words\n",
        "ans = [] # words matches with corpus\n",
        "print(\"MENU\")\n",
        "print(\"-----------\")\n",
        "print(\" 1 . Hash tag segmentation \")\n",
        "print(\" 2 . URL segmentation \")\n",
        "print(\"enter the input choice for performing word segmentation\")\n",
        "choice = int(input())\n",
        "if choice == 1:\n",
        "  text = \"#whatismyname\" # hash tag test data to segment\n",
        "  print(\"input with HashTag\", text)\n",
        "  pattern = re.compile(\"[^\\w']\")\n",
        "  a = pattern.sub('', text)\n",
        "elif choice == 2:\n",
        "  text = \"www.whatismyname.com\" # url test data to segment\n",
        "  print(\"input with URL\", text)\n",
        "  a = re.split('\\s|(?<!\\d)[,.](?!\\d)', text)\n",
        "  splitwords = [\"www\", \"com\", \"in\"] # remove the words which is containg in the list\n",
        "  a = \"\".join([each for each in a if each not in splitwords])\n",
        "else:\n",
        "  print(\"wrong choice...try again\")\n",
        "print(a)\n",
        "for each in a:\n",
        "  testword.append(each) # test word\n",
        "test_lenth = len(testword) # lenth of the test data\n",
        "# Reading the corpus\n",
        "with open('words.txt', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "  words = [(e.strip()) for e in lines]\n",
        "\n",
        "def Seg(a, lenth):\n",
        "  ans = []\n",
        "  for k in range(0, lenth + 1): # this loop checks char by char in the corpus\n",
        "    if a[0:k] in words:\n",
        "      print(a[0:k], \"-appears in the corpus\")\n",
        "      ans.append(a[0:k])\n",
        "      break\n",
        "  if ans != []:\n",
        "    g = max(ans, key=len)\n",
        "    return g\n",
        "test_tot_itr = 0 # each iteration value\n",
        "answer = [] # Store the each word contains the corpus\n",
        "Score = 0 # initial value for score\n",
        "N = 37 # total no of corpus\n",
        "M = 0\n",
        "C = 0\n",
        "while test_tot_itr < test_lenth:\n",
        "  ans_words = Seg(a, test_lenth)\n",
        "  if ans_words != 0:\n",
        "    test_itr = len(ans_words)\n",
        "  answer.append(ans_words)\n",
        "  a = a[test_itr:test_lenth]\n",
        "  test_tot_itr += test_itr\n",
        "Aft_Seg = \" \".join([each for each in answer])\n",
        "# print segmented words in the list\n",
        "print(\"output\")\n",
        "print(\"---------\")\n",
        "print(Aft_Seg) # print After segmentation the input\n",
        "# Calculating Score\n",
        "C = len(answer)\n",
        "score = C * N / N # Calculate the score\n",
        "print(\"Score\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkbPqaYL8tHA",
        "outputId": "c08f6c97-275d-4b84-f164-ac0c26871f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MENU\n",
            "-----------\n",
            " 1 . Hash tag segmentation \n",
            " 2 . URL segmentation \n",
            "enter the input choice for performing word segmentation\n",
            "1\n",
            "input with HashTag #whatismyname\n",
            "whatismyname\n",
            "what -appears in the corpus\n",
            "is -appears in the corpus\n",
            "my -appears in the corpus\n",
            "name -appears in the corpus\n",
            "output\n",
            "---------\n",
            "what is my name\n",
            "Score 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a txt file name words and add what is my name one below other\n"
      ],
      "metadata": {
        "id": "h2HC0YCTRbVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 3**\n",
        "\n",
        "**Aim:**  a. Study of Wordnet Dictionary with methods as synsets, definitions, examples,\n",
        "antonyms."
      ],
      "metadata": {
        "id": "1RaSlW4SAjr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "print(wordnet.synsets(\"sunrise\"))\n",
        "print(\"My word is Sunrise:- \\n\", \"Definition:\",\n",
        "wordnet.synset(\"sunrise.n.01\").definition())\n",
        "print(\"Examples:\",\n",
        "wordnet.synset(\"sunrise.n.01\").examples())\n",
        "anto = wordnet.lemma('sunrise.n.01.sunrise')\n",
        "print(\"\\nAntonym of word Sell (Noun):\", anto.antonyms())"
      ],
      "metadata": {
        "id": "f118zs6TNUaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3263e8bb-c09c-4d09-8aae-6c0370af9f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('dawn.n.01'), Synset('sunrise.n.02'), Synset('sunrise.n.03'), Synset('sunrise.s.01')]\n",
            "My word is Sunrise:- \n",
            " Definition: the first light of day\n",
            "Examples: ['we got up before dawn', 'they talked until morning']\n",
            "\n",
            "Antonym of word Sell (Noun): [Lemma('sunset.n.01.sunset')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** b. Study lemmas, hyponyms, hypernyms."
      ],
      "metadata": {
        "id": "UuN3J-VBA-Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from journal\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "# Display synsets for the word \"computer\"\n",
        "print(wordnet.synsets(\"computer\"))\n",
        "# Display lemma names for the first synset of \"computer\"\n",
        "print(wordnet.synset(\"computer.n.01\").lemma_names())\n",
        "# Loop through all synsets of \"computer\" and print their lemma names\n",
        "for e in wordnet.synsets(\"computer\"):\n",
        "  print(f'{e} --> {e.lemma_names()}')\n",
        "# Display lemmas for the first synset of \"computer\"\n",
        "print(wordnet.synset('computer.n.01').lemmas())\n",
        "# Display synsets for a specific lemma\n",
        "print(wordnet.lemma('computer.n.01.computing_device').synset())\n",
        "# Display the name of a specific lemma\n",
        "print(wordnet.lemma('computer.n.01.computing_device').name())\n",
        "print(\"\\n\\nHyponyms of computer:\\n\")\n",
        "syn = wordnet.synset('computer.n.01')\n",
        "print(syn.hyponyms()) # Print hyponyms of 'computer'\n",
        "print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])\n",
        "# Listvhyponym names\n",
        "print(\"\\n\\nHyponyms of vehicle:\\n\")\n",
        "vehicle = wordnet.synset('vehicle.n.01')\n",
        "print(vehicle.hyponyms()) # Print hyponyms of 'vehicle'\n",
        "print([lemma.name() for synset in vehicle.hyponyms() for lemma in synset.lemmas()])\n",
        "# List hyponym names\n",
        "print(\"\\n\\nHyponyms of car:\\n\")\n",
        "car = wordnet.synset('car.n.01')\n",
        "print(car.hyponyms()) # Print hyponyms of 'car'\n",
        "print([lemma.name() for synset in car.hyponyms() for lemma in synset.lemmas()])\n",
        "# List hyponym names\n",
        "# Find and print the lowest common hypernyms between 'car' and 'vehicle'\n",
        "print(car.lowest_common_hypernyms(vehicle))\n",
        "print(\"\\n\\nVehicle Hypernyms:\")\n",
        "vehi1 = wordnet.synset('vehicle.n.01')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "784wvY9yUphb",
        "outputId": "77989e50-10a7-4a78-cd3a-54077320520d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
            "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "Synset('computer.n.01') --> ['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "Synset('calculator.n.01') --> ['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
            "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n",
            "Synset('computer.n.01')\n",
            "computing_device\n",
            "\n",
            "\n",
            "Hyponyms of computer:\n",
            "\n",
            "[Synset('node.n.08'), Synset('digital_computer.n.01'), Synset('predictor.n.03'), Synset('web_site.n.01'), Synset('server.n.03'), Synset('analog_computer.n.01'), Synset('turing_machine.n.01'), Synset('number_cruncher.n.02'), Synset('home_computer.n.01'), Synset('pari-mutuel_machine.n.01')]\n",
            "['node', 'client', 'guest', 'digital_computer', 'predictor', 'web_site', 'website', 'internet_site', 'site', 'server', 'host', 'analog_computer', 'analogue_computer', 'Turing_machine', 'number_cruncher', 'home_computer', 'pari-mutuel_machine', 'totalizer', 'totaliser', 'totalizator', 'totalisator']\n",
            "\n",
            "\n",
            "Hyponyms of vehicle:\n",
            "\n",
            "[Synset('rocket.n.01'), Synset('craft.n.02'), Synset('steamroller.n.02'), Synset('skibob.n.01'), Synset('sled.n.01'), Synset('military_vehicle.n.01'), Synset('wheeled_vehicle.n.01'), Synset('bumper_car.n.01')]\n",
            "['rocket', 'projectile', 'craft', 'steamroller', 'road_roller', 'skibob', 'sled', 'sledge', 'sleigh', 'military_vehicle', 'wheeled_vehicle', 'bumper_car', 'Dodgem']\n",
            "\n",
            "\n",
            "Hyponyms of car:\n",
            "\n",
            "[Synset('stock_car.n.01'), Synset('cab.n.03'), Synset('racer.n.02'), Synset('hardtop.n.01'), Synset('model_t.n.01'), Synset('minivan.n.01'), Synset('limousine.n.01'), Synset('used-car.n.01'), Synset('bus.n.04'), Synset('sport_utility.n.01'), Synset('horseless_carriage.n.01'), Synset('ambulance.n.01'), Synset('roadster.n.01'), Synset('convertible.n.01'), Synset('gas_guzzler.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('beach_wagon.n.01'), Synset('coupe.n.01'), Synset('pace_car.n.01'), Synset('stanley_steamer.n.01'), Synset('electric.n.01'), Synset('jeep.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('compact.n.03'), Synset('hot_rod.n.01'), Synset('cruiser.n.01'), Synset('hatchback.n.01'), Synset('sedan.n.01'), Synset('sports_car.n.01')]\n",
            "['stock_car', 'cab', 'hack', 'taxi', 'taxicab', 'racer', 'race_car', 'racing_car', 'hardtop', 'Model_T', 'minivan', 'limousine', 'limo', 'used-car', 'secondhand_car', 'bus', 'jalopy', 'heap', 'sport_utility', 'sport_utility_vehicle', 'S.U.V.', 'SUV', 'horseless_carriage', 'ambulance', 'roadster', 'runabout', 'two-seater', 'convertible', 'gas_guzzler', 'subcompact', 'subcompact_car', 'touring_car', 'phaeton', 'tourer', 'beach_wagon', 'station_wagon', 'wagon', 'estate_car', 'beach_waggon', 'station_waggon', 'waggon', 'coupe', 'pace_car', 'Stanley_Steamer', 'electric', 'electric_automobile', 'electric_car', 'jeep', 'landrover', 'loaner', 'minicar', 'compact', 'compact_car', 'hot_rod', 'hot-rod', 'cruiser', 'police_cruiser', 'patrol_car', 'police_car', 'prowl_car', 'squad_car', 'hatchback', 'sedan', 'saloon', 'sports_car', 'sport_car']\n",
            "[Synset('vehicle.n.01')]\n",
            "\n",
            "\n",
            "Vehicle Hypernyms:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "print(wordnet.synsets(\"computer\"))\n",
        "print(wordnet.synset(\"computer.n.01\").lemma_names())\n",
        "#all lemmas for each synset.\n",
        "for e in wordnet.synsets(\"computer\"):\n",
        "  print(f'{e} --> {e.lemma_names()}')\n",
        "#print all lemmas for a given synset\n",
        "  print(wordnet.synset('computer.n.01').lemmas())\n",
        "#get the synset corresponding to lemma\n",
        "  print(wordnet.lemma('computer.n.01.computing_device').synset())\n",
        "#Get the name of the lemma\n",
        "  print(wordnet.lemma('computer.n.01.computing_device').name())\n",
        "#Hyponyms give abstract concepts of the word that are much more specific\n",
        "#the list of hyponyms words of the computer\n",
        "syn = wordnet.synset('computer.n.01')\n",
        "print(syn.hyponyms)\n",
        "print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])\n",
        "#the semantic similarity in WordNet\n",
        "vehicle = wordnet.synset('vehicle.n.01')\n",
        "car = wordnet.synset('car.n.01')\n",
        "print(car.lowest_common_hypernyms(vehicle))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOxq-OR6BAHa",
        "outputId": "9a151b24-4702-4d64-9761-251951e07c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
            "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "Synset('computer.n.01') --> ['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n",
            "Synset('computer.n.01')\n",
            "computing_device\n",
            "Synset('calculator.n.01') --> ['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
            "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n",
            "Synset('computer.n.01')\n",
            "computing_device\n",
            "<bound method _WordNetObject.hyponyms of Synset('computer.n.01')>\n",
            "['node', 'client', 'guest', 'digital_computer', 'predictor', 'web_site', 'website', 'internet_site', 'site', 'server', 'host', 'analog_computer', 'analogue_computer', 'Turing_machine', 'number_cruncher', 'home_computer', 'pari-mutuel_machine', 'totalizer', 'totaliser', 'totalizator', 'totalisator']\n",
            "[Synset('vehicle.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** c.Write a program using python to find synonym and antonym of word \"active\"\n",
        "using Wordnet."
      ],
      "metadata": {
        "id": "44ltjcnFBXnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "print( wordnet.synsets(\"active\"))\n",
        "print(wordnet.lemma('active.a.01.active').antonyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EifbICvBa5R",
        "outputId": "a55b1f44-1ed3-4524-e37c-dfdf4c5551f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('active_agent.n.01'), Synset('active_voice.n.01'), Synset('active.n.03'), Synset('active.a.01'), Synset('active.s.02'), Synset('active.a.03'), Synset('active.s.04'), Synset('active.a.05'), Synset('active.a.06'), Synset('active.a.07'), Synset('active.s.08'), Synset('active.a.09'), Synset('active.a.10'), Synset('active.a.11'), Synset('active.a.12'), Synset('active.a.13'), Synset('active.a.14')]\n",
            "[Lemma('inactive.a.02.inactive')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** d. Compare two nouns"
      ],
      "metadata": {
        "id": "qC8bx0P0B0Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "syn1 = wordnet.synsets('football')\n",
        "syn2 = wordnet.synsets('soccer')\n",
        "# A word may have multiple synsets, so need to compare each synset of word1 with synset of word2\n",
        "for s1 in syn1:\n",
        " for s2 in syn2:\n",
        "  print(\"Path similarity of: \")\n",
        "  print(s1, '(', s1.pos(), ')', '[', s1.definition(), ']')\n",
        "  print(s2, '(', s2.pos(), ')', '[', s2.definition(), ']')\n",
        "  print(\" is\", s1.path_similarity(s2))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brm3qgUSB2Sx",
        "outputId": "c71ae46f-901c-4bd6-db64-7d07f000fb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path similarity of: \n",
            "Synset('football.n.01') ( n ) [ any of various games played with a ball (round or oval) in which two teams try to kick or carry or propel the ball into each other's goal ]\n",
            "Synset('soccer.n.01') ( n ) [ a football game in which two teams of 11 players try to kick or head a ball into the opponents' goal ]\n",
            " is 0.5\n",
            "\n",
            "Path similarity of: \n",
            "Synset('football.n.02') ( n ) [ the inflated oblong ball used in playing American football ]\n",
            "Synset('soccer.n.01') ( n ) [ a football game in which two teams of 11 players try to kick or head a ball into the opponents' goal ]\n",
            " is 0.05\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** e. Handling stopword"
      ],
      "metadata": {
        "id": "a6OOb7WZCNT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install gensim # install the gensim package."
      ],
      "metadata": {
        "id": "UGdeEsW_4IB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc648bf-ae91-48ee-d588-b0fafed85322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorboard, ml-dtypes, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 numpy-2.1.3 tensorboard-2.19.0 tensorflow-2.19.0\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "h. Using nltk Adding or Removing Stop Words in NLTK's Default Stop Word List"
      ],
      "metadata": {
        "id": "VQW611u4XLYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using nltk Adding or Removing Stop Words in NLTK's Default Stop Word List\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in\n",
        "stopwords.words()]\n",
        "print(tokens_without_sw)\n",
        "#add the word play to the NLTK stop word collection\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.append('play')\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)\n",
        "#remove ‘not’ from stop word collection\n",
        "all_stopwords.remove('not')\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afn40BqdCPx1",
        "outputId": "0b86bdf7-41f3-447c-cde1-beac97a63be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yashesh', 'likes', 'play', 'football', ',', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'football', ',', 'however', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'football', ',', 'however', 'not', 'fond', 'tennis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** f. Using Gensim Adding and Removing Stop Words in Default Gensim Stop Words List\n"
      ],
      "metadata": {
        "id": "bNKZQ3c2DSiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim # install the gensim package."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC57bumNKqNC",
        "outputId": "386e2a6d-c79a-4a81-bbf6-f8f47d0f67c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade gensim\n",
        "import numpy as np # Import numpy before gensim\n",
        "import gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "WglGJ899Lals",
        "outputId": "a55dd51f-45a9-4681-fcc6-d853c5c336b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-bd9feb08697c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;31m# Import numpy before gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "text = \"Harshad likes to play cricket, however he is not too fond of basketball.\"\n",
        "# Remove stopwords from the text\n",
        "filtered_sentence = remove_stopwords(text)\n",
        "print(filtered_sentence)\n",
        "# Print all stopwords in Gensim\n",
        "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "print(all_stopwords)\n",
        "# The following script adds 'likes' and 'play' to the list of stopwords in Gensim\n",
        "all_stopwords_gensim = STOPWORDS.union(set(['likes', 'play']))\n",
        "# Tokenize the text\n",
        "text_tokens = word_tokenize(text)\n",
        "# Remove stopwords from the tokenized text\n",
        "tokens_without_sw = [word for word in text_tokens if word not in all_stopwords_gensim]\n",
        "print(tokens_without_sw)\n",
        "# Output:\n",
        "#['Harshad', 'cricket', ',', 'fond', 'basketball', '.']\n",
        "# The following script removes the word \"not\" from the set of stopwords in Gensim:\n",
        "sw_list = {\"not\"}\n",
        "all_stopwords_gensim = STOPWORDS.difference(sw_list)\n",
        "# Tokenize the text again\n",
        "text_tokens = word_tokenize(text)\n",
        "# Remove stopwords from the tokenized text (including the word \"not\")\n",
        "tokens_without_sw = [word for word in text_tokens if word not in all_stopwords_gensim]\n",
        "print(tokens_without_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "V2GF843zVgGk",
        "outputId": "bed2e381-fe49-4bc9-8af6-d1b3186c388c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harshad likes play cricket, fond basketball.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gensim' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-792d69baa915>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print all stopwords in Gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mall_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# The following script adds 'likes' and 'play' to the list of stopwords in Gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gensim_stopwords_\n",
        "# chatgpt\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# === 1. View default Gensim stop words ===\n",
        "print(\"Default Gensim Stop Words (first 10):\")\n",
        "print(list(STOPWORDS)[:10])\n",
        "print(\"\\nTotal default stop words:\", len(STOPWORDS))\n",
        "\n",
        "# === 2. Add custom stop words ===\n",
        "additional_stopwords = {'coffee', 'shop'}\n",
        "custom_stopwords_add = STOPWORDS.union(additional_stopwords)\n",
        "\n",
        "print(\"\\nAfter Adding Custom Stop Words:\")\n",
        "print(f\"'coffee' in stopwords: {'coffee' in custom_stopwords_add}\")\n",
        "print(f\"'shop' in stopwords: {'shop' in custom_stopwords_add}\")\n",
        "print(\"Total custom stop words (after addition):\", len(custom_stopwords_add))\n",
        "\n",
        "# === 3. Remove specific stop words ===\n",
        "removed_words = {'would', 'could'}\n",
        "custom_stopwords_remove = STOPWORDS.difference(removed_words)\n",
        "\n",
        "print(\"\\nAfter Removing Some Stop Words:\")\n",
        "print(f\"'would' in stopwords: {'would' in custom_stopwords_remove}\")\n",
        "print(f\"'could' in stopwords: {'could' in custom_stopwords_remove}\")\n",
        "print(\"Total custom stop words (after removal):\", len(custom_stopwords_remove))\n",
        "\n",
        "# === 4. Use custom stop words to clean text ===\n",
        "text = \"The coffee shop could be a great place to work, but it would be noisy.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = simple_preprocess(text)\n",
        "print(\"\\nOriginal Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Filter tokens using custom stop words (added + removed)\n",
        "final_stopwords = custom_stopwords_add.difference(removed_words)\n",
        "filtered_tokens = [word for word in tokens if word not in final_stopwords]\n",
        "\n",
        "print(\"\\nFiltered Tokens (using custom stop words):\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBzSzsuxUEzE",
        "outputId": "69efa232-fc93-40db-f28f-ade397f07589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Gensim Stop Words (first 10):\n",
            "['sometime', 'who', 'quite', 'afterwards', 'move', 'you', 'put', 'always', 'onto', 'whole']\n",
            "\n",
            "Total default stop words: 337\n",
            "\n",
            "After Adding Custom Stop Words:\n",
            "'coffee' in stopwords: True\n",
            "'shop' in stopwords: True\n",
            "Total custom stop words (after addition): 339\n",
            "\n",
            "After Removing Some Stop Words:\n",
            "'would' in stopwords: False\n",
            "'could' in stopwords: False\n",
            "Total custom stop words (after removal): 335\n",
            "\n",
            "Original Tokens:\n",
            "['the', 'coffee', 'shop', 'could', 'be', 'great', 'place', 'to', 'work', 'but', 'it', 'would', 'be', 'noisy']\n",
            "\n",
            "Filtered Tokens (using custom stop words):\n",
            "['could', 'great', 'place', 'work', 'would', 'noisy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** g. Using Spacy Adding and Removing Stop Words in Default Spacy Stop Words List"
      ],
      "metadata": {
        "id": "Id9k8obbDt3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install spacy\n",
        "#python -m spacy download en_core_web_sm\n",
        "#python -m spacy download en\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "#add the word play to the NLTK stop word collection\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "all_stopwords.add(\"play\")\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)\n",
        "#remove 'not' from stop word collection\n",
        "all_stopwords.remove('not')\n",
        "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
        "print(tokens_without_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR6GW8SlD4b7",
        "outputId": "439459ff-f4f0-4ba0-e0fa-d602a60746a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "['Yashesh', 'likes', 'football', ',', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'football', ',', 'not', 'fond', 'tennis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling stopword.**"
      ],
      "metadata": {
        "id": "rOTT-0JsFTez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text, custom_stopwords=None, remove_punctuation=True):\n",
        "    \"\"\"\n",
        "    Removes stop words from a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        custom_stopwords (set, optional): A set of custom stop words to add or replace.\n",
        "                                            Defaults to None (using default NLTK stopwords).\n",
        "        remove_punctuation (bool, optional): If True, removes punctuation. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of words with stop words removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    if custom_stopwords:\n",
        "        stop_words = custom_stopwords  #Or stop_words.union(custom_stopwords) to add to existing list.\n",
        "    words = word_tokenize(text.lower()) # Tokenize and lowercase\n",
        "\n",
        "    if remove_punctuation:\n",
        "        import string\n",
        "        punctuation = set(string.punctuation)\n",
        "        words = [word for word in words if word not in punctuation]\n",
        "\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is an example sentence with some stop words, and also punctuation!\"\n",
        "\n",
        "# Using default stop words:\n",
        "filtered_text = remove_stopwords(text)\n",
        "print(\"Filtered text (default stopwords):\", filtered_text)\n",
        "\n",
        "# Using custom stop words:\n",
        "custom_stop_words = {'this', 'is', 'an', 'example'} # Example of replacing all default stopwords.\n",
        "filtered_text_custom = remove_stopwords(text, custom_stopwords=custom_stop_words)\n",
        "print(\"Filtered text (custom stopwords):\", filtered_text_custom)\n",
        "\n",
        "#Example of adding to the default stopwords.\n",
        "custom_added_stop_words = {\"also\"}\n",
        "filtered_added_text = remove_stopwords(text, custom_stopwords = stopwords.words('english') + list(custom_added_stop_words))\n",
        "print(\"Filtered text (added stop words):\", filtered_added_text)\n",
        "\n",
        "# Example without removing punctuation:\n",
        "filtered_text_no_punct = remove_stopwords(text, remove_punctuation=False)\n",
        "print(\"Filtered text (no punctuation removal):\", filtered_text_no_punct)\n",
        "\n",
        "# Example with a sentence that contains words that are often kept.\n",
        "text2 = \"The company does not have many problems. It is not a bad company. No one is complaining.\"\n",
        "filtered_text2 = remove_stopwords(text2)\n",
        "print(\"Filtered text 2 (default stopwords):\", filtered_text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6WT33TPFT8V",
        "outputId": "2c4d577d-b78e-4a2c-cf8d-f19246ad1ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered text (default stopwords): ['example', 'sentence', 'stop', 'words', 'also', 'punctuation']\n",
            "Filtered text (custom stopwords): ['sentence', 'with', 'some', 'stop', 'words', 'and', 'also', 'punctuation']\n",
            "Filtered text (added stop words): ['example', 'sentence', 'stop', 'words', 'punctuation']\n",
            "Filtered text (no punctuation removal): ['example', 'sentence', 'stop', 'words', ',', 'also', 'punctuation', '!']\n",
            "Filtered text 2 (default stopwords): ['company', 'many', 'problems', 'bad', 'company', 'one', 'complaining']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import STOPWORDS as gensim_stopwords\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# NLTK Stopwords\n",
        "nltk_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Gensim Stopwords\n",
        "gensim_stop_words = set(gensim_stopwords)\n",
        "\n",
        "# --- NLTK Stopwords ---\n",
        "\n",
        "# Adding Stop Words (NLTK)\n",
        "new_nltk_stop_words = nltk_stop_words.union({'extra', 'words'})\n",
        "print(\"NLTK Stop Words after adding:\", len(new_nltk_stop_words))\n",
        "\n",
        "# Removing Stop Words (NLTK)\n",
        "removed_nltk_stop_words = nltk_stop_words.difference({'no', 'not'}) #Keep 'no' and 'not'\n",
        "print(\"NLTK Stop Words after removing:\", len(removed_nltk_stop_words))\n",
        "\n",
        "# Example usage with NLTK stopwords:\n",
        "\n",
        "sentence = \"This is an example sentence with some stop words, but not no extra words.\"\n",
        "words = nltk.word_tokenize(sentence.lower()) #lower case and tokenize.\n",
        "\n",
        "filtered_words_added = [word for word in words if word not in new_nltk_stop_words]\n",
        "filtered_words_removed = [word for word in words if word not in removed_nltk_stop_words]\n",
        "\n",
        "print(\"Filtered with added NLTK stopwords:\", filtered_words_added)\n",
        "print(\"Filtered with removed NLTK stopwords:\", filtered_words_removed)\n",
        "\n",
        "# --- Gensim Stopwords ---\n",
        "\n",
        "# Adding Stop Words (Gensim)\n",
        "new_gensim_stop_words = gensim_stop_words.union({'extra', 'words'})\n",
        "print(\"Gensim Stop Words after adding:\", len(new_gensim_stop_words))\n",
        "\n",
        "# Removing Stop Words (Gensim)\n",
        "removed_gensim_stop_words = gensim_stop_words.difference({'no', 'not'})\n",
        "print(\"Gensim Stop Words after removing:\", len(removed_gensim_stop_words))\n",
        "\n",
        "# Example usage with Gensim stopwords:\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "sentence2 = \"This is an example sentence with some stop words, but not no extra words.\"\n",
        "filtered_gensim_added = remove_stopwords(sentence2.lower(), stopwords=new_gensim_stop_words).split()\n",
        "filtered_gensim_removed = remove_stopwords(sentence2.lower(), stopwords=removed_gensim_stop_words).split()\n",
        "\n",
        "print(\"Filtered with added Gensim stopwords:\", filtered_gensim_added)\n",
        "print(\"Filtered with removed Gensim stopwords:\", filtered_gensim_removed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMlaJ4u7Fiu4",
        "outputId": "80e16326-d6b9-4ff7-fd5f-ee7ff76f3f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Stop Words after adding: 200\n",
            "NLTK Stop Words after removing: 196\n",
            "Filtered with added NLTK stopwords: ['example', 'sentence', 'stop', ',', '.']\n",
            "Filtered with removed NLTK stopwords: ['example', 'sentence', 'stop', 'words', ',', 'not', 'no', 'extra', 'words', '.']\n",
            "Gensim Stop Words after adding: 339\n",
            "Gensim Stop Words after removing: 335\n",
            "Filtered with added Gensim stopwords: ['example', 'sentence', 'stop', 'words,', 'words.']\n",
            "Filtered with removed Gensim stopwords: ['example', 'sentence', 'stop', 'words,', 'not', 'no', 'extra', 'words.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 4**\n",
        "Text Tokenization:\n"
      ],
      "metadata": {
        "id": "pHPxLU8Km85T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** a. Tokenization using Python’s split() function"
      ],
      "metadata": {
        "id": "5MyYQBhLm8j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" This tool is an a beta stage. Alexa developers can use Get Metrics API to\n",
        "seamlessly analyse metric. It also supports custom skill model, prebuilt Flash Briefing\n",
        "model, and the Smart Home Skill API. You can use this tool for creation of monitors,\n",
        "alarms, and dashboards that spotlight changes. The release of these three tools will\n",
        "enable developers to create visual rich skills for Alexa devices with screens. Amazon\n",
        "describes these tools as the collection of tech and tools for creating visually rich and\n",
        "interactive voice experiences. \"\"\"\n",
        "data = text.split('.')\n",
        "for i in data:\n",
        "  print (i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cehfHQ-unQ8q",
        "outputId": "c99a48d5-6808-4ef0-bf8c-fd0fba068535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This tool is an a beta stage\n",
            " Alexa developers can use Get Metrics API to\n",
            "seamlessly analyse metric\n",
            " It also supports custom skill model, prebuilt Flash Briefing\n",
            "model, and the Smart Home Skill API\n",
            " You can use this tool for creation of monitors,\n",
            "alarms, and dashboards that spotlight changes\n",
            " The release of these three tools will\n",
            "enable developers to create visual rich skills for Alexa devices with screens\n",
            " Amazon\n",
            "describes these tools as the collection of tech and tools for creating visually rich and\n",
            "interactive voice experiences\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:**b.  Tokenization using Regular Expressions (RegEx)"
      ],
      "metadata": {
        "id": "1GUy1ZzanbG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# import RegexpTokenizer() method from nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Create a reference variable for Class RegexpTokenizer\n",
        "tk = RegexpTokenizer('\\s+', gaps=True)\n",
        "\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "\n",
        "# Use tokenize method\n",
        "tokens = tk.tokenize(str)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6dxtH3yne5I",
        "outputId": "40c016e3-26e5-43e1-c97a-777a5237f505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** c.Tokenization using NLTK"
      ],
      "metadata": {
        "id": "buaMa-e_n8DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Use tokenize method\n",
        "print(word_tokenize(str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZYXa5t2oOim",
        "outputId": "aad312f2-2b87-403f-db91-13b10a4bbad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** d. Tokenization using the spaCy library"
      ],
      "metadata": {
        "id": "tYd8P6tToZFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.blank(\"en\")\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# Create an instance of document;\n",
        "# doc object is a container for a sequence of Token objects.\n",
        "doc = nlp(str)\n",
        "# Read the words; Print the words\n",
        "#\n",
        "words = [word.text for word in doc]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujqryx5SofUS",
        "outputId": "295cc130-9bd5-4389-fec6-62f3efa756af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:**e.  Tokenization using Keras"
      ],
      "metadata": {
        "id": "LvyY7XkNo0Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow  # Install TensorFlow if you haven't already\n",
        "# import tensorflow as tf\n",
        "\n",
        "# Use tensorflow.keras.preprocessing.text\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# Rest of your code\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "tokens = text_to_word_sequence(str)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xj-q1TvqsCV",
        "outputId": "be9ab4ae-d250-4ba6-dd1d-e39f5559f4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "['i', 'love', 'to', 'study', 'natural', 'language', 'processing', 'in', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** f. Tokenization using Gensim"
      ],
      "metadata": {
        "id": "nFqrgJRZpmi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install gensim # install the gensim package."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBla_0JWVV9d",
        "outputId": "df221cc7-61d1-4518-e083-85de623b87dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.1.3\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy  # Upgrade numpy if you haven't already\n",
        "!pip install --upgrade gensim # Reinstall gensim\n",
        "\n",
        "import gensim # Import gensim after upgrading numpy and gensim\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "\n",
        "# tokenizing the text\n",
        "print(list(tokenize(str)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "Qcv2Q5sVp2zj",
        "outputId": "a73dad53-189c-4740-acef-7191dee693ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-fc10b79a88ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade gensim # Reinstall gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;31m# Import gensim after upgrading numpy and gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final one\n",
        "from gensim.utils import tokenize\n",
        "# Create a string input\n",
        "input_str = \"Players unknown battlegrounds ready to launch\"\n",
        "tokens = list(tokenize(input_str)) # Tokenize the text\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5YjRnsQWiEF",
        "outputId": "5a80f2e3-0eb0-41e6-e44c-39911187405d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Players', 'unknown', 'battlegrounds', 'ready', 'to', 'launch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL: 05**"
      ],
      "metadata": {
        "id": "jkb1zufR4j_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical no: 5 (a)\n",
        "**Aim:** Word tokenization in Hindi"
      ],
      "metadata": {
        "id": "L0fxF3oE4Z01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install indic-nlp-library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2INa-PEMXLC7",
        "outputId": "8a33c639-a50a-4675-ca23-146abacc24a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from indicnlp import common\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"indic_nlp_library\"\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=r\"indic_nlp_resources\"\n",
        "# Add library to Python path\n",
        "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n",
        "# Set environment variable for resources folder\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "indic_string='सुनो, कुछ आवाज़ आ रही है। फोन?'\n",
        "print('Input String: {}'.format(indic_string))\n",
        "print('Tokens: ')\n",
        "for t in indic_tokenize.trivial_tokenize(indic_string): print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfg9il6BW-Wg",
        "outputId": "c196ec6c-9b0c-4a15-db66-aaa4167380f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String: सुनो, कुछ आवाज़ आ रही है। फोन?\n",
            "Tokens: \n",
            "सुनो\n",
            ",\n",
            "कुछ\n",
            "आवाज़\n",
            "आ\n",
            "रही\n",
            "है\n",
            "।\n",
            "फोन\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** b. Generate similar sentences from a given Hindi text input."
      ],
      "metadata": {
        "id": "gXualWpM5Sl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = {\n",
        "\"खुश\": [\"प्रसन्न\", \"आनंदित\", \"खुशी\"],\n",
        "\"बहुत\": [\"अधिक\", \"बहुत ज्यािा\", \"काफी\"]\n",
        "}\n",
        "# Function to generate similar sentences by replacing some words with synonyms\n",
        "def generate_similar_sentences(input_sentence,\n",
        "num_sentences=5):\n",
        "  similar_sentences = []\n",
        "# Replace some words with synonyms\n",
        "  for word, word_synonyms in synonyms.items():\n",
        "    for synonym in word_synonyms:\n",
        "      modified_sentence = input_sentence.replace(word, synonym)\n",
        "      similar_sentences.append(modified_sentence)\n",
        "  return similar_sentences[:num_sentences]\n",
        "input_sentence = \"मैं आज बहुत खुश हूँ।\"\n",
        "similar_sentences = generate_similar_sentences(input_sentence)\n",
        "print(\"Original sentence:\", input_sentence)\n",
        "print(\"Similar sentences:\")\n",
        "for sentence in similar_sentences:\n",
        "  print(\"-\", sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZOGtXwpXgXJ",
        "outputId": "7a38bcd5-709a-42a4-f026-8b70ebfb7ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: मैं आज बहुत खुश हूँ।\n",
            "Similar sentences:\n",
            "- मैं आज बहुत प्रसन्न हूँ।\n",
            "- मैं आज बहुत आनंदित हूँ।\n",
            "- मैं आज बहुत खुशी हूँ।\n",
            "- मैं आज अधिक खुश हूँ।\n",
            "- मैं आज बहुत ज्यािा खुश हूँ।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** c. Identify the Indian language from the given text."
      ],
      "metadata": {
        "id": "_NvLl44H5aYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymmn72CQZMt5",
        "outputId": "ed392e9f-2ae9-408d-fc26-104c64601845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from langid) (1.26.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=46d562bdca799103289403b18db4a84ed5af26854c79853feedc0c50048edcf3\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import langid\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "def identify_language(text):\n",
        "  lang, _ = langid.classify(text)\n",
        "  return lang\n",
        "# Identify the Indian Language from the given text\n",
        "language = identify_language(\"नमस्ते, आप कै से हैं?\")\n",
        "print(\"Identified language:\", language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGUEsTWpYxK6",
        "outputId": "3e83993e-a699-45c2-ec08-ba15de2cc8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified language: hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 6**\n",
        "\n",
        "Aim: Illustrate part of speech tagging."
      ],
      "metadata": {
        "id": "6NUeeL_VuL8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[A] Part of speech Tagging and chunking of user defined text.**\n",
        "\n",
        "[i] sentence tokenization\n",
        "\n",
        "[ii]word tokenization\n",
        "\n",
        "[iii]Part of speech Tagging and chunking of user defined text."
      ],
      "metadata": {
        "id": "h4hUH3KTuUqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "from nltk import tokenize\n",
        "from nltk import tag\n",
        "from nltk import chunk\n",
        "para = \"Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\\n===================\\n\",sents)\n",
        "# word tokenization\n",
        "print(\"\\nword tokenization\\n===================\\n\")\n",
        "for index in range(len(sents)):\n",
        "  words = tokenize.word_tokenize(sents[index])\n",
        "print(words)\n",
        "# POS Tagging\n",
        "tagged_words = []\n",
        "for index in range(len(sents)):\n",
        "  tagged_words.append(tag.pos_tag(words))\n",
        "print(\"\\nPOS Tagging\\n===========\\n\",tagged_words)\n",
        "# chunking\n",
        "tree = []\n",
        "for index in range(len(sents)):\n",
        "  tree.append(chunk.ne_chunk(tagged_words[index]))\n",
        "print(\"\\nchunking\\n========\\n\")\n",
        "print(\"Tree: \",tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNMhS29M-R33",
        "outputId": "2adf6617-82eb-4955-d075-790ca649983f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence tokenization\n",
            "===================\n",
            " ['Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.']\n",
            "\n",
            "word tokenization\n",
            "===================\n",
            "\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'machine', 'learning', 'technology', 'that', 'gives', 'computers', 'the', 'ability', 'to', 'interpret', ',', 'manipulate', ',', 'and', 'comprehend', 'human', 'language', '.']\n",
            "\n",
            "POS Tagging\n",
            "===========\n",
            " [[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('technology', 'NN'), ('that', 'WDT'), ('gives', 'VBZ'), ('computers', 'NNS'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('interpret', 'VB'), (',', ','), ('manipulate', 'VB'), (',', ','), ('and', 'CC'), ('comprehend', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]]\n",
            "\n",
            "chunking\n",
            "========\n",
            "\n",
            "Tree:  [Tree('S', [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), Tree('ORGANIZATION', [('NLP', 'NNP')]), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('technology', 'NN'), ('that', 'WDT'), ('gives', 'VBZ'), ('computers', 'NNS'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('interpret', 'VB'), (',', ','), ('manipulate', 'VB'), (',', ','), ('and', 'CC'), ('comprehend', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[B] Named Entity recognition of user defined text."
      ],
      "metadata": {
        "id": "CLqF3dBOvcZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "\"Google in 2007, few people outside of the company took him \"\n",
        "\"seriously. “I can tell you very senior CEOs of major American \"\n",
        "\"car companies would shake my hand and turn away because I wasn’t \"\n",
        "\"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "\"this week.\")\n",
        "print(\"Original text: \", text, \"\\n\")\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "# Analyse syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pYf7oA2vdcQ",
        "outputId": "47409f5d-6461-46e7-8109-9cd275c9a31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. “I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week. \n",
            "\n",
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[C] Named Entity recognition with diagram using NLTK corpus – treebank POS\n",
        "Tagging, chunking and NER:"
      ],
      "metadata": {
        "id": "WtiUaXlvvmhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#journal code\n",
        "import nltk\n",
        "# First download required datasets\n",
        "nltk.download('treebank') # For parsed sentences\n",
        "nltk.download('tagsets') # For part-of-speech tags\n",
        "nltk.download('book') # For additional resources (needed for .draw())\n",
        "from nltk.corpus import treebank\n",
        "# Access the first tagged sentence (POS tags)\n",
        "tagged_sentence = treebank.tagged_sents()[0]\n",
        "print(\"First tagged sentence:\\n\", tagged_sentence)\n",
        "# Access the first parsed sentence (tree structure)\n",
        "parsed_sentence = treebank.parsed_sents()[0]\n",
        "print(\"\\nFirst parsed sentence structure:\\n\", parsed_sentence)\n",
        "# Instead of parsed_sentence.draw(), print the tree structure:\n",
        "print(parsed_sentence) # or parsed_sentence.pretty_print() for a formatted output\n",
        "print(parsed_sentence.pretty_print())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOgfPUyC92JP",
        "outputId": "36c43fb1-3911-409f-aa96-767183d57391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First tagged sentence:\n",
            " [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "\n",
            "First parsed sentence structure:\n",
            " (S\n",
            "  (NP-SBJ\n",
            "    (NP (NNP Pierre) (NNP Vinken))\n",
            "    (, ,)\n",
            "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
            "    (, ,))\n",
            "  (VP\n",
            "    (MD will)\n",
            "    (VP\n",
            "      (VB join)\n",
            "      (NP (DT the) (NN board))\n",
            "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
            "      (NP-TMP (NNP Nov.) (CD 29))))\n",
            "  (. .))\n",
            "(S\n",
            "  (NP-SBJ\n",
            "    (NP (NNP Pierre) (NNP Vinken))\n",
            "    (, ,)\n",
            "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
            "    (, ,))\n",
            "  (VP\n",
            "    (MD will)\n",
            "    (VP\n",
            "      (VB join)\n",
            "      (NP (DT the) (NN board))\n",
            "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
            "      (NP-TMP (NNP Nov.) (CD 29))))\n",
            "  (. .))\n",
            "                                                     S                                                                         \n",
            "                         ____________________________|_______________________________________________________________________   \n",
            "                        |                                               VP                                                   | \n",
            "                        |                        _______________________|___                                                 |  \n",
            "                      NP-SBJ                    |                           VP                                               | \n",
            "         _______________|___________________    |     ______________________|______________________________________          |  \n",
            "        |          |              ADJP      |   |    |        |                PP-CLR                              |         | \n",
            "        |          |           ____|____    |   |    |        |          ________|_________                        |         |  \n",
            "        NP         |          NP        |   |   |    |        NP        |                  NP                    NP-TMP      | \n",
            "   _____|____      |     _____|____     |   |   |    |     ___|____     |    ______________|__________        _____|_____    |  \n",
            " NNP        NNP    ,    CD        NNS   JJ  ,   MD   VB   DT       NN   IN  DT             JJ         NN    NNP          CD  . \n",
            "  |          |     |    |          |    |   |   |    |    |        |    |   |              |          |      |           |   |  \n",
            "Pierre     Vinken  ,    61       years old  ,  will join the     board  as  a         nonexecutive director Nov.         29  . \n",
            "\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 7**"
      ],
      "metadata": {
        "id": "FlAwev7_vxZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[A] Define grammar using nltk. Analyze a sentence using the same."
      ],
      "metadata": {
        "id": "GpdyLI4nvx8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#journal code\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt_tab')\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> VP\n",
        "VP -> VP NP\n",
        "NP -> Det NP\n",
        "Det -> 'that'\n",
        "NP -> 'flight'\n",
        "VP -> 'Book'\n",
        "\"\"\")\n",
        "sentence = \"Book that flight\"\n",
        "all_tokens = tokenize.word_tokenize(sentence)\n",
        "print(all_tokens)\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "for tree in parser.parse(all_tokens):\n",
        "  print(tree)\n",
        "tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJQ7PeG6-7tJ",
        "outputId": "511efb4d-b8ff-488a-a0ad-06bba84bbf7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Book', 'that', 'flight']\n",
            "(S (VP (VP Book) (NP (Det that) (NP flight))))\n",
            "      S             \n",
            "      |              \n",
            "      VP            \n",
            "  ____|____          \n",
            " |         NP       \n",
            " |     ____|____     \n",
            " VP  Det        NP  \n",
            " |    |         |    \n",
            "Book that     flight\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "# nltk.download('punkt') # Download the 'punkt' tokenizer if not already present\n",
        "# nltk.download('averaged_perceptron_tagger_eng') # Download the 'averaged_perceptron_tagger' if not already present\n",
        "# nltk.download('maxent_ne_chunker') # Download the 'maxent_ne_chunker' if not already present\n",
        "# nltk.download('words') # Download the 'words' dataset if not already present\n",
        "\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> VP\n",
        "VP -> VP NP\n",
        "NP -> Det NP\n",
        "Det -> 'that'\n",
        "NP -> singular Noun\n",
        "NP -> 'flight'\n",
        "VP -> 'Book'\n",
        "\"\"\")\n",
        "\n",
        "sentence = \"Book that flight\"\n",
        "# Tokenize the sentence outside the loop\n",
        "all_tokens = tokenize.word_tokenize(sentence)\n",
        "\n",
        "# Initialize the parser outside the loop\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "\n",
        "# Parse the sentence and print the tree structure\n",
        "for tree in parser.parse(all_tokens):\n",
        "  print(tree)\n",
        "  # Instead of tree.draw(), print the tree structure to the console\n",
        "  # tree.draw()  # This line is causing the error - remove or comment out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQ5sZmPwPKy",
        "outputId": "f4a1a724-9d6c-495d-ad47-f1ee4b1d7bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (VP (VP Book) (NP (Det that) (NP flight))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** [B] Accept the input string with Regular expression of FA: 101+"
      ],
      "metadata": {
        "id": "R89mfePU_O09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FA(s):\n",
        "#if the length is less than 3 then it can't be accepted, Therefore end the process.\n",
        "  if len(s)<3:\n",
        "    return \"Rejected\"\n",
        "# first three characters are fixed. Therefore, checking them using index\n",
        "  if s[0] == '1':\n",
        "    if s[1] == '0':\n",
        "      if s[2] == '1':\n",
        "# After index 2 only \"1\" can appear. Therefore break the process if any other character is detected\n",
        "        for i in range(3, len(s)):\n",
        "          if s[i] != '1':\n",
        "            return \"Rejected\"\n",
        "        return \"Accepted\" # if all 4 nested if true\n",
        "      return \"Rejected\" # else of 3rd if\n",
        "    return \"Rejected\" # else of 2nd if\n",
        "  return \"Rejected\" # else of 1st if\n",
        "inputs = ['1', '10101', '101', '10111', '01010', '100', '', '10111101', '1011111']\n",
        "for i in inputs:\n",
        "  print(FA(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01zhCL_lwZL5",
        "outputId": "3266924d-e6d7-4f8a-d437-7361f0ce7040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rejected\n",
            "Rejected\n",
            "Accepted\n",
            "Accepted\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n",
            "Accepted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[C] Aim: Accept the input string with Regular expression of FA: (a+b)*bba"
      ],
      "metadata": {
        "id": "DaZvnn8cyWtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FA(s):\n",
        "  size=0\n",
        "  for i in s:\n",
        "    if i=='a' or i=='b':\n",
        "      size+=1\n",
        "    else:\n",
        "      return \"Rejected\"\n",
        "  if size>=3:\n",
        "    if s[size-3]=='b':\n",
        "      if s[size-2]=='b':\n",
        "        if s[size-1]=='a':\n",
        "          return \"Accepted\" # if all 4 if true\n",
        "        return \"Rejected\" # else of 4th if\n",
        "      return \"Rejected\" # else of 3rd if\n",
        "    return \"Rejected\" # else of 2nd if\n",
        "  return \"Rejected\" # else of 1st if\n",
        "inputs=['bba', 'ababbba', 'abba','abb', 'baba','bbb','']\n",
        "for i in inputs:\n",
        "  print(FA(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81ddTLoqyZsP",
        "outputId": "c2522375-df6c-4921-995c-329766c613b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accepted\n",
            "Accepted\n",
            "Accepted\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n",
            "Rejected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[D] Aim: Implementation of Deductive Chart Parsing using context free grammar and\n",
        "a given sentence."
      ],
      "metadata": {
        "id": "9_B8T0zuy-pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "\n",
        "# Download necessary resources if you haven't already\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'a' | 'my'\n",
        "N -> 'bird' | 'balcony'\n",
        "V -> 'saw'\n",
        "P -> 'in'\n",
        "\"\"\")\n",
        "\n",
        "sentence = \"I saw a bird in my balcony\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "all_tokens = tokenize.word_tokenize(sentence)\n",
        "print(all_tokens)\n",
        "\n",
        "# Parse the sentence\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "for tree in parser.parse(all_tokens):\n",
        "  print(tree)\n",
        "  tree.pretty_print() # Use pretty_print instead of draw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zUt4TBMhDWr",
        "outputId": "53009f43-3770-4f9b-eea2-d9a3111dbc2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V saw) (NP (Det a) (N bird)))\n",
            "    (PP (P in) (NP (Det my) (N balcony)))))\n",
            "     S                                  \n",
            "  ___|___________                        \n",
            " |               VP                     \n",
            " |        _______|________               \n",
            " |       VP               PP            \n",
            " |    ___|___          ___|___           \n",
            " |   |       NP       |       NP        \n",
            " |   |    ___|___     |    ___|_____     \n",
            " NP  V  Det      N    P  Det        N   \n",
            " |   |   |       |    |   |         |    \n",
            " I  saw  a      bird  in  my     balcony\n",
            "\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP (Det a) (N bird) (PP (P in) (NP (Det my) (N balcony))))))\n",
            "     S                              \n",
            "  ___|_______                        \n",
            " |           VP                     \n",
            " |    _______|____                   \n",
            " |   |            NP                \n",
            " |   |    ________|___               \n",
            " |   |   |   |        PP            \n",
            " |   |   |   |     ___|___           \n",
            " |   |   |   |    |       NP        \n",
            " |   |   |   |    |    ___|_____     \n",
            " NP  V  Det  N    P  Det        N   \n",
            " |   |   |   |    |   |         |    \n",
            " I  saw  a  bird  in  my     balcony\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "break # Exit after printing the first (and only) parse tree"
      ],
      "metadata": {
        "id": "m1JdqKU8_-oo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "06441885-2b88-4688-bf02-5519044436ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'break' outside loop (<ipython-input-92-da644ad4cd35>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-92-da644ad4cd35>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break # Exit after printing the first (and only) parse tree\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 8**"
      ],
      "metadata": {
        "id": "dCGTcB-bzWZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim: a. Study PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
        "Study WordNetLemmatizer"
      ],
      "metadata": {
        "id": "SzSS-FtezZSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PorterStemmer:\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "print(word_stemmer.stem('writing'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xa6zjZVzYTD",
        "outputId": "e357447f-14b0-4e4d-f90c-1e8129220bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LancasterStemmer:\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "Lanc_stemmer = LancasterStemmer()\n",
        "print(Lanc_stemmer.stem('writing'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCbf80-4LEuL",
        "outputId": "a0e059f1-fdf6-4693-9d35-a76ba24c3b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "writ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RegexpStemmer:\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import RegexpStemmer\n",
        "Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "print(Reg_stemmer.stem('writing'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f2jklutLN6K",
        "outputId": "df68af3b-0dfc-4888-f78c-1070801f11d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "writ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SnowballStemmer\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "english_stemmer = SnowballStemmer('english')\n",
        "print(english_stemmer.stem ('writing'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX1yN0DPMMyY",
        "outputId": "e9139665-7325-4ab2-9ef5-c749398d17ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** b. Study WordNet Lemmatizer."
      ],
      "metadata": {
        "id": "3wYhLj3YBlI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNetLemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk # Added this line\n",
        "\n",
        "nltk.download('wordnet') # Download WordNet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"word :\\tlemma\")\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37RrjPkeMjxF",
        "outputId": "ebcf2beb-882d-4f86-968e-59eb26932226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word :\tlemma\n",
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICAL 9**"
      ],
      "metadata": {
        "id": "0Lm3JasFMrCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Implement Naive Bayes classifier"
      ],
      "metadata": {
        "id": "l2sJNbliMtg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pandas\n",
        "#pip install sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk  # Make sure nltk is imported\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# ... (Rest of the code)\n",
        "sms_data = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
        "import re\n",
        "# ... (Rest of the code)"
      ],
      "metadata": {
        "id": "k_R08Ump1L5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pandas\n",
        "#pip install sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sms_data = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemming = PorterStemmer()\n",
        "corpus = []\n",
        "for i in range (0,len(sms_data)):\n",
        "    # Indent the code block within the for loop\n",
        "    s1 = re.sub('[^a-zA-Z]',repl = ' ',string = sms_data['v2'][i])\n",
        "    s1.lower()\n",
        "    s1 = s1.split()\n",
        "    s1 = [stemming.stem(word) for word in s1 if word not in\n",
        "    set(stopwords.words('english'))]\n",
        "    s1 = ' '.join(s1)\n",
        "    corpus.append(s1)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "countvectorizer =CountVectorizer()\n",
        "x = countvectorizer.fit_transform(corpus).toarray()\n",
        "print(x)\n",
        "y = sms_data['v1'].values\n",
        "print(y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,\n",
        "stratify=y,random_state=2)\n",
        "#Multinomial Naïve Bayes.\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "multinomialnb = MultinomialNB()\n",
        "multinomialnb.fit(x_train,y_train)\n",
        "# Predicting on test data:\n",
        "y_pred = multinomialnb.predict(x_test)\n",
        "print(y_pred)\n",
        "#Results of our Model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"accuracy_score: \",accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVmUbFpmNqn4",
        "outputId": "87939802-bf45-4819-8a04-9cfd264276c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n",
            "['ham' 'ham' 'ham' ... 'ham' 'ham' 'ham']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99      1448\n",
            "        spam       0.92      0.92      0.92       224\n",
            "\n",
            "    accuracy                           0.98      1672\n",
            "   macro avg       0.95      0.95      0.95      1672\n",
            "weighted avg       0.98      0.98      0.98      1672\n",
            "\n",
            "accuracy_score:  0.9784688995215312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Speech tagging\n",
        "\n",
        "1.Speech Tagging using spaCy"
      ],
      "metadata": {
        "id": "Blj2VE_dFFNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "sen = sp(u\"I like to play cricket. I hated it in my childhood though\")\n",
        "print(sen.text)\n",
        "print(sen[7].pos_)\n",
        "print(sen[7].tag_)\n",
        "print(spacy.explain(sen[7].tag_))\n",
        "for word in sen:\n",
        "  print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
        "sen = sp(u'Can you google it?')\n",
        "word = sen[2]\n",
        "print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
        "sen = sp(u'Can you search it on google?')\n",
        "word = sen[5]\n",
        "print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n",
        "# Finding the Number of POS Tags\n",
        "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
        "num_pos = sen.count_by(spacy.attrs.POS)\n",
        "for k, v in sorted(num_pos.items()):\n",
        "  print(f'{k}. {sen.vocab[k].text:{8}}: {v}')\n",
        "# Visualizing Parts of Speech Tags\n",
        "from spacy import displacy\n",
        "sen = sp(u\"I like to play football. I hated it in my childhood though\")\n",
        "# Use `displacy.render` instead of `displacy.serve`\n",
        "displacy.render(sen, style='dep', jupyter=True, options={'distance': 120})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "DdwnH8_y5qfe",
        "outputId": "3cd1b4cb-8702-400c-af44-200775da67ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to play cricket. I hated it in my childhood though\n",
            "VERB\n",
            "VBD\n",
            "verb, past tense\n",
            "I            PRON       PRP      pronoun, personal\n",
            "like         VERB       VBP      verb, non-3rd person singular present\n",
            "to           PART       TO       infinitival \"to\"\n",
            "play         VERB       VB       verb, base form\n",
            "cricket      NOUN       NN       noun, singular or mass\n",
            ".            PUNCT      .        punctuation mark, sentence closer\n",
            "I            PRON       PRP      pronoun, personal\n",
            "hated        VERB       VBD      verb, past tense\n",
            "it           PRON       PRP      pronoun, personal\n",
            "in           ADP        IN       conjunction, subordinating or preposition\n",
            "my           PRON       PRP$     pronoun, possessive\n",
            "childhood    NOUN       NN       noun, singular or mass\n",
            "though       SCONJ      IN       conjunction, subordinating or preposition\n",
            "google       VERB       VB       verb, base form\n",
            "google       PROPN      NNP      noun, proper singular\n",
            "85. ADP     : 1\n",
            "92. NOUN    : 2\n",
            "94. PART    : 1\n",
            "95. PRON    : 4\n",
            "97. PUNCT   : 1\n",
            "98. SCONJ   : 1\n",
            "100. VERB    : 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"35b7a112deb349ba9e183123b9eb3a7d-0\" class=\"displacy\" width=\"1490\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">like</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">play</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">football.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">hated</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">it</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">my</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">childhood</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">though</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">SCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,122.0 160.0,122.0 160.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-1\" stroke-width=\"2px\" d=\"M310,182.0 C310,122.0 400.0,122.0 400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M310,184.0 L302,172.0 318,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-2\" stroke-width=\"2px\" d=\"M190,182.0 C190,62.0 405.0,62.0 405.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M405.0,184.0 L413.0,172.0 397.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,122.0 520.0,122.0 520.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520.0,184.0 L528.0,172.0 512.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-4\" stroke-width=\"2px\" d=\"M670,182.0 C670,122.0 760.0,122.0 760.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M670,184.0 L662,172.0 678,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-5\" stroke-width=\"2px\" d=\"M790,182.0 C790,122.0 880.0,122.0 880.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M880.0,184.0 L888.0,172.0 872.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-6\" stroke-width=\"2px\" d=\"M790,182.0 C790,62.0 1005.0,62.0 1005.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1005.0,184.0 L1013.0,172.0 997.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-7\" stroke-width=\"2px\" d=\"M1150,182.0 C1150,122.0 1240.0,122.0 1240.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1150,184.0 L1142,172.0 1158,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-8\" stroke-width=\"2px\" d=\"M1030,182.0 C1030,62.0 1245.0,62.0 1245.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1245.0,184.0 L1253.0,172.0 1237.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-35b7a112deb349ba9e183123b9eb3a7d-0-9\" stroke-width=\"2px\" d=\"M790,182.0 C790,2.0 1370.0,2.0 1370.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-35b7a112deb349ba9e183123b9eb3a7d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1370.0,184.0 L1378.0,172.0 1362.0,172.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Speech tagging using nktl"
      ],
      "metadata": {
        "id": "yqNpzUYC238A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "# Create our training and testing data:\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "# Train the Punkt tokenizer:\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "# Tokenize:\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized[:2]:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    process_content()\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agM-Zep4HHqU",
        "outputId": "2747b2ca-5bfe-4a92-d4a8-ca50400ba8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/teropa/nlp/blob/master/resources/corpora/state_union/2005-GWBush.txt\n",
        "\n",
        "https://github.com/teropa/nlp/blob/master/resources/corpora/state_union/2006-GWBush.txt"
      ],
      "metadata": {
        "id": "JNozV2usj5OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9 (c)\n",
        "**Aim:** Statistical parsing\n",
        "\n",
        "1.Usage of Give and Gave in the Penn Treebank sample"
      ],
      "metadata": {
        "id": "vFGoMKc95TfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'treebank' corpus\n",
        "nltk.download('treebank')\n",
        "\n",
        "def give(t):\n",
        "  return t.label() == 'VP' and len(t) > 2 and t[1].label() == 'NP'  and (t[2].label() == 'PP-DTV' or t[2].label() == 'NP')  and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\n",
        "def sent(t):\n",
        "  return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\n",
        "def print_node(t, width):\n",
        "  output = \"%s %s: %s / %s: %s\" %  (sent(t[0]), t[1].label(), sent(t[1]), t[2].label(), sent(t[2]))\n",
        "  if len(output) > width:\n",
        "    output = output[:width] + \"...\"\n",
        "  print(output)\n",
        "for tree in nltk.corpus.treebank.parsed_sents():\n",
        "  for t in tree.subtrees(give):\n",
        "    print_node(t, 72)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJuVAIOvIuYd",
        "outputId": "46d9e26c-032d-496b-c589-004f6c169ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gave NP: the chefs / NP: a standing ovation\n",
            "give NP: advertisers / NP: discounts for maintaining or increasing ad sp...\n",
            "give NP: it / PP-DTV: to the politicians\n",
            "gave NP: them / NP: similar help\n",
            "give NP: them / NP: \n",
            "give NP: only French history questions / PP-DTV: to students in a Europe...\n",
            "give NP: federal judges / NP: a raise\n",
            "give NP: consumers / NP: the straight scoop on the U.S. waste crisis\n",
            "gave NP: Mitsui / NP: access to a high-tech medical product\n",
            "give NP: Mitsubishi / NP: a window on the U.S. glass industry\n",
            "give NP: much thought / PP-DTV: to the rates she was receiving , nor to ...\n",
            "give NP: your Foster Savings Institution / NP: the gift of hope and free...\n",
            "give NP: market operators / NP: the authority to suspend trading in futu...\n",
            "gave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...\n",
            "give NP: the Transportation Department / NP: up to 50 days to review any...\n",
            "give NP: the president / NP: such power\n",
            "give NP: me / NP: the heebie-jeebies\n",
            "give NP: holders / NP: the right , but not the obligation , to buy a cal...\n",
            "gave NP: Mr. Thomas / NP: only a `` qualified '' rating , rather than ``...\n",
            "give NP: the president / NP: line-item veto power\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Probabilistic parser**"
      ],
      "metadata": {
        "id": "HD-M6wnV3FAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import PCFG\n",
        "# Define the PCFG grammar correctly\n",
        "grammar = PCFG.fromstring('''\n",
        "NP -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]\n",
        "NNS -> \"men\" [0.1] | \"women\" [0.2] | \"children\" [0.3] | \"people\" [0.4]\n",
        "JJ -> \"old\" [0.4] | \"young\" [0.6]\n",
        "CC -> \"and\" [0.9] | \"or\" [0.1]\n",
        "''')\n",
        "print(grammar) # Print the grammar\n",
        "# Initialize Viterbi Parser\n",
        "viterbi_parser = nltk.ViterbiParser(grammar)\n",
        "# Input sentence\n",
        "tokens = \"old men and women\".split()\n",
        "# Parse the sentence\n",
        "print(\"\\nOutput:\")\n",
        "for tree in viterbi_parser.parse(tokens):\n",
        "  print(tree)\n",
        "# Instead of tree.draw(), use tree.pretty_print()\n",
        "tree.pretty_print() # This will print the tree to the console"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR6EQIT86l6D",
        "outputId": "49ae16fd-ec55-41f5-fd4a-fd5777f661a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar with 11 productions (start state = NP)\n",
            "    NP -> NNS [0.5]\n",
            "    NP -> JJ NNS [0.3]\n",
            "    NP -> NP CC NP [0.2]\n",
            "    NNS -> 'men' [0.1]\n",
            "    NNS -> 'women' [0.2]\n",
            "    NNS -> 'children' [0.3]\n",
            "    NNS -> 'people' [0.4]\n",
            "    JJ -> 'old' [0.4]\n",
            "    JJ -> 'young' [0.6]\n",
            "    CC -> 'and' [0.9]\n",
            "    CC -> 'or' [0.1]\n",
            "\n",
            "Output:\n",
            "(NP (NP (JJ old) (NNS men)) (CC and) (NP (NNS women))) (p=0.000216)\n",
            "         NP          \n",
            "      ___|________    \n",
            "     NP      |    NP \n",
            "  ___|___    |    |   \n",
            " JJ     NNS  CC  NNS \n",
            " |       |   |    |   \n",
            "old     men and women\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical no: 10 (a)**\n",
        "\n",
        "**Aim:** a. Parse a sentence and draw a tree using malt parsing."
      ],
      "metadata": {
        "id": "mWFwz41k4lGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source Code:\n",
        "a. Java should be installed.\n",
        "\n",
        "b. maltparser-1.7.2 zip file should be copied in\n",
        "C:\\Users\\AppData\\Local\\Programs\\Python\\Python39 folder and should be\n",
        "extracted in the same folder.\n",
        "\n",
        "c. engmalt.linear-1.7.mco & engmalt.poly-1.7.mco file should be copied to\n",
        "C:\\Users\\ AppData\\Local\\Programs\\Python\\Python39 folder"
      ],
      "metadata": {
        "id": "6Mrpa3oq4qgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.parse import malt\n",
        "mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco')#file\n",
        "t = mp.parse_one('I saw a bird from my window.'.split()).tree()\n",
        "print(t)\n",
        "t.draw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "9Yk9LE_GKh89",
        "outputId": "459f0698-8f7f-4622-f632-fd7335160aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n\n===========================================================================\nNLTK was unable to find the maltparser-1.7.2 file!\nUse software specific configuration parameters or set the MALT_PARSER environment variable.\n===========================================================================",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-864987e67c73>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmalt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaltParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maltparser-1.7.2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'engmalt.linear-1.7.mco'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I saw a bird from my window.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/parse/malt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser_dirname, model_filename, tagger, additional_java_args)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Find all the necessary jar files for MaltParser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmalt_jars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_maltparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_dirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Initialize additional java arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         self.additional_java_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/parse/malt.py\u001b[0m in \u001b[0;36mfind_maltparser\u001b[0;34m(parser_dirname)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0m_malt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser_dirname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Try to find path to maltparser directory in environment variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0m_malt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_dirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MALT_PARSER\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Checks that that the found directory contains all the necessary .jar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mmalt_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_dir\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m ):\n\u001b[0;32m--> 636\u001b[0;31m     return next(\n\u001b[0m\u001b[1;32m    637\u001b[0m         find_file_iter(\n\u001b[1;32m    638\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinding_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n\\n  For more information on {filename}, see:\\n    <{url}>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\n{div}\\n{msg}\\n{div}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the maltparser-1.7.2 file!\nUse software specific configuration parameters or set the MALT_PARSER environment variable.\n==========================================================================="
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:**  10 b Multiword Expressions in NLP."
      ],
      "metadata": {
        "id": "6vRE276l3VMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "# Download 'punkt_tab' for sentence tokenization\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "s = '''Good cake cost Rs.1500\\kg in Hong Kong. Please buy me one of them.\\n\\nThanks.'''\n",
        "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')],\n",
        "separator='-')\n",
        "for sent in sent_tokenize(s):\n",
        "  print(mwe.tokenize(word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbh1MNv-PlX2",
        "outputId": "4fd527b4-21cd-4f19-aa5a-d7c9d17070c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Hong-Kong', '.']\n",
            "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
            "['Thanks', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** 10 c Normalized Web Distance and Word Similarity."
      ],
      "metadata": {
        "id": "ApK8YE9v3kam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy  # Upgrade scipy to the latest version\n",
        "!pip install --upgrade scikit-learn  # Upgrade scikit-learn as well\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import textdistance\n",
        "import sklearn\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "texts = ['Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market','Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport','k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading']\n",
        "\n",
        "def normalize(text):\n",
        "  \"\"\" Keep only lower-cased text and numbers\"\"\"\n",
        "  return re.sub('[^a-z0-9]+', ' ', text.lower())\n",
        "\n",
        "def group_texts(texts, threshold=0.4):\n",
        "  \"\"\" Replace each text with the representative of its cluster\"\"\"\n",
        "  normalized_texts = np.array([normalize(text) for text in texts])\n",
        "  distances = 1 - np.array([[textdistance.jaro_winkler(one, another) for one in normalized_texts] for another in normalized_texts])\n",
        "  clustering = AgglomerativeClustering(\n",
        "  distance_threshold=threshold, # this parameter needs to be tuned carefully\n",
        "  metric=\"precomputed\", linkage=\"complete\",n_clusters=None).fit(distances)\n",
        "  centers = dict()\n",
        "  for cluster_id in set(clustering.labels_):\n",
        "    index = clustering.labels_ == cluster_id\n",
        "    centrality = distances[:, index][index].sum(axis=1)\n",
        "    centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
        "  return [centers[i] for i in clustering.labels_]\n",
        "\n",
        "print(group_texts(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "h2hbsAxZlV_L",
        "outputId": "f6b169b7-825b-4146-9f5c-5683900397ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'textdistance'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-42145894256f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtextdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textdistance'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textdistance\n",
        "import numpy as np\n",
        "import re\n",
        "import textdistance # pip install textdistance\n",
        "# we will need scikit-learn>=0.21\n",
        "import sklearn # pip install sklearn\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "texts = ['Reliance supermarket', 'Reliance hypermarket',\n",
        "'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market','Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport','k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading']\n",
        "\n",
        "def normalize(text):\n",
        "  \"\"\" Keep only lower-cased text and numbers\"\"\"\n",
        "  return re.sub('[^a-z0-9]+', ' ', text.lower())\n",
        "\n",
        "def group_texts(texts, threshold=0.4):\n",
        "  \"\"\" Replace each text with the representative of its cluster\"\"\"\n",
        "  normalized_texts = np.array([normalize(text) for text in texts])\n",
        "  distances = 1 - np.array([[textdistance.jaro_winkler(one, another) for one in normalized_texts] for another in normalized_texts])\n",
        "  clustering = AgglomerativeClustering(\n",
        "  distance_threshold=threshold, # this parameter needs to be tuned carefully\n",
        "  metric=\"precomputed\", linkage=\"complete\",n_clusters=None).fit(distances)\n",
        "  centers = dict()\n",
        "  for cluster_id in set(clustering.labels_):\n",
        "    index = clustering.labels_ == cluster_id\n",
        "    centrality = distances[:, index][index].sum(axis=1)\n",
        "    centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
        "  return [centers[i] for i in clustering.labels_]\n",
        "print(group_texts(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVWvMiSpRFoo",
        "outputId": "21519f19-0fca-401a-af89-2d81746bce93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textdistance\n",
            "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.6.3\n",
            "[np.str_('reliance'), np.str_('reliance'), np.str_('reliance'), np.str_('reliance'), np.str_('reliance'), np.str_('reliance'), np.str_('mumbai'), np.str_('mumbai'), np.str_('mumbai'), np.str_('mumbai'), np.str_('km trading'), np.str_('km trading'), np.str_('km trading'), np.str_('km trading'), np.str_('km trading')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** 10 d Word Sense Disambiguation."
      ],
      "metadata": {
        "id": "UU7bl2ML4Gax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download the 'wordnet' dataset before using it.\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_first_sense(word, pos=None):\n",
        "  if pos:\n",
        "    synsets = wn.synsets(word,pos)\n",
        "  else:\n",
        "    synsets = wn.synsets(word)\n",
        "  return synsets[0]\n",
        "\n",
        "best_synset = get_first_sense('bank')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "\n",
        "best_synset = get_first_sense('set','n')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
        "\n",
        "best_synset = get_first_sense('set','v')\n",
        "print ('%s: %s' % (best_synset.name, best_synset.definition))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZmPqRsaQDqG",
        "outputId": "26b3a9bd-3504-43c2-971d-96e4dd3c5b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n",
            "<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n",
            "<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10c"
      ],
      "metadata": {
        "id": "HcA0Nt1kzakU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import textdistance\n",
        "import sklearn\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "texts = ['Reliance supermarket', 'Reliance hypermarket','Reliance', 'Reliance','Mumbai Hyper', 'Mumbai dxb','mumbai airport','k.m trading', 'KM Trading', 'KM trade', 'K.M.Trading', 'KM.Trading']\n",
        "\n",
        "def normalize(text):\n",
        "    return re.sub('[^a-z0-9]+', '',text.lower())\n",
        "\n",
        "def group_texts(texts, threshold=0.4):\n",
        "    normalized_texts = np.array([normalize(text) for text in texts])\n",
        "    distances = 1 - np.array([[textdistance.jaro_winkler(one, another) for one in normalized_texts] for another in normalized_texts])\n",
        "\n",
        "    clustering = AgglomerativeClustering(distance_threshold=threshold,metric='precomputed',linkage=\"complete\",n_clusters=None).fit(distances)\n",
        "\n",
        "    centers = dict()\n",
        "\n",
        "    for cluster_id in set(clustering.labels_):\n",
        "        index = clustering.labels_ == cluster_id\n",
        "        centrality = distances[:, index][index].sum(axis=1)\n",
        "        centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
        "    return [centers[i] for i in clustering.labels_]\n",
        "\n",
        "l1 = []\n",
        "for i in group_texts(texts):\n",
        "    l1.append(str(i))\n",
        "\n",
        "print(l1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QAUTF1bzbok",
        "outputId": "5049bd24-fc50-4e14-eaee-dfa03a8611e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reliance', 'reliance', 'reliance', 'reliance', 'mumbaihyper', 'mumbaihyper', 'mumbaihyper', 'kmtrading', 'kmtrading', 'kmtrading', 'kmtrading', 'kmtrading']\n"
          ]
        }
      ]
    }
  ]
}